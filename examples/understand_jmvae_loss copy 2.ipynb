{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example to show the effect of the flows and of the functions $f_i$ on the JMVAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an artificial dataset with a 2D latent space\n",
    "\n",
    "# Fix the random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "N = 10000\n",
    "x = dist.Normal(0,1).sample([N])\n",
    "y = dist.Normal(0,1).sample([N])\n",
    "print(x.shape)\n",
    "plt.scatter(x,y)\n",
    "data=[x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10000])\n"
     ]
    }
   ],
   "source": [
    "# We define the joint posterior to be a 2D gaussian with diagonal covariance matrix\n",
    "s1 = 0.1\n",
    "s2 = 0.1\n",
    "\n",
    "def sample_from_joint_posterior(x,y,K=1):\n",
    "    z1 = dist.Normal(x,s1).sample([K])\n",
    "    z2 = dist.Normal(y,s2).sample([K])\n",
    "    \n",
    "    return torch.stack([z1, z2])\n",
    "\n",
    "def sample_from_uni_posterior_x(x,K=1):\n",
    "    z1 = dist.Normal(x,s1).sample([K])\n",
    "    z2 = dist.Normal(0,np.sqrt(1+s2**2)).sample([K])\n",
    "    \n",
    "    return torch.stack([z1,z2])\n",
    "\n",
    "def sample_from_uni_posterior_y(y,K=1):\n",
    "    z1 = dist.Normal(0,np.sqrt(1+s1**2)).sample([K])\n",
    "    z2 = dist.Normal(y,s2).sample([K])\n",
    "    \n",
    "    return torch.stack([z1,z2])\n",
    "\n",
    "z1,z2 = sample_from_joint_posterior(x,y,K=100)\n",
    "print(z1.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from bivae.my_pythae.models.vae_maf import my_VAE_MAF, VAE_MAF_Config\n",
    "from bivae.my_pythae.models.vae import my_VAE\n",
    "from pythae.models.vae import VAEConfig\n",
    "from pythae.models.nn.base_architectures import BaseDecoder, BaseEncoder\n",
    "from pythae.models.base.base_utils import ModelOutput\n",
    "\n",
    "n_hidden_dim = 100\n",
    "\n",
    "class Encoder_VAE_MLP(BaseEncoder):\n",
    "    def __init__(self, args: dict):\n",
    "        BaseEncoder.__init__(self)\n",
    "        self.input_dim = args.input_dim\n",
    "        self.latent_dim = args.latent_dim\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        layers.append(nn.Sequential(nn.Linear(np.prod(args.input_dim), n_hidden_dim), nn.ReLU()))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.depth = len(layers)\n",
    "\n",
    "        self.embedding = nn.Linear(n_hidden_dim, self.latent_dim)\n",
    "        self.log_var = nn.Linear(n_hidden_dim, self.latent_dim)\n",
    "\n",
    "    def forward(self, x, output_layer_levels = None):\n",
    "        output = ModelOutput()\n",
    "\n",
    "        max_depth = self.depth\n",
    "\n",
    "        if output_layer_levels is not None:\n",
    "\n",
    "            assert all(\n",
    "                self.depth >= levels > 0 or levels == -1\n",
    "                for levels in output_layer_levels\n",
    "            ), (\n",
    "                f\"Cannot output layer deeper than depth ({self.depth}). \"\n",
    "                f\"Got ({output_layer_levels}).\"\n",
    "            )\n",
    "\n",
    "            if -1 in output_layer_levels:\n",
    "                max_depth = self.depth\n",
    "            else:\n",
    "                max_depth = max(output_layer_levels)\n",
    "\n",
    "        out = x.reshape(-1, np.prod(self.input_dim))\n",
    "\n",
    "        for i in range(max_depth):\n",
    "            out = self.layers[i](out)\n",
    "\n",
    "            if output_layer_levels is not None:\n",
    "                if i + 1 in output_layer_levels:\n",
    "                    output[f\"embedding_layer_{i+1}\"] = out\n",
    "            if i + 1 == self.depth:\n",
    "                output[\"embedding\"] = self.embedding(out)\n",
    "                output[\"log_covariance\"] = self.log_var(out)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Decoder_AE_MLP(BaseDecoder):\n",
    "    def __init__(self, args: dict):\n",
    "        BaseDecoder.__init__(self)\n",
    "\n",
    "        self.input_dim = args.input_dim\n",
    "\n",
    "        # assert 0, np.prod(args.input_dim)\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        layers.append(nn.Sequential(nn.Linear(args.latent_dim, n_hidden_dim), nn.ReLU()))\n",
    "\n",
    "        layers.append(\n",
    "            nn.Sequential(nn.Linear(n_hidden_dim, int(np.prod(args.input_dim))), nn.Sigmoid())\n",
    "        )\n",
    "\n",
    "        self.layers = layers\n",
    "        self.depth = len(layers)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, output_layer_levels=None):\n",
    "\n",
    "        output = ModelOutput()\n",
    "\n",
    "        max_depth = self.depth\n",
    "\n",
    "        if output_layer_levels is not None:\n",
    "\n",
    "            assert all(\n",
    "                self.depth >= levels > 0 or levels == -1\n",
    "                for levels in output_layer_levels\n",
    "            ), (\n",
    "                f\"Cannot output layer deeper than depth ({self.depth}). \"\n",
    "                f\"Got ({output_layer_levels}).\"\n",
    "            )\n",
    "\n",
    "            if -1 in output_layer_levels:\n",
    "                max_depth = self.depth\n",
    "            else:\n",
    "                max_depth = max(output_layer_levels)\n",
    "\n",
    "        out = z\n",
    "\n",
    "        for i in range(max_depth):\n",
    "            out = self.layers[i](out)\n",
    "\n",
    "            if output_layer_levels is not None:\n",
    "                if i + 1 in output_layer_levels:\n",
    "                    output[f\"reconstruction_layer_{i+1}\"] = out\n",
    "            if i + 1 == self.depth:\n",
    "                output[\"reconstruction\"] = out.reshape((z.shape[0],) + self.input_dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class my_model(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        vae = my_VAE_MAF\n",
    "        vae_config = VAE_MAF_Config((1,1),latent_dim=2)\n",
    "        e1,e2 = Encoder_VAE_MLP(vae_config), Encoder_VAE_MLP(vae_config)\n",
    "        d1,d2 =Decoder_AE_MLP(vae_config), Decoder_AE_MLP(vae_config)\n",
    "        self.vaes = nn.ModuleList([\n",
    "            vae(model_config=vae_config, encoder=e1, decoder=d1),\n",
    "            vae(model_config=vae_config, encoder=e2, decoder=d2)\n",
    "\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def compute_kld(self, data):\n",
    "        \"\"\" Computes KL(q(z|x,y) || q(z|x)) + KL(q(z|x,y) || q(z|y))\"\"\"\n",
    "\n",
    "        z_xy=sample_from_joint_posterior(data[0],data[1]).squeeze(1).permute(1,0)\n",
    "        # print(z_xy.size())\n",
    "        reg = 0\n",
    "        details_reg = {}\n",
    "        for m, vae in enumerate(self.vaes):\n",
    "            # print(z_xy.shape)\n",
    "            flow_output = vae.flow(z_xy) if hasattr(vae, \"flow\") else vae.inverse_flow(z_xy)\n",
    "            vae_output = vae.encoder(data[m].unsqueeze(1))\n",
    "            mu, log_var, z0 = vae_output.embedding, vae_output.log_covariance, flow_output.out\n",
    "            log_q_z0 = (-0.5 * (log_var + np.log(2*np.pi) + torch.pow(z0 - mu, 2) / torch.exp(log_var))).sum(dim=1)\n",
    "\n",
    "            # kld -= log_q_z0 + flow_output.log_abs_det_jac\n",
    "            # details_reg[f'kld_{m}'] = qz_xy.sum() - (log_q_z0 + flow_output.log_abs_det_jac).sum()\n",
    "            details_reg[f'kld_{m}'] =  - (log_q_z0 + flow_output.log_abs_det_jac).sum()\n",
    "\n",
    "            reg += details_reg[f'kld_{m}']\n",
    "            \n",
    "        return reg, details_reg\n",
    "    \n",
    "    def sample_from_x(self, x, K=100):\n",
    "        \n",
    "        d = torch.stack([x]*K)\n",
    "        z = self.vaes[0](d).z\n",
    "        return z\n",
    "    \n",
    "    def sample_from_y(self, y, K=100):\n",
    "        d = torch.stack([y]*K)\n",
    "        z = self.vaes[1](d).z\n",
    "        return z\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a dataloader and train the flows\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "def train(n_train ):\n",
    "    model = my_model()\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                        lr=1e-5, amsgrad=True)\n",
    "    scheduler = ReduceLROnPlateau(optimizer,'min')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    n_test = 500\n",
    "    data_train = TensorDataset(x[:n_train], y[:n_train])\n",
    "    data_test = TensorDataset(x[n_train:n_train+n_test], y[n_train:n_train+n_test])\n",
    "    train_loader = DataLoader(data_train, 300)\n",
    "    test_loader = DataLoader(data_test, 300, shuffle=False)\n",
    "\n",
    "    best_test_loss = np.inf\n",
    "    for epoch in range(500):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for data_ in train_loader:\n",
    "            data_ = [d.cuda() for d in data_]\n",
    "            loss,dict = model.compute_kld(data_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        for data_ in test_loader:\n",
    "            data_ = [d.cuda() for d in data_]\n",
    "            with torch.no_grad():\n",
    "                loss,dict = model.compute_kld(data_)\n",
    "                test_loss += loss.item()\n",
    "        if test_loss < best_test_loss:\n",
    "            print('saved model')\n",
    "            torch.save(model.state_dict(), f'model_{n_train}.pt')\n",
    "            best_test_loss = test_loss  \n",
    "        scheduler.step(test_loss)\n",
    "        print(f'Epoch {epoch} : train_loss {train_loss/len(data_train)}')\n",
    "        print(f'Epoch {epoch} : test_loss {test_loss/len(data_test)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the learned flows \n",
    "def vis(n_train):\n",
    "    model = my_model()\n",
    "    model.load_state_dict(torch.load(f'model_{n_train}.pt'))\n",
    "    idx = 0\n",
    "    z_simulated_y = model.sample_from_y(y[idx], K = 500).detach().cpu()\n",
    "    z_simulated_x = model.sample_from_x(x[idx], K = 500).detach().cpu()\n",
    "\n",
    "    true_z_x = sample_from_uni_posterior_x(x[idx], K=500).detach().cpu()\n",
    "    true_z_y = sample_from_uni_posterior_y(y[idx], K=500).detach().cpu()\n",
    "    print(true_z_x.shape)\n",
    "\n",
    "    print(x[idx],y[idx])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (7,7))\n",
    "    ax.scatter(z_simulated_x[:,0],z_simulated_x[:,1])\n",
    "    ax.scatter(z_simulated_y[:,0],z_simulated_y[:,1])\n",
    "\n",
    "    ax.scatter(true_z_x[0],true_z_x[1])\n",
    "    ax.scatter(true_z_y[0],true_z_y[1])\n",
    "\n",
    "    ax.set_xlim([-10,10])\n",
    "    ax.set_ylim([-10,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1000\n",
      "saved model\n",
      "Epoch 0 : train_loss 6.927590942382812\n",
      "Epoch 0 : test_loss 7.07548388671875\n",
      "saved model\n",
      "Epoch 1 : train_loss 6.89862548828125\n",
      "Epoch 1 : test_loss 7.04061083984375\n",
      "Epoch 2 : train_loss 6.903511840820313\n",
      "Epoch 2 : test_loss 7.0441025390625\n",
      "Epoch 3 : train_loss 6.8580797729492184\n",
      "Epoch 3 : test_loss 7.042009765625\n",
      "saved model\n",
      "Epoch 4 : train_loss 6.844788208007812\n",
      "Epoch 4 : test_loss 6.955106201171875\n",
      "Epoch 5 : train_loss 6.829014404296875\n",
      "Epoch 5 : test_loss 6.9661689453125\n",
      "saved model\n",
      "Epoch 6 : train_loss 6.836720764160156\n",
      "Epoch 6 : test_loss 6.9322001953125\n",
      "saved model\n",
      "Epoch 7 : train_loss 6.80469091796875\n",
      "Epoch 7 : test_loss 6.921502197265625\n",
      "saved model\n",
      "Epoch 8 : train_loss 6.800282592773438\n",
      "Epoch 8 : test_loss 6.857419921875\n",
      "saved model\n",
      "Epoch 9 : train_loss 6.745238037109375\n",
      "Epoch 9 : test_loss 6.8478779296875\n",
      "saved model\n",
      "Epoch 10 : train_loss 6.729834899902344\n",
      "Epoch 10 : test_loss 6.8334482421875\n",
      "saved model\n",
      "Epoch 11 : train_loss 6.694275573730469\n",
      "Epoch 11 : test_loss 6.828206787109375\n",
      "saved model\n",
      "Epoch 12 : train_loss 6.660984985351562\n",
      "Epoch 12 : test_loss 6.7414423828125\n",
      "Epoch 13 : train_loss 6.666531188964844\n",
      "Epoch 13 : test_loss 6.75606982421875\n",
      "saved model\n",
      "Epoch 14 : train_loss 6.630591369628906\n",
      "Epoch 14 : test_loss 6.70933056640625\n",
      "saved model\n",
      "Epoch 15 : train_loss 6.595810302734375\n",
      "Epoch 15 : test_loss 6.702349609375\n",
      "saved model\n",
      "Epoch 16 : train_loss 6.594447998046875\n",
      "Epoch 16 : test_loss 6.665934814453125\n",
      "Epoch 17 : train_loss 6.57329345703125\n",
      "Epoch 17 : test_loss 6.69204296875\n",
      "saved model\n",
      "Epoch 18 : train_loss 6.539774963378906\n",
      "Epoch 18 : test_loss 6.629189697265625\n",
      "saved model\n",
      "Epoch 19 : train_loss 6.529127807617187\n",
      "Epoch 19 : test_loss 6.62484765625\n",
      "saved model\n",
      "Epoch 20 : train_loss 6.474134521484375\n",
      "Epoch 20 : test_loss 6.5818359375\n",
      "Epoch 21 : train_loss 6.489818115234375\n",
      "Epoch 21 : test_loss 6.595451171875\n",
      "saved model\n",
      "Epoch 22 : train_loss 6.456179992675781\n",
      "Epoch 22 : test_loss 6.542237060546875\n",
      "saved model\n",
      "Epoch 23 : train_loss 6.43047314453125\n",
      "Epoch 23 : test_loss 6.5283232421875\n",
      "saved model\n",
      "Epoch 24 : train_loss 6.429974060058594\n",
      "Epoch 24 : test_loss 6.51203271484375\n",
      "saved model\n",
      "Epoch 25 : train_loss 6.408812133789063\n",
      "Epoch 25 : test_loss 6.452849365234375\n",
      "Epoch 26 : train_loss 6.369152893066406\n",
      "Epoch 26 : test_loss 6.456557373046875\n",
      "saved model\n",
      "Epoch 27 : train_loss 6.374694396972656\n",
      "Epoch 27 : test_loss 6.442718994140625\n",
      "saved model\n",
      "Epoch 28 : train_loss 6.343166870117187\n",
      "Epoch 28 : test_loss 6.42875\n",
      "saved model\n",
      "Epoch 29 : train_loss 6.328670715332032\n",
      "Epoch 29 : test_loss 6.422260498046875\n",
      "Epoch 30 : train_loss 6.305610778808593\n",
      "Epoch 30 : test_loss 6.42631103515625\n",
      "saved model\n",
      "Epoch 31 : train_loss 6.301742065429687\n",
      "Epoch 31 : test_loss 6.355888427734375\n",
      "Epoch 32 : train_loss 6.274142395019531\n",
      "Epoch 32 : test_loss 6.364240234375\n",
      "saved model\n",
      "Epoch 33 : train_loss 6.260166137695313\n",
      "Epoch 33 : test_loss 6.354686767578125\n",
      "saved model\n",
      "Epoch 34 : train_loss 6.248737487792969\n",
      "Epoch 34 : test_loss 6.2991728515625\n",
      "saved model\n",
      "Epoch 35 : train_loss 6.245265808105469\n",
      "Epoch 35 : test_loss 6.290267822265625\n",
      "saved model\n",
      "Epoch 36 : train_loss 6.213657287597656\n",
      "Epoch 36 : test_loss 6.29017919921875\n",
      "saved model\n",
      "Epoch 37 : train_loss 6.200685363769531\n",
      "Epoch 37 : test_loss 6.262763671875\n",
      "saved model\n",
      "Epoch 38 : train_loss 6.195577514648438\n",
      "Epoch 38 : test_loss 6.22676025390625\n",
      "Epoch 39 : train_loss 6.162607177734375\n",
      "Epoch 39 : test_loss 6.25898974609375\n",
      "Epoch 40 : train_loss 6.179650451660156\n",
      "Epoch 40 : test_loss 6.24142431640625\n",
      "saved model\n",
      "Epoch 41 : train_loss 6.143923950195313\n",
      "Epoch 41 : test_loss 6.205189453125\n",
      "saved model\n",
      "Epoch 42 : train_loss 6.162844360351563\n",
      "Epoch 42 : test_loss 6.183740478515625\n",
      "saved model\n",
      "Epoch 43 : train_loss 6.143725769042969\n",
      "Epoch 43 : test_loss 6.17892919921875\n",
      "saved model\n",
      "Epoch 44 : train_loss 6.125400695800781\n",
      "Epoch 44 : test_loss 6.17318505859375\n",
      "saved model\n",
      "Epoch 45 : train_loss 6.110402404785156\n",
      "Epoch 45 : test_loss 6.124554931640625\n",
      "Epoch 46 : train_loss 6.079620788574219\n",
      "Epoch 46 : test_loss 6.1512978515625\n",
      "Epoch 47 : train_loss 6.1009189453125\n",
      "Epoch 47 : test_loss 6.144215576171875\n",
      "Epoch 48 : train_loss 6.090077392578125\n",
      "Epoch 48 : test_loss 6.136224365234375\n",
      "saved model\n",
      "Epoch 49 : train_loss 6.072504638671875\n",
      "Epoch 49 : test_loss 6.12444482421875\n",
      "saved model\n",
      "Epoch 50 : train_loss 6.080391235351563\n",
      "Epoch 50 : test_loss 6.073900390625\n",
      "Epoch 51 : train_loss 6.066038696289063\n",
      "Epoch 51 : test_loss 6.111045166015625\n",
      "Epoch 52 : train_loss 6.028892883300781\n",
      "Epoch 52 : test_loss 6.092362060546875\n",
      "saved model\n",
      "Epoch 53 : train_loss 6.033396667480469\n",
      "Epoch 53 : test_loss 6.050136474609375\n",
      "Epoch 54 : train_loss 6.02240966796875\n",
      "Epoch 54 : test_loss 6.063074462890625\n",
      "Epoch 55 : train_loss 6.009472900390625\n",
      "Epoch 55 : test_loss 6.060671142578125\n",
      "saved model\n",
      "Epoch 56 : train_loss 6.002232299804687\n",
      "Epoch 56 : test_loss 6.04040966796875\n",
      "saved model\n",
      "Epoch 57 : train_loss 6.003854370117187\n",
      "Epoch 57 : test_loss 6.020085205078125\n",
      "saved model\n",
      "Epoch 58 : train_loss 5.993444641113281\n",
      "Epoch 58 : test_loss 6.017937744140625\n",
      "Epoch 59 : train_loss 5.985971923828125\n",
      "Epoch 59 : test_loss 6.027986328125\n",
      "saved model\n",
      "Epoch 60 : train_loss 5.984160888671875\n",
      "Epoch 60 : test_loss 6.000391357421875\n",
      "Epoch 61 : train_loss 5.971934265136719\n",
      "Epoch 61 : test_loss 6.013436279296875\n",
      "saved model\n",
      "Epoch 62 : train_loss 5.959861633300782\n",
      "Epoch 62 : test_loss 5.989286376953125\n",
      "Epoch 63 : train_loss 5.9604603271484375\n",
      "Epoch 63 : test_loss 5.994296142578125\n",
      "saved model\n",
      "Epoch 64 : train_loss 5.951271362304688\n",
      "Epoch 64 : test_loss 5.98320703125\n",
      "saved model\n",
      "Epoch 65 : train_loss 5.964400024414062\n",
      "Epoch 65 : test_loss 5.973622314453125\n",
      "saved model\n",
      "Epoch 66 : train_loss 5.941551330566406\n",
      "Epoch 66 : test_loss 5.95391943359375\n",
      "saved model\n",
      "Epoch 67 : train_loss 5.942194580078125\n",
      "Epoch 67 : test_loss 5.942050048828125\n",
      "saved model\n",
      "Epoch 68 : train_loss 5.933973022460938\n",
      "Epoch 68 : test_loss 5.9350859375\n",
      "Epoch 69 : train_loss 5.920672424316407\n",
      "Epoch 69 : test_loss 5.947922119140625\n",
      "Epoch 70 : train_loss 5.915313110351563\n",
      "Epoch 70 : test_loss 5.9483349609375\n",
      "saved model\n",
      "Epoch 71 : train_loss 5.926618835449219\n",
      "Epoch 71 : test_loss 5.934869140625\n",
      "Epoch 72 : train_loss 5.922016784667969\n",
      "Epoch 72 : test_loss 5.9463134765625\n",
      "saved model\n",
      "Epoch 73 : train_loss 5.909675170898438\n",
      "Epoch 73 : test_loss 5.91309619140625\n",
      "Epoch 74 : train_loss 5.90856201171875\n",
      "Epoch 74 : test_loss 5.921912109375\n",
      "saved model\n",
      "Epoch 75 : train_loss 5.909217224121094\n",
      "Epoch 75 : test_loss 5.901041259765625\n",
      "Epoch 76 : train_loss 5.899559692382812\n",
      "Epoch 76 : test_loss 5.9142890625\n",
      "Epoch 77 : train_loss 5.89700439453125\n",
      "Epoch 77 : test_loss 5.912102294921875\n",
      "Epoch 78 : train_loss 5.905277404785156\n",
      "Epoch 78 : test_loss 5.909136962890625\n",
      "saved model\n",
      "Epoch 79 : train_loss 5.893650268554688\n",
      "Epoch 79 : test_loss 5.891922607421875\n",
      "Epoch 80 : train_loss 5.8843777465820315\n",
      "Epoch 80 : test_loss 5.9191416015625\n",
      "Epoch 81 : train_loss 5.877133972167969\n",
      "Epoch 81 : test_loss 5.900446533203125\n",
      "Epoch 82 : train_loss 5.8958438110351565\n",
      "Epoch 82 : test_loss 5.90028515625\n",
      "Epoch 83 : train_loss 5.892825622558593\n",
      "Epoch 83 : test_loss 5.895085205078125\n",
      "saved model\n",
      "Epoch 84 : train_loss 5.877991394042969\n",
      "Epoch 84 : test_loss 5.88708740234375\n",
      "saved model\n",
      "Epoch 85 : train_loss 5.8729453125\n",
      "Epoch 85 : test_loss 5.87281689453125\n",
      "Epoch 86 : train_loss 5.863523864746094\n",
      "Epoch 86 : test_loss 5.873224609375\n",
      "saved model\n",
      "Epoch 87 : train_loss 5.8845587158203125\n",
      "Epoch 87 : test_loss 5.860841064453125\n",
      "Epoch 88 : train_loss 5.868568176269531\n",
      "Epoch 88 : test_loss 5.8928447265625\n",
      "Epoch 89 : train_loss 5.873100830078125\n",
      "Epoch 89 : test_loss 5.865740234375\n",
      "Epoch 90 : train_loss 5.869865844726562\n",
      "Epoch 90 : test_loss 5.86988818359375\n",
      "Epoch 91 : train_loss 5.854356506347656\n",
      "Epoch 91 : test_loss 5.868318359375\n",
      "Epoch 92 : train_loss 5.855020874023437\n",
      "Epoch 92 : test_loss 5.8781005859375\n",
      "saved model\n",
      "Epoch 93 : train_loss 5.860406066894531\n",
      "Epoch 93 : test_loss 5.85154541015625\n",
      "saved model\n",
      "Epoch 94 : train_loss 5.856239868164063\n",
      "Epoch 94 : test_loss 5.845510498046875\n",
      "saved model\n",
      "Epoch 95 : train_loss 5.865448181152344\n",
      "Epoch 95 : test_loss 5.826361572265625\n",
      "Epoch 96 : train_loss 5.86967724609375\n",
      "Epoch 96 : test_loss 5.830369140625\n",
      "Epoch 97 : train_loss 5.859307006835937\n",
      "Epoch 97 : test_loss 5.841420166015625\n",
      "Epoch 98 : train_loss 5.85042724609375\n",
      "Epoch 98 : test_loss 5.84155615234375\n",
      "saved model\n",
      "Epoch 99 : train_loss 5.836592102050782\n",
      "Epoch 99 : test_loss 5.8257880859375\n",
      "Epoch 100 : train_loss 5.849906433105469\n",
      "Epoch 100 : test_loss 5.833886474609375\n",
      "Epoch 101 : train_loss 5.845482971191406\n",
      "Epoch 101 : test_loss 5.847909423828125\n",
      "Epoch 102 : train_loss 5.830693359375\n",
      "Epoch 102 : test_loss 5.842791015625\n",
      "Epoch 103 : train_loss 5.824354919433594\n",
      "Epoch 103 : test_loss 5.83450439453125\n",
      "saved model\n",
      "Epoch 104 : train_loss 5.831679382324219\n",
      "Epoch 104 : test_loss 5.8226708984375\n",
      "saved model\n",
      "Epoch 105 : train_loss 5.820322998046875\n",
      "Epoch 105 : test_loss 5.818927734375\n",
      "saved model\n",
      "Epoch 106 : train_loss 5.820381103515625\n",
      "Epoch 106 : test_loss 5.80644580078125\n",
      "Epoch 107 : train_loss 5.834116088867187\n",
      "Epoch 107 : test_loss 5.811739990234375\n",
      "Epoch 108 : train_loss 5.819652587890625\n",
      "Epoch 108 : test_loss 5.81861181640625\n",
      "saved model\n",
      "Epoch 109 : train_loss 5.809087158203125\n",
      "Epoch 109 : test_loss 5.802599365234375\n",
      "Epoch 110 : train_loss 5.805091735839844\n",
      "Epoch 110 : test_loss 5.8206767578125\n",
      "saved model\n",
      "Epoch 111 : train_loss 5.810702880859375\n",
      "Epoch 111 : test_loss 5.8023134765625\n",
      "saved model\n",
      "Epoch 112 : train_loss 5.804856323242188\n",
      "Epoch 112 : test_loss 5.791669921875\n",
      "saved model\n",
      "Epoch 113 : train_loss 5.817449462890625\n",
      "Epoch 113 : test_loss 5.789353271484375\n",
      "Epoch 114 : train_loss 5.799996459960938\n",
      "Epoch 114 : test_loss 5.801282470703125\n",
      "Epoch 115 : train_loss 5.794994079589844\n",
      "Epoch 115 : test_loss 5.80668896484375\n",
      "Epoch 116 : train_loss 5.796388061523437\n",
      "Epoch 116 : test_loss 5.80358056640625\n",
      "Epoch 117 : train_loss 5.7838111572265625\n",
      "Epoch 117 : test_loss 5.80596337890625\n",
      "Epoch 118 : train_loss 5.7857032470703125\n",
      "Epoch 118 : test_loss 5.819072265625\n",
      "Epoch 119 : train_loss 5.785935791015625\n",
      "Epoch 119 : test_loss 5.7970927734375\n",
      "Epoch 120 : train_loss 5.776732421875\n",
      "Epoch 120 : test_loss 5.79452001953125\n",
      "Epoch 121 : train_loss 5.774072143554688\n",
      "Epoch 121 : test_loss 5.793862548828125\n",
      "saved model\n",
      "Epoch 122 : train_loss 5.765684936523438\n",
      "Epoch 122 : test_loss 5.7864033203125\n",
      "saved model\n",
      "Epoch 123 : train_loss 5.778290832519532\n",
      "Epoch 123 : test_loss 5.779999267578125\n",
      "Epoch 124 : train_loss 5.771770690917969\n",
      "Epoch 124 : test_loss 5.78196240234375\n",
      "saved model\n",
      "Epoch 125 : train_loss 5.763481567382812\n",
      "Epoch 125 : test_loss 5.77916796875\n",
      "Epoch 126 : train_loss 5.761355590820313\n",
      "Epoch 126 : test_loss 5.779537841796875\n",
      "saved model\n",
      "Epoch 127 : train_loss 5.766629455566406\n",
      "Epoch 127 : test_loss 5.760114990234375\n",
      "Epoch 128 : train_loss 5.75692626953125\n",
      "Epoch 128 : test_loss 5.78247607421875\n",
      "saved model\n",
      "Epoch 129 : train_loss 5.749181274414062\n",
      "Epoch 129 : test_loss 5.756618896484375\n",
      "Epoch 130 : train_loss 5.742781372070312\n",
      "Epoch 130 : test_loss 5.7601689453125\n",
      "Epoch 131 : train_loss 5.757031555175781\n",
      "Epoch 131 : test_loss 5.75668994140625\n",
      "saved model\n",
      "Epoch 132 : train_loss 5.73429931640625\n",
      "Epoch 132 : test_loss 5.731576171875\n",
      "Epoch 133 : train_loss 5.723795776367187\n",
      "Epoch 133 : test_loss 5.748933349609375\n",
      "Epoch 134 : train_loss 5.732568664550781\n",
      "Epoch 134 : test_loss 5.738941162109375\n",
      "Epoch 135 : train_loss 5.720362548828125\n",
      "Epoch 135 : test_loss 5.73931591796875\n",
      "Epoch 136 : train_loss 5.721113586425782\n",
      "Epoch 136 : test_loss 5.731626220703125\n",
      "Epoch 137 : train_loss 5.7202041015625\n",
      "Epoch 137 : test_loss 5.73894970703125\n",
      "Epoch 138 : train_loss 5.709697631835938\n",
      "Epoch 138 : test_loss 5.736526611328125\n",
      "Epoch 139 : train_loss 5.713286804199218\n",
      "Epoch 139 : test_loss 5.733410400390625\n",
      "Epoch 140 : train_loss 5.692434326171875\n",
      "Epoch 140 : test_loss 5.7326767578125\n",
      "saved model\n",
      "Epoch 141 : train_loss 5.697456481933593\n",
      "Epoch 141 : test_loss 5.72282568359375\n",
      "saved model\n",
      "Epoch 142 : train_loss 5.701237670898437\n",
      "Epoch 142 : test_loss 5.714884765625\n",
      "Epoch 143 : train_loss 5.681584838867187\n",
      "Epoch 143 : test_loss 5.728812744140625\n",
      "saved model\n",
      "Epoch 144 : train_loss 5.69596435546875\n",
      "Epoch 144 : test_loss 5.692197509765625\n",
      "Epoch 145 : train_loss 5.676265563964844\n",
      "Epoch 145 : test_loss 5.6988486328125\n",
      "Epoch 146 : train_loss 5.686697021484375\n",
      "Epoch 146 : test_loss 5.708771728515625\n",
      "Epoch 147 : train_loss 5.66587646484375\n",
      "Epoch 147 : test_loss 5.69829541015625\n",
      "Epoch 148 : train_loss 5.669426147460937\n",
      "Epoch 148 : test_loss 5.711845947265625\n",
      "saved model\n",
      "Epoch 149 : train_loss 5.6678467407226565\n",
      "Epoch 149 : test_loss 5.67995703125\n",
      "Epoch 150 : train_loss 5.658807983398438\n",
      "Epoch 150 : test_loss 5.698380615234375\n",
      "Epoch 151 : train_loss 5.681372192382812\n",
      "Epoch 151 : test_loss 5.700369384765625\n",
      "saved model\n",
      "Epoch 152 : train_loss 5.64699169921875\n",
      "Epoch 152 : test_loss 5.678603759765625\n",
      "Epoch 153 : train_loss 5.658839111328125\n",
      "Epoch 153 : test_loss 5.6838818359375\n",
      "saved model\n",
      "Epoch 154 : train_loss 5.649237854003906\n",
      "Epoch 154 : test_loss 5.6708642578125\n",
      "saved model\n",
      "Epoch 155 : train_loss 5.641299560546875\n",
      "Epoch 155 : test_loss 5.6597119140625\n",
      "Epoch 156 : train_loss 5.638636779785156\n",
      "Epoch 156 : test_loss 5.678450927734375\n",
      "Epoch 157 : train_loss 5.626137817382813\n",
      "Epoch 157 : test_loss 5.67590283203125\n",
      "Epoch 158 : train_loss 5.639795837402343\n",
      "Epoch 158 : test_loss 5.6642451171875\n",
      "Epoch 159 : train_loss 5.630188842773437\n",
      "Epoch 159 : test_loss 5.662296875\n",
      "saved model\n",
      "Epoch 160 : train_loss 5.617825073242187\n",
      "Epoch 160 : test_loss 5.63945947265625\n",
      "Epoch 161 : train_loss 5.603997314453125\n",
      "Epoch 161 : test_loss 5.66031298828125\n",
      "Epoch 162 : train_loss 5.608463500976563\n",
      "Epoch 162 : test_loss 5.64635205078125\n",
      "saved model\n",
      "Epoch 163 : train_loss 5.5937421875\n",
      "Epoch 163 : test_loss 5.6373359375\n",
      "Epoch 164 : train_loss 5.596349731445312\n",
      "Epoch 164 : test_loss 5.64786376953125\n",
      "Epoch 165 : train_loss 5.601392578125\n",
      "Epoch 165 : test_loss 5.6502822265625\n",
      "Epoch 166 : train_loss 5.591981750488281\n",
      "Epoch 166 : test_loss 5.65709716796875\n",
      "saved model\n",
      "Epoch 167 : train_loss 5.601272155761719\n",
      "Epoch 167 : test_loss 5.630255615234375\n",
      "saved model\n",
      "Epoch 168 : train_loss 5.589527221679687\n",
      "Epoch 168 : test_loss 5.627112548828125\n",
      "saved model\n",
      "Epoch 169 : train_loss 5.5747060546875\n",
      "Epoch 169 : test_loss 5.626892333984375\n",
      "saved model\n",
      "Epoch 170 : train_loss 5.56349951171875\n",
      "Epoch 170 : test_loss 5.613030517578125\n",
      "Epoch 171 : train_loss 5.578849853515625\n",
      "Epoch 171 : test_loss 5.62566357421875\n",
      "Epoch 172 : train_loss 5.57405810546875\n",
      "Epoch 172 : test_loss 5.63411767578125\n",
      "Epoch 173 : train_loss 5.557696838378906\n",
      "Epoch 173 : test_loss 5.62615087890625\n",
      "saved model\n",
      "Epoch 174 : train_loss 5.564395385742188\n",
      "Epoch 174 : test_loss 5.5900498046875\n",
      "Epoch 175 : train_loss 5.56064404296875\n",
      "Epoch 175 : test_loss 5.59329296875\n",
      "saved model\n",
      "Epoch 176 : train_loss 5.544191162109375\n",
      "Epoch 176 : test_loss 5.58121142578125\n",
      "Epoch 177 : train_loss 5.54066162109375\n",
      "Epoch 177 : test_loss 5.6045439453125\n",
      "saved model\n",
      "Epoch 178 : train_loss 5.549356140136719\n",
      "Epoch 178 : test_loss 5.58072265625\n",
      "Epoch 179 : train_loss 5.544919738769531\n",
      "Epoch 179 : test_loss 5.5878505859375\n",
      "Epoch 180 : train_loss 5.545589477539062\n",
      "Epoch 180 : test_loss 5.60139794921875\n",
      "Epoch 181 : train_loss 5.5236378784179685\n",
      "Epoch 181 : test_loss 5.5941474609375\n",
      "Epoch 182 : train_loss 5.528040405273438\n",
      "Epoch 182 : test_loss 5.58634619140625\n",
      "saved model\n",
      "Epoch 183 : train_loss 5.538417846679687\n",
      "Epoch 183 : test_loss 5.5706826171875\n",
      "Epoch 184 : train_loss 5.508617431640625\n",
      "Epoch 184 : test_loss 5.5717666015625\n",
      "Epoch 185 : train_loss 5.511179382324219\n",
      "Epoch 185 : test_loss 5.593853271484375\n",
      "saved model\n",
      "Epoch 186 : train_loss 5.510199645996094\n",
      "Epoch 186 : test_loss 5.568291015625\n",
      "Epoch 187 : train_loss 5.504328002929688\n",
      "Epoch 187 : test_loss 5.572381103515625\n",
      "Epoch 188 : train_loss 5.508761962890625\n",
      "Epoch 188 : test_loss 5.57175390625\n",
      "saved model\n",
      "Epoch 189 : train_loss 5.4953480834960935\n",
      "Epoch 189 : test_loss 5.550900146484375\n",
      "saved model\n",
      "Epoch 190 : train_loss 5.4925645751953125\n",
      "Epoch 190 : test_loss 5.547815673828125\n",
      "saved model\n",
      "Epoch 191 : train_loss 5.485246704101563\n",
      "Epoch 191 : test_loss 5.5475986328125\n",
      "Epoch 192 : train_loss 5.490096435546875\n",
      "Epoch 192 : test_loss 5.57412060546875\n",
      "saved model\n",
      "Epoch 193 : train_loss 5.470391357421875\n",
      "Epoch 193 : test_loss 5.543039306640625\n",
      "saved model\n",
      "Epoch 194 : train_loss 5.462228881835937\n",
      "Epoch 194 : test_loss 5.528865966796875\n",
      "Epoch 195 : train_loss 5.487189758300781\n",
      "Epoch 195 : test_loss 5.5459111328125\n",
      "Epoch 196 : train_loss 5.461157958984375\n",
      "Epoch 196 : test_loss 5.558007080078125\n",
      "Epoch 197 : train_loss 5.45983154296875\n",
      "Epoch 197 : test_loss 5.549700439453125\n",
      "saved model\n",
      "Epoch 198 : train_loss 5.454131958007813\n",
      "Epoch 198 : test_loss 5.52777294921875\n",
      "Epoch 199 : train_loss 5.462943908691407\n",
      "Epoch 199 : test_loss 5.530879150390625\n",
      "saved model\n",
      "Epoch 200 : train_loss 5.453697692871094\n",
      "Epoch 200 : test_loss 5.511188720703125\n",
      "saved model\n",
      "Epoch 201 : train_loss 5.435171447753906\n",
      "Epoch 201 : test_loss 5.510833740234375\n",
      "Epoch 202 : train_loss 5.432325439453125\n",
      "Epoch 202 : test_loss 5.515227294921875\n",
      "saved model\n",
      "Epoch 203 : train_loss 5.437046875\n",
      "Epoch 203 : test_loss 5.507427001953125\n",
      "saved model\n",
      "Epoch 204 : train_loss 5.435414367675781\n",
      "Epoch 204 : test_loss 5.504178466796875\n",
      "Epoch 205 : train_loss 5.422633972167969\n",
      "Epoch 205 : test_loss 5.5171845703125\n",
      "saved model\n",
      "Epoch 206 : train_loss 5.432371520996094\n",
      "Epoch 206 : test_loss 5.49095263671875\n",
      "Epoch 207 : train_loss 5.431650451660156\n",
      "Epoch 207 : test_loss 5.4944384765625\n",
      "Epoch 208 : train_loss 5.417584411621093\n",
      "Epoch 208 : test_loss 5.497867431640625\n",
      "saved model\n",
      "Epoch 209 : train_loss 5.4254566650390625\n",
      "Epoch 209 : test_loss 5.470959228515625\n",
      "Epoch 210 : train_loss 5.418084838867188\n",
      "Epoch 210 : test_loss 5.494274658203125\n",
      "saved model\n",
      "Epoch 211 : train_loss 5.41166845703125\n",
      "Epoch 211 : test_loss 5.46578076171875\n",
      "Epoch 212 : train_loss 5.386251953125\n",
      "Epoch 212 : test_loss 5.475007080078125\n",
      "Epoch 213 : train_loss 5.370218139648437\n",
      "Epoch 213 : test_loss 5.474721435546875\n",
      "Epoch 214 : train_loss 5.387409973144531\n",
      "Epoch 214 : test_loss 5.466405517578125\n",
      "Epoch 215 : train_loss 5.372786926269531\n",
      "Epoch 215 : test_loss 5.466273681640625\n",
      "Epoch 216 : train_loss 5.378283447265625\n",
      "Epoch 216 : test_loss 5.474796875\n",
      "saved model\n",
      "Epoch 217 : train_loss 5.3742611083984375\n",
      "Epoch 217 : test_loss 5.44809619140625\n",
      "Epoch 218 : train_loss 5.370022888183594\n",
      "Epoch 218 : test_loss 5.4958447265625\n",
      "saved model\n",
      "Epoch 219 : train_loss 5.379333374023438\n",
      "Epoch 219 : test_loss 5.437560546875\n",
      "Epoch 220 : train_loss 5.366238952636719\n",
      "Epoch 220 : test_loss 5.449810302734375\n",
      "Epoch 221 : train_loss 5.3725803833007815\n",
      "Epoch 221 : test_loss 5.450226806640625\n",
      "Epoch 222 : train_loss 5.361638427734375\n",
      "Epoch 222 : test_loss 5.4479375\n",
      "Epoch 223 : train_loss 5.35525\n",
      "Epoch 223 : test_loss 5.45055224609375\n",
      "Epoch 224 : train_loss 5.351154602050781\n",
      "Epoch 224 : test_loss 5.4571708984375\n",
      "saved model\n",
      "Epoch 225 : train_loss 5.363056396484375\n",
      "Epoch 225 : test_loss 5.425164306640625\n",
      "Epoch 226 : train_loss 5.348547241210937\n",
      "Epoch 226 : test_loss 5.428325927734375\n",
      "saved model\n",
      "Epoch 227 : train_loss 5.348654418945313\n",
      "Epoch 227 : test_loss 5.423966552734375\n",
      "Epoch 228 : train_loss 5.339942993164063\n",
      "Epoch 228 : test_loss 5.43643701171875\n",
      "saved model\n",
      "Epoch 229 : train_loss 5.335737670898437\n",
      "Epoch 229 : test_loss 5.40220703125\n",
      "saved model\n",
      "Epoch 230 : train_loss 5.325743591308593\n",
      "Epoch 230 : test_loss 5.401040771484375\n",
      "Epoch 231 : train_loss 5.321871765136719\n",
      "Epoch 231 : test_loss 5.401734375\n",
      "Epoch 232 : train_loss 5.325945068359375\n",
      "Epoch 232 : test_loss 5.4140048828125\n",
      "saved model\n",
      "Epoch 233 : train_loss 5.323071350097656\n",
      "Epoch 233 : test_loss 5.39696337890625\n",
      "saved model\n",
      "Epoch 234 : train_loss 5.317376220703125\n",
      "Epoch 234 : test_loss 5.380297607421875\n",
      "Epoch 235 : train_loss 5.322687133789063\n",
      "Epoch 235 : test_loss 5.391722412109375\n",
      "saved model\n",
      "Epoch 236 : train_loss 5.311195922851563\n",
      "Epoch 236 : test_loss 5.372553466796875\n",
      "saved model\n",
      "Epoch 237 : train_loss 5.313674438476562\n",
      "Epoch 237 : test_loss 5.353926513671875\n",
      "Epoch 238 : train_loss 5.295057983398437\n",
      "Epoch 238 : test_loss 5.39206640625\n",
      "Epoch 239 : train_loss 5.301371704101562\n",
      "Epoch 239 : test_loss 5.385251708984375\n",
      "Epoch 240 : train_loss 5.286172668457032\n",
      "Epoch 240 : test_loss 5.396416259765625\n",
      "Epoch 241 : train_loss 5.283992553710937\n",
      "Epoch 241 : test_loss 5.36160888671875\n",
      "Epoch 242 : train_loss 5.2962801513671875\n",
      "Epoch 242 : test_loss 5.38595947265625\n",
      "Epoch 243 : train_loss 5.270024536132812\n",
      "Epoch 243 : test_loss 5.36012353515625\n",
      "Epoch 244 : train_loss 5.265476318359375\n",
      "Epoch 244 : test_loss 5.362510009765625\n",
      "Epoch 245 : train_loss 5.260030151367188\n",
      "Epoch 245 : test_loss 5.360694091796875\n",
      "Epoch 246 : train_loss 5.285460388183593\n",
      "Epoch 246 : test_loss 5.3613603515625\n",
      "saved model\n",
      "Epoch 247 : train_loss 5.26436767578125\n",
      "Epoch 247 : test_loss 5.3536904296875\n",
      "saved model\n",
      "Epoch 248 : train_loss 5.269015319824219\n",
      "Epoch 248 : test_loss 5.349765625\n",
      "Epoch 249 : train_loss 5.257136474609375\n",
      "Epoch 249 : test_loss 5.35939013671875\n",
      "saved model\n",
      "Epoch 250 : train_loss 5.253213562011719\n",
      "Epoch 250 : test_loss 5.3375322265625\n",
      "saved model\n",
      "Epoch 251 : train_loss 5.245548645019531\n",
      "Epoch 251 : test_loss 5.32113037109375\n",
      "Epoch 252 : train_loss 5.24676171875\n",
      "Epoch 252 : test_loss 5.326316650390625\n",
      "Epoch 253 : train_loss 5.244266723632813\n",
      "Epoch 253 : test_loss 5.331798095703125\n",
      "saved model\n",
      "Epoch 254 : train_loss 5.234657775878906\n",
      "Epoch 254 : test_loss 5.317030029296875\n",
      "Epoch 255 : train_loss 5.252630859375\n",
      "Epoch 255 : test_loss 5.3238740234375\n",
      "saved model\n",
      "Epoch 256 : train_loss 5.234180541992187\n",
      "Epoch 256 : test_loss 5.3109033203125\n",
      "saved model\n",
      "Epoch 257 : train_loss 5.2318814086914065\n",
      "Epoch 257 : test_loss 5.2884580078125\n",
      "saved model\n",
      "Epoch 258 : train_loss 5.233093872070312\n",
      "Epoch 258 : test_loss 5.27927783203125\n",
      "Epoch 259 : train_loss 5.227678588867187\n",
      "Epoch 259 : test_loss 5.30617578125\n",
      "Epoch 260 : train_loss 5.209467407226563\n",
      "Epoch 260 : test_loss 5.2804912109375\n",
      "Epoch 261 : train_loss 5.223724243164063\n",
      "Epoch 261 : test_loss 5.2793154296875\n",
      "Epoch 262 : train_loss 5.2232381591796875\n",
      "Epoch 262 : test_loss 5.285765380859375\n",
      "Epoch 263 : train_loss 5.211284912109375\n",
      "Epoch 263 : test_loss 5.288658447265625\n",
      "Epoch 264 : train_loss 5.20320849609375\n",
      "Epoch 264 : test_loss 5.28768603515625\n",
      "Epoch 265 : train_loss 5.216731201171875\n",
      "Epoch 265 : test_loss 5.287531982421875\n",
      "Epoch 266 : train_loss 5.196153259277343\n",
      "Epoch 266 : test_loss 5.28413525390625\n",
      "Epoch 267 : train_loss 5.202308471679688\n",
      "Epoch 267 : test_loss 5.281983642578125\n",
      "Epoch 268 : train_loss 5.194437561035156\n",
      "Epoch 268 : test_loss 5.281056884765625\n",
      "saved model\n",
      "Epoch 269 : train_loss 5.17584130859375\n",
      "Epoch 269 : test_loss 5.274677490234375\n",
      "saved model\n",
      "Epoch 270 : train_loss 5.188841430664063\n",
      "Epoch 270 : test_loss 5.263435546875\n",
      "saved model\n",
      "Epoch 271 : train_loss 5.1948309326171875\n",
      "Epoch 271 : test_loss 5.260438232421875\n",
      "Epoch 272 : train_loss 5.1708179931640625\n",
      "Epoch 272 : test_loss 5.27346533203125\n",
      "saved model\n",
      "Epoch 273 : train_loss 5.17438720703125\n",
      "Epoch 273 : test_loss 5.232590087890625\n",
      "Epoch 274 : train_loss 5.186583740234375\n",
      "Epoch 274 : test_loss 5.24856982421875\n",
      "Epoch 275 : train_loss 5.170966125488281\n",
      "Epoch 275 : test_loss 5.25475048828125\n",
      "Epoch 276 : train_loss 5.159490173339844\n",
      "Epoch 276 : test_loss 5.2327529296875\n",
      "Epoch 277 : train_loss 5.158511291503906\n",
      "Epoch 277 : test_loss 5.25981591796875\n",
      "saved model\n",
      "Epoch 278 : train_loss 5.169364379882812\n",
      "Epoch 278 : test_loss 5.228172119140625\n",
      "Epoch 279 : train_loss 5.152357177734375\n",
      "Epoch 279 : test_loss 5.23348486328125\n",
      "Epoch 280 : train_loss 5.164496948242188\n",
      "Epoch 280 : test_loss 5.246141845703125\n",
      "Epoch 281 : train_loss 5.145786499023438\n",
      "Epoch 281 : test_loss 5.229640380859375\n",
      "Epoch 282 : train_loss 5.156269592285156\n",
      "Epoch 282 : test_loss 5.234723388671875\n",
      "saved model\n",
      "Epoch 283 : train_loss 5.14601318359375\n",
      "Epoch 283 : test_loss 5.205891357421875\n",
      "Epoch 284 : train_loss 5.137728515625\n",
      "Epoch 284 : test_loss 5.22386669921875\n",
      "Epoch 285 : train_loss 5.14462841796875\n",
      "Epoch 285 : test_loss 5.22128955078125\n",
      "Epoch 286 : train_loss 5.141177978515625\n",
      "Epoch 286 : test_loss 5.220376220703125\n",
      "saved model\n",
      "Epoch 287 : train_loss 5.135189331054687\n",
      "Epoch 287 : test_loss 5.201302001953125\n",
      "saved model\n",
      "Epoch 288 : train_loss 5.129986694335938\n",
      "Epoch 288 : test_loss 5.187000244140625\n",
      "Epoch 289 : train_loss 5.122520141601562\n",
      "Epoch 289 : test_loss 5.203305908203125\n",
      "Epoch 290 : train_loss 5.134023986816406\n",
      "Epoch 290 : test_loss 5.19308984375\n",
      "Epoch 291 : train_loss 5.133178466796875\n",
      "Epoch 291 : test_loss 5.18718896484375\n",
      "Epoch 292 : train_loss 5.128777587890625\n",
      "Epoch 292 : test_loss 5.199095458984375\n",
      "Epoch 293 : train_loss 5.108435241699219\n",
      "Epoch 293 : test_loss 5.209700439453125\n",
      "saved model\n",
      "Epoch 294 : train_loss 5.107144592285156\n",
      "Epoch 294 : test_loss 5.17840673828125\n",
      "Epoch 295 : train_loss 5.110102355957031\n",
      "Epoch 295 : test_loss 5.179388916015625\n",
      "Epoch 296 : train_loss 5.125427856445312\n",
      "Epoch 296 : test_loss 5.17853857421875\n",
      "saved model\n",
      "Epoch 297 : train_loss 5.09438818359375\n",
      "Epoch 297 : test_loss 5.167515380859375\n",
      "Epoch 298 : train_loss 5.099835083007813\n",
      "Epoch 298 : test_loss 5.177904296875\n",
      "saved model\n",
      "Epoch 299 : train_loss 5.09709912109375\n",
      "Epoch 299 : test_loss 5.167254150390625\n",
      "Epoch 300 : train_loss 5.088459838867188\n",
      "Epoch 300 : test_loss 5.174267578125\n",
      "saved model\n",
      "Epoch 301 : train_loss 5.098948059082031\n",
      "Epoch 301 : test_loss 5.1538203125\n",
      "saved model\n",
      "Epoch 302 : train_loss 5.070513061523437\n",
      "Epoch 302 : test_loss 5.1527734375\n",
      "saved model\n",
      "Epoch 303 : train_loss 5.086487609863282\n",
      "Epoch 303 : test_loss 5.1191123046875\n",
      "Epoch 304 : train_loss 5.0776498413085935\n",
      "Epoch 304 : test_loss 5.15702001953125\n",
      "Epoch 305 : train_loss 5.0839453125\n",
      "Epoch 305 : test_loss 5.154610107421875\n",
      "Epoch 306 : train_loss 5.084741943359375\n",
      "Epoch 306 : test_loss 5.124980712890625\n",
      "Epoch 307 : train_loss 5.060808349609375\n",
      "Epoch 307 : test_loss 5.142916259765625\n",
      "Epoch 308 : train_loss 5.069096069335938\n",
      "Epoch 308 : test_loss 5.128478515625\n",
      "Epoch 309 : train_loss 5.068622436523437\n",
      "Epoch 309 : test_loss 5.1235634765625\n",
      "Epoch 310 : train_loss 5.054537719726563\n",
      "Epoch 310 : test_loss 5.13701513671875\n",
      "Epoch 311 : train_loss 5.06652294921875\n",
      "Epoch 311 : test_loss 5.141391357421875\n",
      "Epoch 312 : train_loss 5.038890197753906\n",
      "Epoch 312 : test_loss 5.12132177734375\n",
      "Epoch 313 : train_loss 5.067639404296875\n",
      "Epoch 313 : test_loss 5.13361474609375\n",
      "Epoch 314 : train_loss 5.045760375976562\n",
      "Epoch 314 : test_loss 5.126162109375\n",
      "saved model\n",
      "Epoch 315 : train_loss 5.059999755859375\n",
      "Epoch 315 : test_loss 5.117302978515625\n",
      "Epoch 316 : train_loss 5.043821838378906\n",
      "Epoch 316 : test_loss 5.1202373046875\n",
      "Epoch 317 : train_loss 5.044587524414062\n",
      "Epoch 317 : test_loss 5.119920532226563\n",
      "saved model\n",
      "Epoch 318 : train_loss 5.037754028320313\n",
      "Epoch 318 : test_loss 5.112855834960937\n",
      "saved model\n",
      "Epoch 319 : train_loss 5.0574622802734375\n",
      "Epoch 319 : test_loss 5.10208544921875\n",
      "Epoch 320 : train_loss 5.054701416015625\n",
      "Epoch 320 : test_loss 5.102918212890625\n",
      "Epoch 321 : train_loss 5.053897888183593\n",
      "Epoch 321 : test_loss 5.110548950195312\n",
      "Epoch 322 : train_loss 5.047081787109375\n",
      "Epoch 322 : test_loss 5.108279052734375\n",
      "saved model\n",
      "Epoch 323 : train_loss 5.046535766601562\n",
      "Epoch 323 : test_loss 5.09958349609375\n",
      "Epoch 324 : train_loss 5.049164184570312\n",
      "Epoch 324 : test_loss 5.12951953125\n",
      "Epoch 325 : train_loss 5.0381992797851565\n",
      "Epoch 325 : test_loss 5.102617065429688\n",
      "Epoch 326 : train_loss 5.040236511230469\n",
      "Epoch 326 : test_loss 5.116037841796875\n",
      "Epoch 327 : train_loss 5.049424072265625\n",
      "Epoch 327 : test_loss 5.119497314453125\n",
      "saved model\n",
      "Epoch 328 : train_loss 5.038373657226563\n",
      "Epoch 328 : test_loss 5.097576538085938\n",
      "saved model\n",
      "Epoch 329 : train_loss 5.043350524902344\n",
      "Epoch 329 : test_loss 5.087341918945312\n",
      "Epoch 330 : train_loss 5.051273803710938\n",
      "Epoch 330 : test_loss 5.101367309570312\n",
      "Epoch 331 : train_loss 5.059066833496094\n",
      "Epoch 331 : test_loss 5.11257177734375\n",
      "Epoch 332 : train_loss 5.028713134765625\n",
      "Epoch 332 : test_loss 5.114895263671875\n",
      "Epoch 333 : train_loss 5.045514038085938\n",
      "Epoch 333 : test_loss 5.114173706054688\n",
      "Epoch 334 : train_loss 5.054396728515625\n",
      "Epoch 334 : test_loss 5.115251220703125\n",
      "Epoch 335 : train_loss 5.04250341796875\n",
      "Epoch 335 : test_loss 5.099001586914063\n",
      "Epoch 336 : train_loss 5.059959716796875\n",
      "Epoch 336 : test_loss 5.1112314453125\n",
      "Epoch 337 : train_loss 5.0505567626953125\n",
      "Epoch 337 : test_loss 5.104716064453125\n",
      "Epoch 338 : train_loss 5.043442138671875\n",
      "Epoch 338 : test_loss 5.111890380859375\n",
      "Epoch 339 : train_loss 5.042054321289062\n",
      "Epoch 339 : test_loss 5.09896875\n",
      "Epoch 340 : train_loss 5.047453979492188\n",
      "Epoch 340 : test_loss 5.135846923828125\n",
      "Epoch 341 : train_loss 5.045826232910156\n",
      "Epoch 341 : test_loss 5.116138916015625\n",
      "Epoch 342 : train_loss 5.05592041015625\n",
      "Epoch 342 : test_loss 5.09324169921875\n",
      "Epoch 343 : train_loss 5.0400322265625\n",
      "Epoch 343 : test_loss 5.091795776367188\n",
      "Epoch 344 : train_loss 5.058734497070312\n",
      "Epoch 344 : test_loss 5.112688842773437\n",
      "Epoch 345 : train_loss 5.038649169921875\n",
      "Epoch 345 : test_loss 5.114449462890625\n",
      "Epoch 346 : train_loss 5.036197021484375\n",
      "Epoch 346 : test_loss 5.128523193359375\n",
      "Epoch 347 : train_loss 5.056471801757812\n",
      "Epoch 347 : test_loss 5.094155395507813\n",
      "Epoch 348 : train_loss 5.037376342773437\n",
      "Epoch 348 : test_loss 5.117425048828125\n",
      "Epoch 349 : train_loss 5.044868896484375\n",
      "Epoch 349 : test_loss 5.112079345703125\n",
      "Epoch 350 : train_loss 5.048975036621094\n",
      "Epoch 350 : test_loss 5.12331787109375\n",
      "Epoch 351 : train_loss 5.036521606445312\n",
      "Epoch 351 : test_loss 5.111327880859375\n",
      "Epoch 352 : train_loss 5.045508911132813\n",
      "Epoch 352 : test_loss 5.12136474609375\n",
      "Epoch 353 : train_loss 5.0327518310546875\n",
      "Epoch 353 : test_loss 5.10631591796875\n",
      "Epoch 354 : train_loss 5.036397888183593\n",
      "Epoch 354 : test_loss 5.114161376953125\n",
      "Epoch 355 : train_loss 5.037482604980469\n",
      "Epoch 355 : test_loss 5.094601928710937\n",
      "Epoch 356 : train_loss 5.035884033203125\n",
      "Epoch 356 : test_loss 5.1095205078125\n",
      "Epoch 357 : train_loss 5.045489501953125\n",
      "Epoch 357 : test_loss 5.104290649414063\n",
      "Epoch 358 : train_loss 5.053125\n",
      "Epoch 358 : test_loss 5.114144653320312\n",
      "Epoch 359 : train_loss 5.0465810546875\n",
      "Epoch 359 : test_loss 5.12215185546875\n",
      "Epoch 360 : train_loss 5.048152099609375\n",
      "Epoch 360 : test_loss 5.100077392578125\n",
      "Epoch 361 : train_loss 5.04366455078125\n",
      "Epoch 361 : test_loss 5.088921142578125\n",
      "Epoch 362 : train_loss 5.036601196289062\n",
      "Epoch 362 : test_loss 5.09391845703125\n",
      "Epoch 363 : train_loss 5.037609313964844\n",
      "Epoch 363 : test_loss 5.097920776367188\n",
      "Epoch 364 : train_loss 5.036409729003906\n",
      "Epoch 364 : test_loss 5.112562744140625\n",
      "Epoch 365 : train_loss 5.052812255859375\n",
      "Epoch 365 : test_loss 5.1237998046875\n",
      "Epoch 366 : train_loss 5.036303588867187\n",
      "Epoch 366 : test_loss 5.094663330078125\n",
      "Epoch 367 : train_loss 5.043297912597656\n",
      "Epoch 367 : test_loss 5.1183125\n",
      "Epoch 368 : train_loss 5.04349755859375\n",
      "Epoch 368 : test_loss 5.10624462890625\n",
      "Epoch 369 : train_loss 5.044114379882813\n",
      "Epoch 369 : test_loss 5.1056015625\n",
      "Epoch 370 : train_loss 5.033028686523438\n",
      "Epoch 370 : test_loss 5.104254638671875\n",
      "Epoch 371 : train_loss 5.037614868164063\n",
      "Epoch 371 : test_loss 5.087577514648437\n",
      "Epoch 372 : train_loss 5.038208129882812\n",
      "Epoch 372 : test_loss 5.112161376953125\n",
      "Epoch 373 : train_loss 5.050230224609375\n",
      "Epoch 373 : test_loss 5.110030395507812\n",
      "Epoch 374 : train_loss 5.055323181152343\n",
      "Epoch 374 : test_loss 5.113708984375\n",
      "Epoch 375 : train_loss 5.0536220703125\n",
      "Epoch 375 : test_loss 5.110784301757812\n",
      "Epoch 376 : train_loss 5.0225654907226565\n",
      "Epoch 376 : test_loss 5.12288623046875\n",
      "Epoch 377 : train_loss 5.029435791015625\n",
      "Epoch 377 : test_loss 5.129272705078125\n",
      "Epoch 378 : train_loss 5.049058715820313\n",
      "Epoch 378 : test_loss 5.1292490234375\n",
      "Epoch 379 : train_loss 5.054210144042969\n",
      "Epoch 379 : test_loss 5.101943115234375\n",
      "Epoch 380 : train_loss 5.05267236328125\n",
      "Epoch 380 : test_loss 5.093962524414063\n",
      "Epoch 381 : train_loss 5.04906884765625\n",
      "Epoch 381 : test_loss 5.101503662109375\n",
      "Epoch 382 : train_loss 5.042431762695313\n",
      "Epoch 382 : test_loss 5.12165966796875\n",
      "Epoch 383 : train_loss 5.041280700683593\n",
      "Epoch 383 : test_loss 5.09620263671875\n",
      "Epoch 384 : train_loss 5.043974304199219\n",
      "Epoch 384 : test_loss 5.099972412109375\n",
      "Epoch 385 : train_loss 5.044636474609375\n",
      "Epoch 385 : test_loss 5.09943798828125\n",
      "saved model\n",
      "Epoch 386 : train_loss 5.033992431640625\n",
      "Epoch 386 : test_loss 5.0813369140625\n",
      "Epoch 387 : train_loss 5.03543505859375\n",
      "Epoch 387 : test_loss 5.113841552734375\n",
      "Epoch 388 : train_loss 5.042209411621093\n",
      "Epoch 388 : test_loss 5.100873657226563\n",
      "Epoch 389 : train_loss 5.038191528320312\n",
      "Epoch 389 : test_loss 5.108608764648437\n",
      "Epoch 390 : train_loss 5.042812744140625\n",
      "Epoch 390 : test_loss 5.1214140625\n",
      "Epoch 391 : train_loss 5.046078979492187\n",
      "Epoch 391 : test_loss 5.087693603515625\n",
      "Epoch 392 : train_loss 5.049399963378907\n",
      "Epoch 392 : test_loss 5.097294677734375\n",
      "Epoch 393 : train_loss 5.037074951171875\n",
      "Epoch 393 : test_loss 5.08786767578125\n",
      "Epoch 394 : train_loss 5.038415893554688\n",
      "Epoch 394 : test_loss 5.081977661132813\n",
      "Epoch 395 : train_loss 5.042736755371093\n",
      "Epoch 395 : test_loss 5.100786010742188\n",
      "Epoch 396 : train_loss 5.044234802246094\n",
      "Epoch 396 : test_loss 5.11149951171875\n",
      "Epoch 397 : train_loss 5.030458190917969\n",
      "Epoch 397 : test_loss 5.11637890625\n",
      "Epoch 398 : train_loss 5.038049926757813\n",
      "Epoch 398 : test_loss 5.100919189453125\n",
      "Epoch 399 : train_loss 5.033396057128906\n",
      "Epoch 399 : test_loss 5.09930517578125\n",
      "Epoch 400 : train_loss 5.034837707519531\n",
      "Epoch 400 : test_loss 5.135080322265625\n",
      "Epoch 401 : train_loss 5.0402861328125\n",
      "Epoch 401 : test_loss 5.097746337890625\n",
      "Epoch 402 : train_loss 5.046636962890625\n",
      "Epoch 402 : test_loss 5.110874755859375\n",
      "Epoch 403 : train_loss 5.042585815429687\n",
      "Epoch 403 : test_loss 5.106513671875\n",
      "Epoch 404 : train_loss 5.0528282470703125\n",
      "Epoch 404 : test_loss 5.106808837890625\n",
      "Epoch 405 : train_loss 5.031988830566406\n",
      "Epoch 405 : test_loss 5.101435668945313\n",
      "Epoch 406 : train_loss 5.032629333496094\n",
      "Epoch 406 : test_loss 5.095209228515625\n",
      "Epoch 407 : train_loss 5.039092224121093\n",
      "Epoch 407 : test_loss 5.093247924804688\n",
      "Epoch 408 : train_loss 5.049205322265625\n",
      "Epoch 408 : test_loss 5.109857788085938\n",
      "Epoch 409 : train_loss 5.03019091796875\n",
      "Epoch 409 : test_loss 5.1175966796875\n",
      "Epoch 410 : train_loss 5.03511083984375\n",
      "Epoch 410 : test_loss 5.102358154296875\n",
      "Epoch 411 : train_loss 5.034605102539063\n",
      "Epoch 411 : test_loss 5.11875390625\n",
      "Epoch 412 : train_loss 5.023124145507812\n",
      "Epoch 412 : test_loss 5.1104384765625\n",
      "Epoch 413 : train_loss 5.049833862304688\n",
      "Epoch 413 : test_loss 5.116088134765625\n",
      "Epoch 414 : train_loss 5.033653442382812\n",
      "Epoch 414 : test_loss 5.103894165039063\n",
      "Epoch 415 : train_loss 5.034516052246094\n",
      "Epoch 415 : test_loss 5.089489868164063\n",
      "Epoch 416 : train_loss 5.0359584350585935\n",
      "Epoch 416 : test_loss 5.12847607421875\n",
      "Epoch 417 : train_loss 5.034363525390625\n",
      "Epoch 417 : test_loss 5.106209228515625\n",
      "Epoch 418 : train_loss 5.027749145507813\n",
      "Epoch 418 : test_loss 5.110575439453125\n",
      "Epoch 419 : train_loss 5.030928283691407\n",
      "Epoch 419 : test_loss 5.10813916015625\n",
      "Epoch 420 : train_loss 5.0418419189453125\n",
      "Epoch 420 : test_loss 5.109640258789063\n",
      "Epoch 421 : train_loss 5.043689086914062\n",
      "Epoch 421 : test_loss 5.1062294921875\n",
      "Epoch 422 : train_loss 5.0427706298828125\n",
      "Epoch 422 : test_loss 5.107487670898437\n",
      "Epoch 423 : train_loss 5.052853393554687\n",
      "Epoch 423 : test_loss 5.1085537109375\n",
      "Epoch 424 : train_loss 5.03214794921875\n",
      "Epoch 424 : test_loss 5.117299682617188\n",
      "Epoch 425 : train_loss 5.041809814453125\n",
      "Epoch 425 : test_loss 5.1309580078125\n",
      "saved model\n",
      "Epoch 426 : train_loss 5.04016064453125\n",
      "Epoch 426 : test_loss 5.07073388671875\n",
      "Epoch 427 : train_loss 5.049970703125\n",
      "Epoch 427 : test_loss 5.074960205078125\n",
      "Epoch 428 : train_loss 5.04642236328125\n",
      "Epoch 428 : test_loss 5.0806690673828125\n",
      "Epoch 429 : train_loss 5.0442590942382814\n",
      "Epoch 429 : test_loss 5.10030029296875\n",
      "Epoch 430 : train_loss 5.049862426757812\n",
      "Epoch 430 : test_loss 5.104149169921875\n",
      "Epoch 431 : train_loss 5.038594482421875\n",
      "Epoch 431 : test_loss 5.109990478515625\n",
      "Epoch 432 : train_loss 5.03787744140625\n",
      "Epoch 432 : test_loss 5.091285522460938\n",
      "Epoch 433 : train_loss 5.043565185546875\n",
      "Epoch 433 : test_loss 5.107801513671875\n",
      "Epoch 434 : train_loss 5.035856689453125\n",
      "Epoch 434 : test_loss 5.11467626953125\n",
      "Epoch 435 : train_loss 5.0464193115234375\n",
      "Epoch 435 : test_loss 5.103256103515625\n",
      "Epoch 436 : train_loss 5.0377064819335935\n",
      "Epoch 436 : test_loss 5.08427734375\n",
      "Epoch 437 : train_loss 5.027528991699219\n",
      "Epoch 437 : test_loss 5.10280029296875\n",
      "Epoch 438 : train_loss 5.029189270019531\n",
      "Epoch 438 : test_loss 5.1031962890625\n",
      "Epoch 439 : train_loss 5.053736450195313\n",
      "Epoch 439 : test_loss 5.105811279296875\n",
      "Epoch 440 : train_loss 5.041971557617187\n",
      "Epoch 440 : test_loss 5.111391357421875\n",
      "Epoch 441 : train_loss 5.033901245117187\n",
      "Epoch 441 : test_loss 5.105944091796875\n",
      "Epoch 442 : train_loss 5.04503564453125\n",
      "Epoch 442 : test_loss 5.1094404296875\n",
      "Epoch 443 : train_loss 5.037051025390625\n",
      "Epoch 443 : test_loss 5.109555908203125\n",
      "Epoch 444 : train_loss 5.045224914550781\n",
      "Epoch 444 : test_loss 5.102864135742188\n",
      "Epoch 445 : train_loss 5.04790771484375\n",
      "Epoch 445 : test_loss 5.107225341796875\n",
      "Epoch 446 : train_loss 5.036275817871093\n",
      "Epoch 446 : test_loss 5.124382202148437\n",
      "Epoch 447 : train_loss 5.034561645507813\n",
      "Epoch 447 : test_loss 5.095439208984375\n",
      "Epoch 448 : train_loss 5.038965209960938\n",
      "Epoch 448 : test_loss 5.0905433349609375\n",
      "Epoch 449 : train_loss 5.038589050292969\n",
      "Epoch 449 : test_loss 5.1257490234375\n",
      "Epoch 450 : train_loss 5.046703918457031\n",
      "Epoch 450 : test_loss 5.10856201171875\n",
      "Epoch 451 : train_loss 5.038777893066406\n",
      "Epoch 451 : test_loss 5.081603759765625\n",
      "Epoch 452 : train_loss 5.022881225585937\n",
      "Epoch 452 : test_loss 5.100523681640625\n",
      "Epoch 453 : train_loss 5.0326886596679685\n",
      "Epoch 453 : test_loss 5.112837158203125\n",
      "Epoch 454 : train_loss 5.0360146484375\n",
      "Epoch 454 : test_loss 5.10005224609375\n",
      "Epoch 455 : train_loss 5.0230859375\n",
      "Epoch 455 : test_loss 5.117447265625\n",
      "Epoch 456 : train_loss 5.038869995117188\n",
      "Epoch 456 : test_loss 5.096344848632812\n",
      "Epoch 457 : train_loss 5.0466634521484375\n",
      "Epoch 457 : test_loss 5.123955810546875\n",
      "Epoch 458 : train_loss 5.049120239257812\n",
      "Epoch 458 : test_loss 5.118346557617188\n",
      "Epoch 459 : train_loss 5.037871765136718\n",
      "Epoch 459 : test_loss 5.098948486328125\n",
      "Epoch 460 : train_loss 5.028312255859375\n",
      "Epoch 460 : test_loss 5.0919931640625\n",
      "Epoch 461 : train_loss 5.049417358398437\n",
      "Epoch 461 : test_loss 5.08393017578125\n",
      "Epoch 462 : train_loss 5.042892944335938\n",
      "Epoch 462 : test_loss 5.089883056640625\n",
      "Epoch 463 : train_loss 5.04174853515625\n",
      "Epoch 463 : test_loss 5.0802415771484375\n",
      "Epoch 464 : train_loss 5.038634216308593\n",
      "Epoch 464 : test_loss 5.097312377929687\n",
      "Epoch 465 : train_loss 5.027961608886719\n",
      "Epoch 465 : test_loss 5.07871240234375\n",
      "Epoch 466 : train_loss 5.039869995117187\n",
      "Epoch 466 : test_loss 5.106611328125\n",
      "Epoch 467 : train_loss 5.057975341796875\n",
      "Epoch 467 : test_loss 5.119527587890625\n",
      "Epoch 468 : train_loss 5.047734313964844\n",
      "Epoch 468 : test_loss 5.111286376953125\n",
      "Epoch 469 : train_loss 5.039346313476562\n",
      "Epoch 469 : test_loss 5.112513427734375\n",
      "Epoch 470 : train_loss 5.035265380859375\n",
      "Epoch 470 : test_loss 5.125544311523438\n",
      "Epoch 471 : train_loss 5.047757751464844\n",
      "Epoch 471 : test_loss 5.0977080078125\n",
      "Epoch 472 : train_loss 5.047050048828125\n",
      "Epoch 472 : test_loss 5.105383422851562\n",
      "Epoch 473 : train_loss 5.046240051269531\n",
      "Epoch 473 : test_loss 5.11459033203125\n",
      "Epoch 474 : train_loss 5.0348232421875\n",
      "Epoch 474 : test_loss 5.107495239257813\n",
      "Epoch 475 : train_loss 5.034275512695313\n",
      "Epoch 475 : test_loss 5.122111572265625\n",
      "Epoch 476 : train_loss 5.026234436035156\n",
      "Epoch 476 : test_loss 5.095682373046875\n",
      "Epoch 477 : train_loss 5.047420104980469\n",
      "Epoch 477 : test_loss 5.1356396484375\n",
      "Epoch 478 : train_loss 5.045375244140625\n",
      "Epoch 478 : test_loss 5.101777465820312\n",
      "Epoch 479 : train_loss 5.052841918945313\n",
      "Epoch 479 : test_loss 5.12144873046875\n",
      "Epoch 480 : train_loss 5.042817565917969\n",
      "Epoch 480 : test_loss 5.101668701171875\n",
      "Epoch 481 : train_loss 5.049252563476562\n",
      "Epoch 481 : test_loss 5.094982299804688\n",
      "Epoch 482 : train_loss 5.050534301757812\n",
      "Epoch 482 : test_loss 5.0954091796875\n",
      "Epoch 483 : train_loss 5.049406982421875\n",
      "Epoch 483 : test_loss 5.08940380859375\n",
      "Epoch 484 : train_loss 5.03262109375\n",
      "Epoch 484 : test_loss 5.108230712890625\n",
      "Epoch 485 : train_loss 5.042423461914063\n",
      "Epoch 485 : test_loss 5.084438354492187\n",
      "Epoch 486 : train_loss 5.037577880859375\n",
      "Epoch 486 : test_loss 5.101399169921875\n",
      "Epoch 487 : train_loss 5.050413024902344\n",
      "Epoch 487 : test_loss 5.111171875\n",
      "Epoch 488 : train_loss 5.05175\n",
      "Epoch 488 : test_loss 5.11276953125\n",
      "Epoch 489 : train_loss 5.0356231079101565\n",
      "Epoch 489 : test_loss 5.11597705078125\n",
      "Epoch 490 : train_loss 5.04556884765625\n",
      "Epoch 490 : test_loss 5.108244384765625\n",
      "Epoch 491 : train_loss 5.0382645874023435\n",
      "Epoch 491 : test_loss 5.105084106445313\n",
      "Epoch 492 : train_loss 5.0422144165039064\n",
      "Epoch 492 : test_loss 5.090841796875\n",
      "Epoch 493 : train_loss 5.0433499755859375\n",
      "Epoch 493 : test_loss 5.09448876953125\n",
      "Epoch 494 : train_loss 5.045506103515625\n",
      "Epoch 494 : test_loss 5.10452392578125\n",
      "Epoch 495 : train_loss 5.03897802734375\n",
      "Epoch 495 : test_loss 5.112056396484375\n",
      "Epoch 496 : train_loss 5.03585595703125\n",
      "Epoch 496 : test_loss 5.1256015625\n",
      "Epoch 497 : train_loss 5.038248718261719\n",
      "Epoch 497 : test_loss 5.128146484375\n",
      "Epoch 498 : train_loss 5.040697998046875\n",
      "Epoch 498 : test_loss 5.085853271484375\n",
      "Epoch 499 : train_loss 5.0348748779296875\n",
      "Epoch 499 : test_loss 5.1306533203125\n",
      "Epoch 500 : train_loss 5.044887573242187\n",
      "Epoch 500 : test_loss 5.100828247070313\n",
      "Epoch 501 : train_loss 5.045918334960938\n",
      "Epoch 501 : test_loss 5.103493408203125\n",
      "Epoch 502 : train_loss 5.048569519042969\n",
      "Epoch 502 : test_loss 5.122816162109375\n",
      "Epoch 503 : train_loss 5.0471473388671875\n",
      "Epoch 503 : test_loss 5.1261884765625\n",
      "Epoch 504 : train_loss 5.038999877929688\n",
      "Epoch 504 : test_loss 5.100301513671875\n",
      "Epoch 505 : train_loss 5.0213134765625\n",
      "Epoch 505 : test_loss 5.119451904296875\n",
      "Epoch 506 : train_loss 5.044859680175781\n",
      "Epoch 506 : test_loss 5.10817626953125\n",
      "Epoch 507 : train_loss 5.05394189453125\n",
      "Epoch 507 : test_loss 5.11269775390625\n",
      "Epoch 508 : train_loss 5.0423543090820315\n",
      "Epoch 508 : test_loss 5.08798095703125\n",
      "Epoch 509 : train_loss 5.041353332519531\n",
      "Epoch 509 : test_loss 5.08651416015625\n",
      "Epoch 510 : train_loss 5.040812744140625\n",
      "Epoch 510 : test_loss 5.121337890625\n",
      "saved model\n",
      "Epoch 511 : train_loss 5.061891540527344\n",
      "Epoch 511 : test_loss 5.068559326171875\n",
      "Epoch 512 : train_loss 5.038434936523437\n",
      "Epoch 512 : test_loss 5.103171264648437\n",
      "Epoch 513 : train_loss 5.045809020996094\n",
      "Epoch 513 : test_loss 5.10100146484375\n",
      "Epoch 514 : train_loss 5.042062622070312\n",
      "Epoch 514 : test_loss 5.1072412109375\n",
      "Epoch 515 : train_loss 5.0470777587890625\n",
      "Epoch 515 : test_loss 5.1309423828125\n",
      "Epoch 516 : train_loss 5.037034912109375\n",
      "Epoch 516 : test_loss 5.103389892578125\n",
      "Epoch 517 : train_loss 5.036145751953125\n",
      "Epoch 517 : test_loss 5.123137939453125\n",
      "Epoch 518 : train_loss 5.0379136962890625\n",
      "Epoch 518 : test_loss 5.10686474609375\n",
      "Epoch 519 : train_loss 5.05379541015625\n",
      "Epoch 519 : test_loss 5.104141967773438\n",
      "Epoch 520 : train_loss 5.042851440429687\n",
      "Epoch 520 : test_loss 5.0911943359375\n",
      "Epoch 521 : train_loss 5.04635302734375\n",
      "Epoch 521 : test_loss 5.099147338867187\n",
      "Epoch 522 : train_loss 5.0522890625\n",
      "Epoch 522 : test_loss 5.12423681640625\n",
      "Epoch 523 : train_loss 5.045538330078125\n",
      "Epoch 523 : test_loss 5.124155517578125\n",
      "Epoch 524 : train_loss 5.033697631835937\n",
      "Epoch 524 : test_loss 5.107552734375\n",
      "Epoch 525 : train_loss 5.046068481445312\n",
      "Epoch 525 : test_loss 5.08958935546875\n",
      "Epoch 526 : train_loss 5.036125122070312\n",
      "Epoch 526 : test_loss 5.103441650390625\n",
      "Epoch 527 : train_loss 5.044243408203125\n",
      "Epoch 527 : test_loss 5.10624365234375\n",
      "Epoch 528 : train_loss 5.045136474609375\n",
      "Epoch 528 : test_loss 5.094937744140625\n",
      "Epoch 529 : train_loss 5.045952453613281\n",
      "Epoch 529 : test_loss 5.110490966796875\n",
      "Epoch 530 : train_loss 5.03964599609375\n",
      "Epoch 530 : test_loss 5.104625732421875\n",
      "Epoch 531 : train_loss 5.0310997314453125\n",
      "Epoch 531 : test_loss 5.101939697265625\n",
      "Epoch 532 : train_loss 5.053230529785156\n",
      "Epoch 532 : test_loss 5.105548828125\n",
      "Epoch 533 : train_loss 5.035945678710937\n",
      "Epoch 533 : test_loss 5.10360302734375\n",
      "Epoch 534 : train_loss 5.047729736328125\n",
      "Epoch 534 : test_loss 5.102356689453125\n",
      "Epoch 535 : train_loss 5.045862915039063\n",
      "Epoch 535 : test_loss 5.12455419921875\n",
      "Epoch 536 : train_loss 5.048220642089844\n",
      "Epoch 536 : test_loss 5.102453247070312\n",
      "Epoch 537 : train_loss 5.032972778320312\n",
      "Epoch 537 : test_loss 5.088984985351562\n",
      "Epoch 538 : train_loss 5.0490877685546875\n",
      "Epoch 538 : test_loss 5.120005859375\n",
      "Epoch 539 : train_loss 5.036916381835938\n",
      "Epoch 539 : test_loss 5.12869287109375\n",
      "Epoch 540 : train_loss 5.046369567871094\n",
      "Epoch 540 : test_loss 5.089400024414062\n",
      "Epoch 541 : train_loss 5.049479064941406\n",
      "Epoch 541 : test_loss 5.126089599609375\n",
      "Epoch 542 : train_loss 5.044711181640625\n",
      "Epoch 542 : test_loss 5.1177158203125\n",
      "Epoch 543 : train_loss 5.040188842773437\n",
      "Epoch 543 : test_loss 5.110149658203125\n",
      "Epoch 544 : train_loss 5.03445751953125\n",
      "Epoch 544 : test_loss 5.097225341796875\n",
      "Epoch 545 : train_loss 5.0267199096679684\n",
      "Epoch 545 : test_loss 5.12221337890625\n",
      "Epoch 546 : train_loss 5.041840942382812\n",
      "Epoch 546 : test_loss 5.095724365234375\n",
      "Epoch 547 : train_loss 5.025519775390625\n",
      "Epoch 547 : test_loss 5.11053515625\n",
      "Epoch 548 : train_loss 5.05044580078125\n",
      "Epoch 548 : test_loss 5.097921020507813\n",
      "Epoch 549 : train_loss 5.022564636230468\n",
      "Epoch 549 : test_loss 5.091461181640625\n",
      "Epoch 550 : train_loss 5.044919189453125\n",
      "Epoch 550 : test_loss 5.099736450195312\n",
      "Epoch 551 : train_loss 5.040074462890625\n",
      "Epoch 551 : test_loss 5.116672241210938\n",
      "Epoch 552 : train_loss 5.038725830078125\n",
      "Epoch 552 : test_loss 5.0847216796875\n",
      "Epoch 553 : train_loss 5.058950439453125\n",
      "Epoch 553 : test_loss 5.09160302734375\n",
      "Epoch 554 : train_loss 5.050827331542969\n",
      "Epoch 554 : test_loss 5.11596044921875\n",
      "Epoch 555 : train_loss 5.055542846679687\n",
      "Epoch 555 : test_loss 5.109506713867187\n",
      "Epoch 556 : train_loss 5.0315093994140625\n",
      "Epoch 556 : test_loss 5.108531494140625\n",
      "Epoch 557 : train_loss 5.046878173828125\n",
      "Epoch 557 : test_loss 5.102045166015625\n",
      "Epoch 558 : train_loss 5.052298828125\n",
      "Epoch 558 : test_loss 5.108234619140625\n",
      "Epoch 559 : train_loss 5.040697387695312\n",
      "Epoch 559 : test_loss 5.100505126953125\n",
      "Epoch 560 : train_loss 5.043081298828125\n",
      "Epoch 560 : test_loss 5.106949584960938\n",
      "Epoch 561 : train_loss 5.035835876464843\n",
      "Epoch 561 : test_loss 5.096825073242187\n",
      "Epoch 562 : train_loss 5.057193908691406\n",
      "Epoch 562 : test_loss 5.09628662109375\n",
      "Epoch 563 : train_loss 5.036300170898437\n",
      "Epoch 563 : test_loss 5.103404052734375\n",
      "Epoch 564 : train_loss 5.0358343505859375\n",
      "Epoch 564 : test_loss 5.092176147460938\n",
      "Epoch 565 : train_loss 5.038901000976563\n",
      "Epoch 565 : test_loss 5.109812377929687\n",
      "Epoch 566 : train_loss 5.059934814453125\n",
      "Epoch 566 : test_loss 5.088289306640625\n",
      "Epoch 567 : train_loss 5.057281005859375\n",
      "Epoch 567 : test_loss 5.0948935546875\n",
      "Epoch 568 : train_loss 5.02781884765625\n",
      "Epoch 568 : test_loss 5.094196533203125\n",
      "Epoch 569 : train_loss 5.037947265625\n",
      "Epoch 569 : test_loss 5.096930908203125\n",
      "Epoch 570 : train_loss 5.041468322753906\n",
      "Epoch 570 : test_loss 5.106157470703125\n",
      "Epoch 571 : train_loss 5.037434936523438\n",
      "Epoch 571 : test_loss 5.10907861328125\n",
      "Epoch 572 : train_loss 5.045049072265625\n",
      "Epoch 572 : test_loss 5.090319213867187\n",
      "Epoch 573 : train_loss 5.035710205078125\n",
      "Epoch 573 : test_loss 5.113398193359375\n",
      "Epoch 574 : train_loss 5.039244018554688\n",
      "Epoch 574 : test_loss 5.106825073242187\n",
      "Epoch 575 : train_loss 5.041990844726563\n",
      "Epoch 575 : test_loss 5.110624755859375\n",
      "Epoch 576 : train_loss 5.04805615234375\n",
      "Epoch 576 : test_loss 5.109465087890625\n",
      "Epoch 577 : train_loss 5.04991796875\n",
      "Epoch 577 : test_loss 5.112880981445312\n",
      "Epoch 578 : train_loss 5.04557666015625\n",
      "Epoch 578 : test_loss 5.091068969726562\n",
      "Epoch 579 : train_loss 5.042043395996094\n",
      "Epoch 579 : test_loss 5.103816650390625\n",
      "Epoch 580 : train_loss 5.032325561523438\n",
      "Epoch 580 : test_loss 5.115284423828125\n",
      "Epoch 581 : train_loss 5.048160400390625\n",
      "Epoch 581 : test_loss 5.09548583984375\n",
      "Epoch 582 : train_loss 5.030233764648438\n",
      "Epoch 582 : test_loss 5.105174072265625\n",
      "Epoch 583 : train_loss 5.034383544921875\n",
      "Epoch 583 : test_loss 5.108056396484375\n",
      "Epoch 584 : train_loss 5.051173706054687\n",
      "Epoch 584 : test_loss 5.113676147460938\n",
      "Epoch 585 : train_loss 5.043932373046875\n",
      "Epoch 585 : test_loss 5.078194213867188\n",
      "Epoch 586 : train_loss 5.051578125\n",
      "Epoch 586 : test_loss 5.08884765625\n",
      "Epoch 587 : train_loss 5.050851440429687\n",
      "Epoch 587 : test_loss 5.1231806640625\n",
      "Epoch 588 : train_loss 5.034431396484375\n",
      "Epoch 588 : test_loss 5.10747412109375\n",
      "Epoch 589 : train_loss 5.043700561523438\n",
      "Epoch 589 : test_loss 5.111810546875\n",
      "Epoch 590 : train_loss 5.0483349609375\n",
      "Epoch 590 : test_loss 5.115071533203125\n",
      "Epoch 591 : train_loss 5.036691528320312\n",
      "Epoch 591 : test_loss 5.08334033203125\n",
      "Epoch 592 : train_loss 5.043290893554688\n",
      "Epoch 592 : test_loss 5.1027041015625\n",
      "Epoch 593 : train_loss 5.037332397460937\n",
      "Epoch 593 : test_loss 5.108764404296875\n",
      "Epoch 594 : train_loss 5.043042419433593\n",
      "Epoch 594 : test_loss 5.093452758789063\n",
      "Epoch 595 : train_loss 5.039910888671875\n",
      "Epoch 595 : test_loss 5.1159892578125\n",
      "Epoch 596 : train_loss 5.042990112304688\n",
      "Epoch 596 : test_loss 5.103408081054687\n",
      "Epoch 597 : train_loss 5.051240356445312\n",
      "Epoch 597 : test_loss 5.111265869140625\n",
      "Epoch 598 : train_loss 5.043174072265625\n",
      "Epoch 598 : test_loss 5.1156455078125\n",
      "Epoch 599 : train_loss 5.028867797851563\n",
      "Epoch 599 : test_loss 5.101271118164062\n",
      "Epoch 600 : train_loss 5.0503778076171875\n",
      "Epoch 600 : test_loss 5.094735107421875\n",
      "Epoch 601 : train_loss 5.040214111328125\n",
      "Epoch 601 : test_loss 5.112118408203125\n",
      "Epoch 602 : train_loss 5.034155700683594\n",
      "Epoch 602 : test_loss 5.11081396484375\n",
      "Epoch 603 : train_loss 5.048578430175781\n",
      "Epoch 603 : test_loss 5.1113544921875\n",
      "Epoch 604 : train_loss 5.031123168945313\n",
      "Epoch 604 : test_loss 5.09701220703125\n",
      "Epoch 605 : train_loss 5.051623291015625\n",
      "Epoch 605 : test_loss 5.0996029052734375\n",
      "Epoch 606 : train_loss 5.044046020507812\n",
      "Epoch 606 : test_loss 5.12291943359375\n",
      "Epoch 607 : train_loss 5.0442698974609375\n",
      "Epoch 607 : test_loss 5.098708251953125\n",
      "Epoch 608 : train_loss 5.045444091796875\n",
      "Epoch 608 : test_loss 5.09656787109375\n",
      "Epoch 609 : train_loss 5.0438017578125\n",
      "Epoch 609 : test_loss 5.078588500976562\n",
      "Epoch 610 : train_loss 5.052300903320313\n",
      "Epoch 610 : test_loss 5.100300537109375\n",
      "Epoch 611 : train_loss 5.033662841796875\n",
      "Epoch 611 : test_loss 5.123390014648438\n",
      "Epoch 612 : train_loss 5.043286499023438\n",
      "Epoch 612 : test_loss 5.100636962890625\n",
      "Epoch 613 : train_loss 5.041004638671875\n",
      "Epoch 613 : test_loss 5.11191845703125\n",
      "Epoch 614 : train_loss 5.03508837890625\n",
      "Epoch 614 : test_loss 5.112007568359375\n",
      "Epoch 615 : train_loss 5.057470520019531\n",
      "Epoch 615 : test_loss 5.1217919921875\n",
      "Epoch 616 : train_loss 5.0319019775390625\n",
      "Epoch 616 : test_loss 5.0937861328125\n",
      "Epoch 617 : train_loss 5.041864624023438\n",
      "Epoch 617 : test_loss 5.088347045898438\n",
      "Epoch 618 : train_loss 5.035591796875\n",
      "Epoch 618 : test_loss 5.1223427734375\n",
      "Epoch 619 : train_loss 5.0494337768554685\n",
      "Epoch 619 : test_loss 5.106261962890625\n",
      "Epoch 620 : train_loss 5.0447861328125\n",
      "Epoch 620 : test_loss 5.09876953125\n",
      "Epoch 621 : train_loss 5.049427856445313\n",
      "Epoch 621 : test_loss 5.10406298828125\n",
      "Epoch 622 : train_loss 5.0441243286132815\n",
      "Epoch 622 : test_loss 5.10901611328125\n",
      "Epoch 623 : train_loss 5.046419921875\n",
      "Epoch 623 : test_loss 5.1250556640625\n",
      "Epoch 624 : train_loss 5.05661572265625\n",
      "Epoch 624 : test_loss 5.09517919921875\n",
      "Epoch 625 : train_loss 5.030900756835938\n",
      "Epoch 625 : test_loss 5.069459228515625\n",
      "Epoch 626 : train_loss 5.043163208007813\n",
      "Epoch 626 : test_loss 5.111466064453125\n",
      "Epoch 627 : train_loss 5.051002685546875\n",
      "Epoch 627 : test_loss 5.0926298828125\n",
      "Epoch 628 : train_loss 5.0370701293945315\n",
      "Epoch 628 : test_loss 5.07683984375\n",
      "Epoch 629 : train_loss 5.036023315429688\n",
      "Epoch 629 : test_loss 5.103456787109375\n",
      "Epoch 630 : train_loss 5.0426896362304685\n",
      "Epoch 630 : test_loss 5.097917724609375\n",
      "Epoch 631 : train_loss 5.060742797851563\n",
      "Epoch 631 : test_loss 5.11753662109375\n",
      "Epoch 632 : train_loss 5.0437099609375\n",
      "Epoch 632 : test_loss 5.104387451171875\n",
      "Epoch 633 : train_loss 5.046116088867188\n",
      "Epoch 633 : test_loss 5.11084912109375\n",
      "Epoch 634 : train_loss 5.035283935546875\n",
      "Epoch 634 : test_loss 5.08200048828125\n",
      "Epoch 635 : train_loss 5.034036010742187\n",
      "Epoch 635 : test_loss 5.12159423828125\n",
      "Epoch 636 : train_loss 5.0329873046875\n",
      "Epoch 636 : test_loss 5.13971923828125\n",
      "Epoch 637 : train_loss 5.025343017578125\n",
      "Epoch 637 : test_loss 5.12652294921875\n",
      "Epoch 638 : train_loss 5.056707580566406\n",
      "Epoch 638 : test_loss 5.088360717773438\n",
      "Epoch 639 : train_loss 5.025683227539062\n",
      "Epoch 639 : test_loss 5.109114868164062\n",
      "Epoch 640 : train_loss 5.053593994140625\n",
      "Epoch 640 : test_loss 5.14652001953125\n",
      "Epoch 641 : train_loss 5.0320933837890625\n",
      "Epoch 641 : test_loss 5.132794921875\n",
      "Epoch 642 : train_loss 5.037615661621094\n",
      "Epoch 642 : test_loss 5.115156982421875\n",
      "Epoch 643 : train_loss 5.037195678710938\n",
      "Epoch 643 : test_loss 5.107395751953125\n",
      "Epoch 644 : train_loss 5.041359252929688\n",
      "Epoch 644 : test_loss 5.09684521484375\n",
      "Epoch 645 : train_loss 5.03585986328125\n",
      "Epoch 645 : test_loss 5.110403076171875\n",
      "Epoch 646 : train_loss 5.042348022460938\n",
      "Epoch 646 : test_loss 5.076281005859375\n",
      "Epoch 647 : train_loss 5.041419006347656\n",
      "Epoch 647 : test_loss 5.12135205078125\n",
      "saved model\n",
      "Epoch 648 : train_loss 5.053934692382812\n",
      "Epoch 648 : test_loss 5.066094116210937\n",
      "Epoch 649 : train_loss 5.0500986328125\n",
      "Epoch 649 : test_loss 5.107081909179687\n",
      "Epoch 650 : train_loss 5.040810913085937\n",
      "Epoch 650 : test_loss 5.101700561523438\n",
      "Epoch 651 : train_loss 5.020649353027344\n",
      "Epoch 651 : test_loss 5.136829833984375\n",
      "Epoch 652 : train_loss 5.044312194824219\n",
      "Epoch 652 : test_loss 5.11813916015625\n",
      "Epoch 653 : train_loss 5.036606811523438\n",
      "Epoch 653 : test_loss 5.11544970703125\n",
      "Epoch 654 : train_loss 5.03975439453125\n",
      "Epoch 654 : test_loss 5.099841064453125\n",
      "Epoch 655 : train_loss 5.040355285644531\n",
      "Epoch 655 : test_loss 5.1038056640625\n",
      "Epoch 656 : train_loss 5.04373828125\n",
      "Epoch 656 : test_loss 5.110480224609375\n",
      "Epoch 657 : train_loss 5.0490166015625\n",
      "Epoch 657 : test_loss 5.105257568359375\n",
      "Epoch 658 : train_loss 5.026817260742187\n",
      "Epoch 658 : test_loss 5.10384619140625\n",
      "Epoch 659 : train_loss 5.033783813476562\n",
      "Epoch 659 : test_loss 5.108110229492188\n",
      "Epoch 660 : train_loss 5.03502294921875\n",
      "Epoch 660 : test_loss 5.105465576171875\n",
      "Epoch 661 : train_loss 5.037973999023437\n",
      "Epoch 661 : test_loss 5.0984208984375\n",
      "Epoch 662 : train_loss 5.052632019042969\n",
      "Epoch 662 : test_loss 5.11196923828125\n",
      "Epoch 663 : train_loss 5.039520751953125\n",
      "Epoch 663 : test_loss 5.134217529296875\n",
      "Epoch 664 : train_loss 5.041157958984375\n",
      "Epoch 664 : test_loss 5.102113403320312\n",
      "Epoch 665 : train_loss 5.038731872558594\n",
      "Epoch 665 : test_loss 5.09752392578125\n",
      "Epoch 666 : train_loss 5.050743530273437\n",
      "Epoch 666 : test_loss 5.102081298828125\n",
      "Epoch 667 : train_loss 5.044787109375\n",
      "Epoch 667 : test_loss 5.091341796875\n",
      "Epoch 668 : train_loss 5.06552197265625\n",
      "Epoch 668 : test_loss 5.106869384765625\n",
      "Epoch 669 : train_loss 5.028155517578125\n",
      "Epoch 669 : test_loss 5.09071875\n",
      "Epoch 670 : train_loss 5.04827099609375\n",
      "Epoch 670 : test_loss 5.100618896484375\n",
      "Epoch 671 : train_loss 5.041406066894531\n",
      "Epoch 671 : test_loss 5.091967407226562\n",
      "Epoch 672 : train_loss 5.03397265625\n",
      "Epoch 672 : test_loss 5.101077392578125\n",
      "Epoch 673 : train_loss 5.05182861328125\n",
      "Epoch 673 : test_loss 5.107048461914062\n",
      "Epoch 674 : train_loss 5.0277230834960935\n",
      "Epoch 674 : test_loss 5.097332397460938\n",
      "Epoch 675 : train_loss 5.038356079101563\n",
      "Epoch 675 : test_loss 5.12582421875\n",
      "Epoch 676 : train_loss 5.035207275390625\n",
      "Epoch 676 : test_loss 5.12189208984375\n",
      "Epoch 677 : train_loss 5.034434326171875\n",
      "Epoch 677 : test_loss 5.111717041015625\n",
      "Epoch 678 : train_loss 5.052418640136719\n",
      "Epoch 678 : test_loss 5.1208560791015625\n",
      "Epoch 679 : train_loss 5.032355712890625\n",
      "Epoch 679 : test_loss 5.118521240234375\n",
      "Epoch 680 : train_loss 5.042810180664063\n",
      "Epoch 680 : test_loss 5.09931298828125\n",
      "Epoch 681 : train_loss 5.023965759277344\n",
      "Epoch 681 : test_loss 5.115472534179688\n",
      "Epoch 682 : train_loss 5.043823486328125\n",
      "Epoch 682 : test_loss 5.088966796875\n",
      "Epoch 683 : train_loss 5.04583056640625\n",
      "Epoch 683 : test_loss 5.109861083984375\n",
      "Epoch 684 : train_loss 5.036109130859375\n",
      "Epoch 684 : test_loss 5.089036987304688\n",
      "Epoch 685 : train_loss 5.045278869628906\n",
      "Epoch 685 : test_loss 5.090501831054688\n",
      "Epoch 686 : train_loss 5.04146142578125\n",
      "Epoch 686 : test_loss 5.117775146484375\n",
      "Epoch 687 : train_loss 5.060638000488281\n",
      "Epoch 687 : test_loss 5.121942138671875\n",
      "Epoch 688 : train_loss 5.0404161376953125\n",
      "Epoch 688 : test_loss 5.114612548828125\n",
      "Epoch 689 : train_loss 5.039518432617188\n",
      "Epoch 689 : test_loss 5.104010986328125\n",
      "Epoch 690 : train_loss 5.042015075683594\n",
      "Epoch 690 : test_loss 5.093266357421875\n",
      "Epoch 691 : train_loss 5.042716430664062\n",
      "Epoch 691 : test_loss 5.1232587890625\n",
      "Epoch 692 : train_loss 5.038877380371094\n",
      "Epoch 692 : test_loss 5.119234741210938\n",
      "Epoch 693 : train_loss 5.038434936523437\n",
      "Epoch 693 : test_loss 5.10606884765625\n",
      "Epoch 694 : train_loss 5.0348369140625\n",
      "Epoch 694 : test_loss 5.0840703125\n",
      "Epoch 695 : train_loss 5.041880310058594\n",
      "Epoch 695 : test_loss 5.1116806640625\n",
      "Epoch 696 : train_loss 5.068000915527343\n",
      "Epoch 696 : test_loss 5.083054321289063\n",
      "Epoch 697 : train_loss 5.035599609375\n",
      "Epoch 697 : test_loss 5.114100952148437\n",
      "Epoch 698 : train_loss 5.03433154296875\n",
      "Epoch 698 : test_loss 5.11739599609375\n",
      "Epoch 699 : train_loss 5.029005615234375\n",
      "Epoch 699 : test_loss 5.093623779296875\n",
      "Epoch 700 : train_loss 5.0450361328125\n",
      "Epoch 700 : test_loss 5.108333740234375\n",
      "Epoch 701 : train_loss 5.047946655273438\n",
      "Epoch 701 : test_loss 5.106437377929687\n",
      "Epoch 702 : train_loss 5.041125244140625\n",
      "Epoch 702 : test_loss 5.091685302734375\n",
      "Epoch 703 : train_loss 5.042969787597657\n",
      "Epoch 703 : test_loss 5.110168212890625\n",
      "Epoch 704 : train_loss 5.046095275878907\n",
      "Epoch 704 : test_loss 5.09750732421875\n",
      "Epoch 705 : train_loss 5.034135009765625\n",
      "Epoch 705 : test_loss 5.085918212890625\n",
      "Epoch 706 : train_loss 5.042730102539062\n",
      "Epoch 706 : test_loss 5.10736328125\n",
      "Epoch 707 : train_loss 5.018616577148437\n",
      "Epoch 707 : test_loss 5.102275146484375\n",
      "Epoch 708 : train_loss 5.031446166992188\n",
      "Epoch 708 : test_loss 5.10434228515625\n",
      "Epoch 709 : train_loss 5.034088500976562\n",
      "Epoch 709 : test_loss 5.1102548828125\n",
      "Epoch 710 : train_loss 5.042088989257812\n",
      "Epoch 710 : test_loss 5.10457373046875\n",
      "Epoch 711 : train_loss 5.032929565429687\n",
      "Epoch 711 : test_loss 5.114753173828125\n",
      "Epoch 712 : train_loss 5.031892761230469\n",
      "Epoch 712 : test_loss 5.108396240234375\n",
      "Epoch 713 : train_loss 5.042313598632813\n",
      "Epoch 713 : test_loss 5.10078466796875\n",
      "Epoch 714 : train_loss 5.035911865234375\n",
      "Epoch 714 : test_loss 5.109744995117188\n",
      "Epoch 715 : train_loss 5.0390364379882815\n",
      "Epoch 715 : test_loss 5.113022705078125\n",
      "Epoch 716 : train_loss 5.051859375\n",
      "Epoch 716 : test_loss 5.12294970703125\n",
      "Epoch 717 : train_loss 5.056889343261719\n",
      "Epoch 717 : test_loss 5.101259887695313\n",
      "Epoch 718 : train_loss 5.042263305664062\n",
      "Epoch 718 : test_loss 5.107158203125\n",
      "Epoch 719 : train_loss 5.037157958984375\n",
      "Epoch 719 : test_loss 5.10562548828125\n",
      "Epoch 720 : train_loss 5.042314514160156\n",
      "Epoch 720 : test_loss 5.1177099609375\n",
      "Epoch 721 : train_loss 5.04450146484375\n",
      "Epoch 721 : test_loss 5.09192578125\n",
      "Epoch 722 : train_loss 5.0424478759765625\n",
      "Epoch 722 : test_loss 5.109539794921875\n",
      "Epoch 723 : train_loss 5.047218994140625\n",
      "Epoch 723 : test_loss 5.1110849609375\n",
      "Epoch 724 : train_loss 5.0352373046875\n",
      "Epoch 724 : test_loss 5.089073974609375\n",
      "Epoch 725 : train_loss 5.056822448730469\n",
      "Epoch 725 : test_loss 5.110363525390625\n",
      "Epoch 726 : train_loss 5.045696166992188\n",
      "Epoch 726 : test_loss 5.11414892578125\n",
      "Epoch 727 : train_loss 5.030782409667969\n",
      "Epoch 727 : test_loss 5.127275146484375\n",
      "Epoch 728 : train_loss 5.050020263671875\n",
      "Epoch 728 : test_loss 5.110728149414062\n",
      "Epoch 729 : train_loss 5.048812133789062\n",
      "Epoch 729 : test_loss 5.100545166015625\n",
      "Epoch 730 : train_loss 5.03810888671875\n",
      "Epoch 730 : test_loss 5.10932666015625\n",
      "Epoch 731 : train_loss 5.048647827148438\n",
      "Epoch 731 : test_loss 5.106159423828125\n",
      "Epoch 732 : train_loss 5.034737915039062\n",
      "Epoch 732 : test_loss 5.114348388671875\n",
      "Epoch 733 : train_loss 5.046682250976563\n",
      "Epoch 733 : test_loss 5.113640869140625\n",
      "Epoch 734 : train_loss 5.052142150878907\n",
      "Epoch 734 : test_loss 5.11076025390625\n",
      "Epoch 735 : train_loss 5.05200146484375\n",
      "Epoch 735 : test_loss 5.12062158203125\n",
      "Epoch 736 : train_loss 5.038164794921875\n",
      "Epoch 736 : test_loss 5.093241455078125\n",
      "Epoch 737 : train_loss 5.055105224609375\n",
      "Epoch 737 : test_loss 5.101479736328125\n",
      "Epoch 738 : train_loss 5.039099426269531\n",
      "Epoch 738 : test_loss 5.115201416015625\n",
      "Epoch 739 : train_loss 5.051593017578125\n",
      "Epoch 739 : test_loss 5.10940673828125\n",
      "Epoch 740 : train_loss 5.042140991210937\n",
      "Epoch 740 : test_loss 5.113521484375\n",
      "Epoch 741 : train_loss 5.049428833007813\n",
      "Epoch 741 : test_loss 5.081726928710937\n",
      "Epoch 742 : train_loss 5.0490460815429685\n",
      "Epoch 742 : test_loss 5.106091064453125\n",
      "Epoch 743 : train_loss 5.033613891601562\n",
      "Epoch 743 : test_loss 5.146187744140625\n",
      "Epoch 744 : train_loss 5.043370300292969\n",
      "Epoch 744 : test_loss 5.09509130859375\n",
      "Epoch 745 : train_loss 5.0329478759765625\n",
      "Epoch 745 : test_loss 5.086833740234375\n",
      "Epoch 746 : train_loss 5.049270812988281\n",
      "Epoch 746 : test_loss 5.10690625\n",
      "Epoch 747 : train_loss 5.037843139648437\n",
      "Epoch 747 : test_loss 5.102328491210938\n",
      "Epoch 748 : train_loss 5.045168762207031\n",
      "Epoch 748 : test_loss 5.08628173828125\n",
      "Epoch 749 : train_loss 5.0406220703125\n",
      "Epoch 749 : test_loss 5.1157021484375\n",
      "Epoch 750 : train_loss 5.0419755859375\n",
      "Epoch 750 : test_loss 5.100634521484375\n",
      "Epoch 751 : train_loss 5.0450982666015625\n",
      "Epoch 751 : test_loss 5.088065185546875\n",
      "Epoch 752 : train_loss 5.035137451171875\n",
      "Epoch 752 : test_loss 5.108309692382813\n",
      "Epoch 753 : train_loss 5.046807006835937\n",
      "Epoch 753 : test_loss 5.099623046875\n",
      "Epoch 754 : train_loss 5.041407165527343\n",
      "Epoch 754 : test_loss 5.116588134765625\n",
      "Epoch 755 : train_loss 5.02694140625\n",
      "Epoch 755 : test_loss 5.07493701171875\n",
      "Epoch 756 : train_loss 5.024825500488281\n",
      "Epoch 756 : test_loss 5.080666748046875\n",
      "Epoch 757 : train_loss 5.0520029296875\n",
      "Epoch 757 : test_loss 5.104113037109375\n",
      "Epoch 758 : train_loss 5.041575317382812\n",
      "Epoch 758 : test_loss 5.11571533203125\n",
      "Epoch 759 : train_loss 5.043440368652344\n",
      "Epoch 759 : test_loss 5.122735107421875\n",
      "Epoch 760 : train_loss 5.0444183349609375\n",
      "Epoch 760 : test_loss 5.115941162109375\n",
      "Epoch 761 : train_loss 5.032153259277344\n",
      "Epoch 761 : test_loss 5.121623291015625\n",
      "Epoch 762 : train_loss 5.038332641601563\n",
      "Epoch 762 : test_loss 5.119246826171875\n",
      "Epoch 763 : train_loss 5.03924267578125\n",
      "Epoch 763 : test_loss 5.102851928710938\n",
      "Epoch 764 : train_loss 5.049634643554688\n",
      "Epoch 764 : test_loss 5.1131630859375\n",
      "Epoch 765 : train_loss 5.042691711425781\n",
      "Epoch 765 : test_loss 5.1023896484375\n",
      "Epoch 766 : train_loss 5.026226684570313\n",
      "Epoch 766 : test_loss 5.1245703125\n",
      "Epoch 767 : train_loss 5.050009765625\n",
      "Epoch 767 : test_loss 5.1104140625\n",
      "Epoch 768 : train_loss 5.049558898925781\n",
      "Epoch 768 : test_loss 5.093223388671875\n",
      "Epoch 769 : train_loss 5.052047607421875\n",
      "Epoch 769 : test_loss 5.096469970703125\n",
      "Epoch 770 : train_loss 5.055179565429688\n",
      "Epoch 770 : test_loss 5.077155517578125\n",
      "Epoch 771 : train_loss 5.041996215820313\n",
      "Epoch 771 : test_loss 5.11982275390625\n",
      "Epoch 772 : train_loss 5.043624267578125\n",
      "Epoch 772 : test_loss 5.122301025390625\n",
      "Epoch 773 : train_loss 5.037417846679688\n",
      "Epoch 773 : test_loss 5.0996201171875\n",
      "Epoch 774 : train_loss 5.044313720703125\n",
      "Epoch 774 : test_loss 5.089329345703125\n",
      "Epoch 775 : train_loss 5.037495849609375\n",
      "Epoch 775 : test_loss 5.104366455078125\n",
      "Epoch 776 : train_loss 5.046721435546875\n",
      "Epoch 776 : test_loss 5.103819091796875\n",
      "Epoch 777 : train_loss 5.04338623046875\n",
      "Epoch 777 : test_loss 5.09868603515625\n",
      "Epoch 778 : train_loss 5.03003125\n",
      "Epoch 778 : test_loss 5.106489990234375\n",
      "Epoch 779 : train_loss 5.040367431640625\n",
      "Epoch 779 : test_loss 5.10788671875\n",
      "Epoch 780 : train_loss 5.051122009277344\n",
      "Epoch 780 : test_loss 5.1113193359375\n",
      "Epoch 781 : train_loss 5.0448701171875\n",
      "Epoch 781 : test_loss 5.107513671875\n",
      "Epoch 782 : train_loss 5.021214660644532\n",
      "Epoch 782 : test_loss 5.087905517578125\n",
      "Epoch 783 : train_loss 5.044968505859375\n",
      "Epoch 783 : test_loss 5.09602587890625\n",
      "Epoch 784 : train_loss 5.042145263671875\n",
      "Epoch 784 : test_loss 5.108343505859375\n",
      "Epoch 785 : train_loss 5.041801635742187\n",
      "Epoch 785 : test_loss 5.081236572265625\n",
      "Epoch 786 : train_loss 5.0318101806640625\n",
      "Epoch 786 : test_loss 5.111917846679687\n",
      "Epoch 787 : train_loss 5.041440002441406\n",
      "Epoch 787 : test_loss 5.109889404296875\n",
      "Epoch 788 : train_loss 5.029107421875\n",
      "Epoch 788 : test_loss 5.104560546875\n",
      "Epoch 789 : train_loss 5.03222705078125\n",
      "Epoch 789 : test_loss 5.118527587890625\n",
      "Epoch 790 : train_loss 5.0399257202148435\n",
      "Epoch 790 : test_loss 5.101317749023438\n",
      "Epoch 791 : train_loss 5.040890380859375\n",
      "Epoch 791 : test_loss 5.096070922851562\n",
      "Epoch 792 : train_loss 5.044981811523438\n",
      "Epoch 792 : test_loss 5.116924072265625\n",
      "Epoch 793 : train_loss 5.045434814453125\n",
      "Epoch 793 : test_loss 5.10903857421875\n",
      "Epoch 794 : train_loss 5.028910583496094\n",
      "Epoch 794 : test_loss 5.095052001953125\n",
      "Epoch 795 : train_loss 5.042914672851563\n",
      "Epoch 795 : test_loss 5.121346435546875\n",
      "Epoch 796 : train_loss 5.033993041992187\n",
      "Epoch 796 : test_loss 5.113419921875\n",
      "Epoch 797 : train_loss 5.0397861328125\n",
      "Epoch 797 : test_loss 5.120934204101562\n",
      "Epoch 798 : train_loss 5.036264282226562\n",
      "Epoch 798 : test_loss 5.1102353515625\n",
      "Epoch 799 : train_loss 5.0441494140625\n",
      "Epoch 799 : test_loss 5.1068095703125\n",
      "Epoch 800 : train_loss 5.039021240234375\n",
      "Epoch 800 : test_loss 5.115642578125\n",
      "Epoch 801 : train_loss 5.034430053710937\n",
      "Epoch 801 : test_loss 5.13207177734375\n",
      "Epoch 802 : train_loss 5.039496948242188\n",
      "Epoch 802 : test_loss 5.111759033203125\n",
      "Epoch 803 : train_loss 5.037371459960937\n",
      "Epoch 803 : test_loss 5.094931762695312\n",
      "Epoch 804 : train_loss 5.026198608398437\n",
      "Epoch 804 : test_loss 5.0891357421875\n",
      "Epoch 805 : train_loss 5.032136108398437\n",
      "Epoch 805 : test_loss 5.115745361328125\n",
      "Epoch 806 : train_loss 5.03529833984375\n",
      "Epoch 806 : test_loss 5.112389404296875\n",
      "Epoch 807 : train_loss 5.0372554931640625\n",
      "Epoch 807 : test_loss 5.107534423828125\n",
      "Epoch 808 : train_loss 5.0303818359375\n",
      "Epoch 808 : test_loss 5.104714111328125\n",
      "Epoch 809 : train_loss 5.039076782226562\n",
      "Epoch 809 : test_loss 5.100498046875\n",
      "Epoch 810 : train_loss 5.035536315917969\n",
      "Epoch 810 : test_loss 5.09904248046875\n",
      "Epoch 811 : train_loss 5.0518470458984375\n",
      "Epoch 811 : test_loss 5.100212158203125\n",
      "Epoch 812 : train_loss 5.038061584472656\n",
      "Epoch 812 : test_loss 5.10921875\n",
      "Epoch 813 : train_loss 5.0522069702148436\n",
      "Epoch 813 : test_loss 5.122499755859375\n",
      "Epoch 814 : train_loss 5.046932006835937\n",
      "Epoch 814 : test_loss 5.11991357421875\n",
      "Epoch 815 : train_loss 5.0438731689453125\n",
      "Epoch 815 : test_loss 5.107799560546875\n",
      "Epoch 816 : train_loss 5.037140319824219\n",
      "Epoch 816 : test_loss 5.095494995117187\n",
      "Epoch 817 : train_loss 5.0455172119140625\n",
      "Epoch 817 : test_loss 5.1052060546875\n",
      "Epoch 818 : train_loss 5.031257629394531\n",
      "Epoch 818 : test_loss 5.101251953125\n",
      "Epoch 819 : train_loss 5.046295349121094\n",
      "Epoch 819 : test_loss 5.0824130859375\n",
      "Epoch 820 : train_loss 5.04696044921875\n",
      "Epoch 820 : test_loss 5.093384033203125\n",
      "Epoch 821 : train_loss 5.038003540039062\n",
      "Epoch 821 : test_loss 5.111087768554688\n",
      "Epoch 822 : train_loss 5.050946533203125\n",
      "Epoch 822 : test_loss 5.128619873046875\n",
      "Epoch 823 : train_loss 5.032918334960938\n",
      "Epoch 823 : test_loss 5.080984375\n",
      "Epoch 824 : train_loss 5.04269873046875\n",
      "Epoch 824 : test_loss 5.09256640625\n",
      "Epoch 825 : train_loss 5.04460009765625\n",
      "Epoch 825 : test_loss 5.11573974609375\n",
      "Epoch 826 : train_loss 5.05158251953125\n",
      "Epoch 826 : test_loss 5.1156806640625\n",
      "Epoch 827 : train_loss 5.042200561523438\n",
      "Epoch 827 : test_loss 5.111059326171875\n",
      "Epoch 828 : train_loss 5.053952270507812\n",
      "Epoch 828 : test_loss 5.128451416015625\n",
      "Epoch 829 : train_loss 5.0373955078125\n",
      "Epoch 829 : test_loss 5.1013076171875\n",
      "Epoch 830 : train_loss 5.044668334960938\n",
      "Epoch 830 : test_loss 5.118427001953125\n",
      "Epoch 831 : train_loss 5.030786193847656\n",
      "Epoch 831 : test_loss 5.107167114257813\n",
      "Epoch 832 : train_loss 5.0437705078125\n",
      "Epoch 832 : test_loss 5.0915067138671875\n",
      "Epoch 833 : train_loss 5.014728637695312\n",
      "Epoch 833 : test_loss 5.108377197265625\n",
      "Epoch 834 : train_loss 5.033601806640625\n",
      "Epoch 834 : test_loss 5.095767944335938\n",
      "Epoch 835 : train_loss 5.032113525390625\n",
      "Epoch 835 : test_loss 5.116213134765625\n",
      "Epoch 836 : train_loss 5.030147094726562\n",
      "Epoch 836 : test_loss 5.09887451171875\n",
      "Epoch 837 : train_loss 5.042257202148438\n",
      "Epoch 837 : test_loss 5.103461669921875\n",
      "Epoch 838 : train_loss 5.03772412109375\n",
      "Epoch 838 : test_loss 5.114760620117187\n",
      "Epoch 839 : train_loss 5.043287109375\n",
      "Epoch 839 : test_loss 5.09304931640625\n",
      "Epoch 840 : train_loss 5.039908752441407\n",
      "Epoch 840 : test_loss 5.101354736328125\n",
      "Epoch 841 : train_loss 5.051959716796875\n",
      "Epoch 841 : test_loss 5.108637939453125\n",
      "Epoch 842 : train_loss 5.058985473632813\n",
      "Epoch 842 : test_loss 5.11599560546875\n",
      "Epoch 843 : train_loss 5.042263916015625\n",
      "Epoch 843 : test_loss 5.09643359375\n",
      "Epoch 844 : train_loss 5.042084228515625\n",
      "Epoch 844 : test_loss 5.105150390625\n",
      "Epoch 845 : train_loss 5.048123046875\n",
      "Epoch 845 : test_loss 5.1144150390625\n",
      "Epoch 846 : train_loss 5.037176025390625\n",
      "Epoch 846 : test_loss 5.114491821289063\n",
      "Epoch 847 : train_loss 5.035947021484375\n",
      "Epoch 847 : test_loss 5.109434448242188\n",
      "Epoch 848 : train_loss 5.032861938476563\n",
      "Epoch 848 : test_loss 5.12934423828125\n",
      "Epoch 849 : train_loss 5.040727233886718\n",
      "Epoch 849 : test_loss 5.078003784179687\n",
      "Epoch 850 : train_loss 5.030034301757812\n",
      "Epoch 850 : test_loss 5.09396337890625\n",
      "Epoch 851 : train_loss 5.0420902099609375\n",
      "Epoch 851 : test_loss 5.11532861328125\n",
      "Epoch 852 : train_loss 5.039156982421875\n",
      "Epoch 852 : test_loss 5.10940869140625\n",
      "Epoch 853 : train_loss 5.0501644287109375\n",
      "Epoch 853 : test_loss 5.097458740234375\n",
      "Epoch 854 : train_loss 5.018131713867188\n",
      "Epoch 854 : test_loss 5.108304931640625\n",
      "Epoch 855 : train_loss 5.035400390625\n",
      "Epoch 855 : test_loss 5.099492065429687\n",
      "Epoch 856 : train_loss 5.026145385742187\n",
      "Epoch 856 : test_loss 5.119537109375\n",
      "Epoch 857 : train_loss 5.0433088989257815\n",
      "Epoch 857 : test_loss 5.112924682617187\n",
      "Epoch 858 : train_loss 5.0345947265625\n",
      "Epoch 858 : test_loss 5.082479248046875\n",
      "Epoch 859 : train_loss 5.0676788940429685\n",
      "Epoch 859 : test_loss 5.093964111328125\n",
      "Epoch 860 : train_loss 5.027188598632812\n",
      "Epoch 860 : test_loss 5.108869140625\n",
      "Epoch 861 : train_loss 5.049792602539062\n",
      "Epoch 861 : test_loss 5.106719970703125\n",
      "Epoch 862 : train_loss 5.051425659179688\n",
      "Epoch 862 : test_loss 5.0873521728515625\n",
      "Epoch 863 : train_loss 5.034515991210937\n",
      "Epoch 863 : test_loss 5.095943603515625\n",
      "Epoch 864 : train_loss 5.032187133789063\n",
      "Epoch 864 : test_loss 5.117060791015625\n",
      "Epoch 865 : train_loss 5.038218017578125\n",
      "Epoch 865 : test_loss 5.10154736328125\n",
      "Epoch 866 : train_loss 5.039749633789063\n",
      "Epoch 866 : test_loss 5.100765747070312\n",
      "Epoch 867 : train_loss 5.05125\n",
      "Epoch 867 : test_loss 5.105457763671875\n",
      "Epoch 868 : train_loss 5.049181945800782\n",
      "Epoch 868 : test_loss 5.1346142578125\n",
      "Epoch 869 : train_loss 5.029046508789063\n",
      "Epoch 869 : test_loss 5.116835205078125\n",
      "Epoch 870 : train_loss 5.029550903320312\n",
      "Epoch 870 : test_loss 5.103817138671875\n",
      "Epoch 871 : train_loss 5.035768798828125\n",
      "Epoch 871 : test_loss 5.09795947265625\n",
      "Epoch 872 : train_loss 5.056495361328125\n",
      "Epoch 872 : test_loss 5.095564575195312\n",
      "Epoch 873 : train_loss 5.044685974121093\n",
      "Epoch 873 : test_loss 5.092343994140625\n",
      "Epoch 874 : train_loss 5.024916015625\n",
      "Epoch 874 : test_loss 5.099599487304688\n",
      "Epoch 875 : train_loss 5.036865600585937\n",
      "Epoch 875 : test_loss 5.090370361328125\n",
      "Epoch 876 : train_loss 5.034280578613282\n",
      "Epoch 876 : test_loss 5.0863779296875\n",
      "Epoch 877 : train_loss 5.043185913085938\n",
      "Epoch 877 : test_loss 5.102705810546875\n",
      "Epoch 878 : train_loss 5.05298828125\n",
      "Epoch 878 : test_loss 5.11704931640625\n",
      "Epoch 879 : train_loss 5.0384260864257815\n",
      "Epoch 879 : test_loss 5.1207412109375\n",
      "Epoch 880 : train_loss 5.050573120117187\n",
      "Epoch 880 : test_loss 5.105720947265625\n",
      "Epoch 881 : train_loss 5.043610717773437\n",
      "Epoch 881 : test_loss 5.101069091796875\n",
      "Epoch 882 : train_loss 5.0425045166015625\n",
      "Epoch 882 : test_loss 5.09707861328125\n",
      "Epoch 883 : train_loss 5.03119091796875\n",
      "Epoch 883 : test_loss 5.10291796875\n",
      "Epoch 884 : train_loss 5.039768920898437\n",
      "Epoch 884 : test_loss 5.12978466796875\n",
      "Epoch 885 : train_loss 5.028017333984375\n",
      "Epoch 885 : test_loss 5.0709013671875\n",
      "Epoch 886 : train_loss 5.049376586914063\n",
      "Epoch 886 : test_loss 5.097209716796875\n",
      "Epoch 887 : train_loss 5.052386596679687\n",
      "Epoch 887 : test_loss 5.1071865234375\n",
      "Epoch 888 : train_loss 5.0445230712890625\n",
      "Epoch 888 : test_loss 5.096630615234375\n",
      "Epoch 889 : train_loss 5.04223583984375\n",
      "Epoch 889 : test_loss 5.098441040039062\n",
      "Epoch 890 : train_loss 5.02465673828125\n",
      "Epoch 890 : test_loss 5.109498291015625\n",
      "Epoch 891 : train_loss 5.054971801757812\n",
      "Epoch 891 : test_loss 5.093759155273437\n",
      "Epoch 892 : train_loss 5.0377740478515625\n",
      "Epoch 892 : test_loss 5.09994873046875\n",
      "Epoch 893 : train_loss 5.03824755859375\n",
      "Epoch 893 : test_loss 5.107310302734375\n",
      "Epoch 894 : train_loss 5.056463256835937\n",
      "Epoch 894 : test_loss 5.111277099609375\n",
      "Epoch 895 : train_loss 5.029111083984375\n",
      "Epoch 895 : test_loss 5.089849487304687\n",
      "Epoch 896 : train_loss 5.029236083984375\n",
      "Epoch 896 : test_loss 5.112429931640625\n",
      "Epoch 897 : train_loss 5.039728942871093\n",
      "Epoch 897 : test_loss 5.098095703125\n",
      "Epoch 898 : train_loss 5.02630224609375\n",
      "Epoch 898 : test_loss 5.097379150390625\n",
      "Epoch 899 : train_loss 5.032988098144531\n",
      "Epoch 899 : test_loss 5.09237353515625\n",
      "Epoch 900 : train_loss 5.0265927734375\n",
      "Epoch 900 : test_loss 5.104221435546875\n",
      "Epoch 901 : train_loss 5.027669921875\n",
      "Epoch 901 : test_loss 5.106766235351563\n",
      "Epoch 902 : train_loss 5.045971374511719\n",
      "Epoch 902 : test_loss 5.102320190429688\n",
      "Epoch 903 : train_loss 5.033939819335938\n",
      "Epoch 903 : test_loss 5.1347724609375\n",
      "Epoch 904 : train_loss 5.041594848632813\n",
      "Epoch 904 : test_loss 5.120228515625\n",
      "Epoch 905 : train_loss 5.043662475585937\n",
      "Epoch 905 : test_loss 5.079237060546875\n",
      "Epoch 906 : train_loss 5.049116333007812\n",
      "Epoch 906 : test_loss 5.113659423828125\n",
      "Epoch 907 : train_loss 5.0455048828125\n",
      "Epoch 907 : test_loss 5.087816284179688\n",
      "Epoch 908 : train_loss 5.03951953125\n",
      "Epoch 908 : test_loss 5.092411010742188\n",
      "Epoch 909 : train_loss 5.043322998046875\n",
      "Epoch 909 : test_loss 5.09567578125\n",
      "Epoch 910 : train_loss 5.030607360839844\n",
      "Epoch 910 : test_loss 5.100874633789062\n",
      "Epoch 911 : train_loss 5.036081115722657\n",
      "Epoch 911 : test_loss 5.107194091796875\n",
      "Epoch 912 : train_loss 5.044729125976563\n",
      "Epoch 912 : test_loss 5.107395751953125\n",
      "Epoch 913 : train_loss 5.049021850585937\n",
      "Epoch 913 : test_loss 5.1075853271484375\n",
      "Epoch 914 : train_loss 5.043300170898437\n",
      "Epoch 914 : test_loss 5.110906494140625\n",
      "Epoch 915 : train_loss 5.034753295898438\n",
      "Epoch 915 : test_loss 5.095978393554687\n",
      "Epoch 916 : train_loss 5.045017395019531\n",
      "Epoch 916 : test_loss 5.092803955078125\n",
      "Epoch 917 : train_loss 5.026642333984375\n",
      "Epoch 917 : test_loss 5.096259033203125\n",
      "Epoch 918 : train_loss 5.044445251464844\n",
      "Epoch 918 : test_loss 5.118940673828125\n",
      "Epoch 919 : train_loss 5.0374952392578125\n",
      "Epoch 919 : test_loss 5.095587890625\n",
      "Epoch 920 : train_loss 5.034518249511719\n",
      "Epoch 920 : test_loss 5.10355126953125\n",
      "Epoch 921 : train_loss 5.057461303710937\n",
      "Epoch 921 : test_loss 5.114201904296875\n",
      "Epoch 922 : train_loss 5.0451650390625\n",
      "Epoch 922 : test_loss 5.11666162109375\n",
      "Epoch 923 : train_loss 5.03973974609375\n",
      "Epoch 923 : test_loss 5.109437133789062\n",
      "Epoch 924 : train_loss 5.038444641113281\n",
      "Epoch 924 : test_loss 5.094348266601562\n",
      "Epoch 925 : train_loss 5.033525268554688\n",
      "Epoch 925 : test_loss 5.1138125\n",
      "Epoch 926 : train_loss 5.045525634765625\n",
      "Epoch 926 : test_loss 5.1121591796875\n",
      "Epoch 927 : train_loss 5.0482998046875\n",
      "Epoch 927 : test_loss 5.103474365234375\n",
      "Epoch 928 : train_loss 5.042545654296875\n",
      "Epoch 928 : test_loss 5.12072216796875\n",
      "Epoch 929 : train_loss 5.039929321289063\n",
      "Epoch 929 : test_loss 5.103193237304687\n",
      "Epoch 930 : train_loss 5.043311767578125\n",
      "Epoch 930 : test_loss 5.088673095703125\n",
      "Epoch 931 : train_loss 5.031634338378907\n",
      "Epoch 931 : test_loss 5.10086279296875\n",
      "Epoch 932 : train_loss 5.017966796875\n",
      "Epoch 932 : test_loss 5.089319702148438\n",
      "Epoch 933 : train_loss 5.036999450683593\n",
      "Epoch 933 : test_loss 5.11340087890625\n",
      "Epoch 934 : train_loss 5.045849243164063\n",
      "Epoch 934 : test_loss 5.11087548828125\n",
      "Epoch 935 : train_loss 5.034024536132812\n",
      "Epoch 935 : test_loss 5.11997412109375\n",
      "Epoch 936 : train_loss 5.047764404296875\n",
      "Epoch 936 : test_loss 5.092593994140625\n",
      "Epoch 937 : train_loss 5.055306396484375\n",
      "Epoch 937 : test_loss 5.10354052734375\n",
      "Epoch 938 : train_loss 5.038121337890625\n",
      "Epoch 938 : test_loss 5.0945087890625\n",
      "Epoch 939 : train_loss 5.032403564453125\n",
      "Epoch 939 : test_loss 5.099891357421875\n",
      "Epoch 940 : train_loss 5.040062744140625\n",
      "Epoch 940 : test_loss 5.114709228515625\n",
      "Epoch 941 : train_loss 5.042547485351562\n",
      "Epoch 941 : test_loss 5.10995654296875\n",
      "Epoch 942 : train_loss 5.038758728027344\n",
      "Epoch 942 : test_loss 5.1012255859375\n",
      "Epoch 943 : train_loss 5.045236206054687\n",
      "Epoch 943 : test_loss 5.11332568359375\n",
      "Epoch 944 : train_loss 5.037874755859375\n",
      "Epoch 944 : test_loss 5.109128662109375\n",
      "Epoch 945 : train_loss 5.042601989746093\n",
      "Epoch 945 : test_loss 5.08948974609375\n",
      "Epoch 946 : train_loss 5.046090209960938\n",
      "Epoch 946 : test_loss 5.118108520507812\n",
      "Epoch 947 : train_loss 5.039931579589844\n",
      "Epoch 947 : test_loss 5.109040893554687\n",
      "Epoch 948 : train_loss 5.040858520507813\n",
      "Epoch 948 : test_loss 5.09155322265625\n",
      "Epoch 949 : train_loss 5.037933837890625\n",
      "Epoch 949 : test_loss 5.09851025390625\n",
      "Epoch 950 : train_loss 5.026403442382812\n",
      "Epoch 950 : test_loss 5.08271630859375\n",
      "Epoch 951 : train_loss 5.0286166381835935\n",
      "Epoch 951 : test_loss 5.0870205078125\n",
      "Epoch 952 : train_loss 5.038619201660156\n",
      "Epoch 952 : test_loss 5.10455029296875\n",
      "Epoch 953 : train_loss 5.026772827148437\n",
      "Epoch 953 : test_loss 5.090681762695312\n",
      "Epoch 954 : train_loss 5.043379516601562\n",
      "Epoch 954 : test_loss 5.109238159179688\n",
      "Epoch 955 : train_loss 5.041883178710938\n",
      "Epoch 955 : test_loss 5.1047265625\n",
      "Epoch 956 : train_loss 5.029649353027343\n",
      "Epoch 956 : test_loss 5.1065\n",
      "Epoch 957 : train_loss 5.047900390625\n",
      "Epoch 957 : test_loss 5.101770751953125\n",
      "Epoch 958 : train_loss 5.035955749511719\n",
      "Epoch 958 : test_loss 5.10577294921875\n",
      "Epoch 959 : train_loss 5.050923889160156\n",
      "Epoch 959 : test_loss 5.11546923828125\n",
      "Epoch 960 : train_loss 5.045098999023438\n",
      "Epoch 960 : test_loss 5.0957333984375\n",
      "Epoch 961 : train_loss 5.0306070556640625\n",
      "Epoch 961 : test_loss 5.112918212890625\n",
      "Epoch 962 : train_loss 5.030483032226562\n",
      "Epoch 962 : test_loss 5.119049560546875\n",
      "Epoch 963 : train_loss 5.038012878417969\n",
      "Epoch 963 : test_loss 5.099979248046875\n",
      "Epoch 964 : train_loss 5.053546752929687\n",
      "Epoch 964 : test_loss 5.089384033203125\n",
      "Epoch 965 : train_loss 5.038958984375\n",
      "Epoch 965 : test_loss 5.106430786132813\n",
      "Epoch 966 : train_loss 5.03648779296875\n",
      "Epoch 966 : test_loss 5.1057041015625\n",
      "Epoch 967 : train_loss 5.040415283203125\n",
      "Epoch 967 : test_loss 5.111617065429687\n",
      "Epoch 968 : train_loss 5.03394189453125\n",
      "Epoch 968 : test_loss 5.101369384765625\n",
      "Epoch 969 : train_loss 5.042797180175781\n",
      "Epoch 969 : test_loss 5.093021118164063\n",
      "Epoch 970 : train_loss 5.050673950195312\n",
      "Epoch 970 : test_loss 5.112946533203125\n",
      "Epoch 971 : train_loss 5.041296264648437\n",
      "Epoch 971 : test_loss 5.130193115234375\n",
      "Epoch 972 : train_loss 5.023680541992188\n",
      "Epoch 972 : test_loss 5.100605224609375\n",
      "Epoch 973 : train_loss 5.0518145141601565\n",
      "Epoch 973 : test_loss 5.11116064453125\n",
      "Epoch 974 : train_loss 5.0530786743164064\n",
      "Epoch 974 : test_loss 5.102237060546875\n",
      "Epoch 975 : train_loss 5.040683288574218\n",
      "Epoch 975 : test_loss 5.086600341796875\n",
      "Epoch 976 : train_loss 5.033887023925781\n",
      "Epoch 976 : test_loss 5.09780224609375\n",
      "Epoch 977 : train_loss 5.027222229003907\n",
      "Epoch 977 : test_loss 5.095389404296875\n",
      "Epoch 978 : train_loss 5.029861328125\n",
      "Epoch 978 : test_loss 5.097005615234375\n",
      "Epoch 979 : train_loss 5.0466267700195315\n",
      "Epoch 979 : test_loss 5.083040893554688\n",
      "Epoch 980 : train_loss 5.047659790039062\n",
      "Epoch 980 : test_loss 5.097091552734375\n",
      "Epoch 981 : train_loss 5.033631286621094\n",
      "Epoch 981 : test_loss 5.107221069335938\n",
      "Epoch 982 : train_loss 5.05352001953125\n",
      "Epoch 982 : test_loss 5.1090244140625\n",
      "Epoch 983 : train_loss 5.0242593994140625\n",
      "Epoch 983 : test_loss 5.103938720703125\n",
      "Epoch 984 : train_loss 5.04713037109375\n",
      "Epoch 984 : test_loss 5.120341064453125\n",
      "Epoch 985 : train_loss 5.046322265625\n",
      "Epoch 985 : test_loss 5.0956708984375\n",
      "Epoch 986 : train_loss 5.030429748535156\n",
      "Epoch 986 : test_loss 5.10796435546875\n",
      "Epoch 987 : train_loss 5.040729736328125\n",
      "Epoch 987 : test_loss 5.0968203125\n",
      "Epoch 988 : train_loss 5.041098937988282\n",
      "Epoch 988 : test_loss 5.0955986328125\n",
      "Epoch 989 : train_loss 5.043094665527343\n",
      "Epoch 989 : test_loss 5.120445556640625\n",
      "Epoch 990 : train_loss 5.045841552734375\n",
      "Epoch 990 : test_loss 5.114078491210938\n",
      "Epoch 991 : train_loss 5.044054565429687\n",
      "Epoch 991 : test_loss 5.092320190429687\n",
      "Epoch 992 : train_loss 5.051789855957031\n",
      "Epoch 992 : test_loss 5.102893920898437\n",
      "Epoch 993 : train_loss 5.034394287109375\n",
      "Epoch 993 : test_loss 5.119431396484375\n",
      "Epoch 994 : train_loss 5.044243041992187\n",
      "Epoch 994 : test_loss 5.108359130859375\n",
      "Epoch 995 : train_loss 5.03627197265625\n",
      "Epoch 995 : test_loss 5.130798828125\n",
      "Epoch 996 : train_loss 5.046109436035156\n",
      "Epoch 996 : test_loss 5.128021240234375\n",
      "Epoch 997 : train_loss 5.028289428710938\n",
      "Epoch 997 : test_loss 5.080164184570313\n",
      "Epoch 998 : train_loss 5.040069580078125\n",
      "Epoch 998 : test_loss 5.0811767578125\n",
      "Epoch 999 : train_loss 5.0337286376953125\n",
      "Epoch 999 : test_loss 5.11556591796875\n",
      "Training with 5000\n",
      "saved model\n",
      "Epoch 0 : train_loss 5.447849462890625\n",
      "Epoch 0 : test_loss 5.52412060546875\n",
      "saved model\n",
      "Epoch 1 : train_loss 5.405885498046875\n",
      "Epoch 1 : test_loss 5.473620849609375\n",
      "saved model\n",
      "Epoch 2 : train_loss 5.365962866210937\n",
      "Epoch 2 : test_loss 5.44289892578125\n",
      "saved model\n",
      "Epoch 3 : train_loss 5.315328637695313\n",
      "Epoch 3 : test_loss 5.405765625\n",
      "saved model\n",
      "Epoch 4 : train_loss 5.271797119140625\n",
      "Epoch 4 : test_loss 5.336174560546875\n",
      "saved model\n",
      "Epoch 5 : train_loss 5.224291357421875\n",
      "Epoch 5 : test_loss 5.29634423828125\n",
      "saved model\n",
      "Epoch 6 : train_loss 5.181403125\n",
      "Epoch 6 : test_loss 5.247735107421875\n",
      "Epoch 7 : train_loss 5.149265283203125\n",
      "Epoch 7 : test_loss 5.250697021484375\n",
      "saved model\n",
      "Epoch 8 : train_loss 5.110409594726563\n",
      "Epoch 8 : test_loss 5.2073720703125\n",
      "saved model\n",
      "Epoch 9 : train_loss 5.09031923828125\n",
      "Epoch 9 : test_loss 5.18715283203125\n",
      "saved model\n",
      "Epoch 10 : train_loss 5.067594970703125\n",
      "Epoch 10 : test_loss 5.140422607421875\n",
      "saved model\n",
      "Epoch 11 : train_loss 5.038997509765625\n",
      "Epoch 11 : test_loss 5.114288330078125\n",
      "Epoch 12 : train_loss 5.018787194824219\n",
      "Epoch 12 : test_loss 5.123693603515625\n",
      "saved model\n",
      "Epoch 13 : train_loss 4.99354853515625\n",
      "Epoch 13 : test_loss 5.086205322265625\n",
      "saved model\n",
      "Epoch 14 : train_loss 4.9688028076171875\n",
      "Epoch 14 : test_loss 5.0683935546875\n",
      "saved model\n",
      "Epoch 15 : train_loss 4.961766723632812\n",
      "Epoch 15 : test_loss 5.038401611328125\n",
      "Epoch 16 : train_loss 4.9460953125\n",
      "Epoch 16 : test_loss 5.051166381835937\n",
      "saved model\n",
      "Epoch 17 : train_loss 4.921209594726562\n",
      "Epoch 17 : test_loss 5.027410400390625\n",
      "saved model\n",
      "Epoch 18 : train_loss 4.918187036132813\n",
      "Epoch 18 : test_loss 5.001777099609375\n",
      "saved model\n",
      "Epoch 19 : train_loss 4.884528735351562\n",
      "Epoch 19 : test_loss 4.986766845703125\n",
      "saved model\n",
      "Epoch 20 : train_loss 4.876054296875\n",
      "Epoch 20 : test_loss 4.966795166015625\n",
      "saved model\n",
      "Epoch 21 : train_loss 4.866218505859375\n",
      "Epoch 21 : test_loss 4.947481689453125\n",
      "saved model\n",
      "Epoch 22 : train_loss 4.842167858886719\n",
      "Epoch 22 : test_loss 4.927238891601562\n",
      "saved model\n",
      "Epoch 23 : train_loss 4.834150512695312\n",
      "Epoch 23 : test_loss 4.906885986328125\n",
      "Epoch 24 : train_loss 4.824140869140625\n",
      "Epoch 24 : test_loss 4.909858154296875\n",
      "saved model\n",
      "Epoch 25 : train_loss 4.81297744140625\n",
      "Epoch 25 : test_loss 4.889839721679688\n",
      "saved model\n",
      "Epoch 26 : train_loss 4.800226538085938\n",
      "Epoch 26 : test_loss 4.861205078125\n",
      "saved model\n",
      "Epoch 27 : train_loss 4.7877146484375\n",
      "Epoch 27 : test_loss 4.85663720703125\n",
      "saved model\n",
      "Epoch 28 : train_loss 4.773581286621094\n",
      "Epoch 28 : test_loss 4.839720581054688\n",
      "saved model\n",
      "Epoch 29 : train_loss 4.763277258300781\n",
      "Epoch 29 : test_loss 4.827735107421875\n",
      "saved model\n",
      "Epoch 30 : train_loss 4.75256298828125\n",
      "Epoch 30 : test_loss 4.826855590820313\n",
      "saved model\n",
      "Epoch 31 : train_loss 4.7422174438476565\n",
      "Epoch 31 : test_loss 4.800860473632812\n",
      "saved model\n",
      "Epoch 32 : train_loss 4.7302015625\n",
      "Epoch 32 : test_loss 4.777114501953125\n",
      "Epoch 33 : train_loss 4.718649462890625\n",
      "Epoch 33 : test_loss 4.777588745117187\n",
      "saved model\n",
      "Epoch 34 : train_loss 4.69884208984375\n",
      "Epoch 34 : test_loss 4.749825927734375\n",
      "saved model\n",
      "Epoch 35 : train_loss 4.689461669921875\n",
      "Epoch 35 : test_loss 4.739718017578125\n",
      "saved model\n",
      "Epoch 36 : train_loss 4.664628161621094\n",
      "Epoch 36 : test_loss 4.72934521484375\n",
      "saved model\n",
      "Epoch 37 : train_loss 4.6486107421875\n",
      "Epoch 37 : test_loss 4.697437744140625\n",
      "Epoch 38 : train_loss 4.634540161132812\n",
      "Epoch 38 : test_loss 4.702631103515625\n",
      "saved model\n",
      "Epoch 39 : train_loss 4.620858630371094\n",
      "Epoch 39 : test_loss 4.67440673828125\n",
      "saved model\n",
      "Epoch 40 : train_loss 4.593532421875\n",
      "Epoch 40 : test_loss 4.662802001953125\n",
      "saved model\n",
      "Epoch 41 : train_loss 4.58360859375\n",
      "Epoch 41 : test_loss 4.612071533203125\n",
      "saved model\n",
      "Epoch 42 : train_loss 4.57040576171875\n",
      "Epoch 42 : test_loss 4.610965942382813\n",
      "saved model\n",
      "Epoch 43 : train_loss 4.5540865112304685\n",
      "Epoch 43 : test_loss 4.591822265625\n",
      "saved model\n",
      "Epoch 44 : train_loss 4.52036875\n",
      "Epoch 44 : test_loss 4.58030419921875\n",
      "saved model\n",
      "Epoch 45 : train_loss 4.515537084960937\n",
      "Epoch 45 : test_loss 4.5637275390625\n",
      "saved model\n",
      "Epoch 46 : train_loss 4.505995043945313\n",
      "Epoch 46 : test_loss 4.558326171875\n",
      "saved model\n",
      "Epoch 47 : train_loss 4.479680456542969\n",
      "Epoch 47 : test_loss 4.5156578369140625\n",
      "saved model\n",
      "Epoch 48 : train_loss 4.4557791015625\n",
      "Epoch 48 : test_loss 4.496836669921875\n",
      "Epoch 49 : train_loss 4.452135864257812\n",
      "Epoch 49 : test_loss 4.496994873046875\n",
      "saved model\n",
      "Epoch 50 : train_loss 4.430213940429687\n",
      "Epoch 50 : test_loss 4.476790771484375\n",
      "Epoch 51 : train_loss 4.412599389648437\n",
      "Epoch 51 : test_loss 4.4853740234375\n",
      "saved model\n",
      "Epoch 52 : train_loss 4.395436401367188\n",
      "Epoch 52 : test_loss 4.451208374023437\n",
      "saved model\n",
      "Epoch 53 : train_loss 4.382158984375\n",
      "Epoch 53 : test_loss 4.437746826171875\n",
      "saved model\n",
      "Epoch 54 : train_loss 4.3680789672851565\n",
      "Epoch 54 : test_loss 4.433257568359375\n",
      "saved model\n",
      "Epoch 55 : train_loss 4.351341186523437\n",
      "Epoch 55 : test_loss 4.41087109375\n",
      "saved model\n",
      "Epoch 56 : train_loss 4.330291015625\n",
      "Epoch 56 : test_loss 4.409978515625\n",
      "saved model\n",
      "Epoch 57 : train_loss 4.31833505859375\n",
      "Epoch 57 : test_loss 4.367479858398437\n",
      "saved model\n",
      "Epoch 58 : train_loss 4.307818481445312\n",
      "Epoch 58 : test_loss 4.355845703125\n",
      "saved model\n",
      "Epoch 59 : train_loss 4.289349072265625\n",
      "Epoch 59 : test_loss 4.344891235351563\n",
      "Epoch 60 : train_loss 4.271977172851562\n",
      "Epoch 60 : test_loss 4.349969360351563\n",
      "saved model\n",
      "Epoch 61 : train_loss 4.254156103515625\n",
      "Epoch 61 : test_loss 4.3305869140625\n",
      "saved model\n",
      "Epoch 62 : train_loss 4.240242651367187\n",
      "Epoch 62 : test_loss 4.28576611328125\n",
      "saved model\n",
      "Epoch 63 : train_loss 4.23025556640625\n",
      "Epoch 63 : test_loss 4.282881103515625\n",
      "saved model\n",
      "Epoch 64 : train_loss 4.215319177246093\n",
      "Epoch 64 : test_loss 4.258218017578125\n",
      "Epoch 65 : train_loss 4.196316821289063\n",
      "Epoch 65 : test_loss 4.2612529296875\n",
      "saved model\n",
      "Epoch 66 : train_loss 4.188523107910156\n",
      "Epoch 66 : test_loss 4.2465791015625\n",
      "saved model\n",
      "Epoch 67 : train_loss 4.167323852539062\n",
      "Epoch 67 : test_loss 4.236117431640625\n",
      "saved model\n",
      "Epoch 68 : train_loss 4.153230236816406\n",
      "Epoch 68 : test_loss 4.201329223632812\n",
      "saved model\n",
      "Epoch 69 : train_loss 4.136878552246094\n",
      "Epoch 69 : test_loss 4.193706298828125\n",
      "saved model\n",
      "Epoch 70 : train_loss 4.126231604003906\n",
      "Epoch 70 : test_loss 4.184853515625\n",
      "saved model\n",
      "Epoch 71 : train_loss 4.114868505859375\n",
      "Epoch 71 : test_loss 4.178421875\n",
      "Epoch 72 : train_loss 4.0985180908203125\n",
      "Epoch 72 : test_loss 4.180437622070312\n",
      "saved model\n",
      "Epoch 73 : train_loss 4.078976525878907\n",
      "Epoch 73 : test_loss 4.1337021484375\n",
      "saved model\n",
      "Epoch 74 : train_loss 4.073184399414062\n",
      "Epoch 74 : test_loss 4.115521362304688\n",
      "Epoch 75 : train_loss 4.064767993164063\n",
      "Epoch 75 : test_loss 4.119453491210938\n",
      "saved model\n",
      "Epoch 76 : train_loss 4.0454797973632814\n",
      "Epoch 76 : test_loss 4.09180615234375\n",
      "Epoch 77 : train_loss 4.038835009765625\n",
      "Epoch 77 : test_loss 4.106749877929688\n",
      "saved model\n",
      "Epoch 78 : train_loss 4.015449011230468\n",
      "Epoch 78 : test_loss 4.05903076171875\n",
      "saved model\n",
      "Epoch 79 : train_loss 4.002043823242188\n",
      "Epoch 79 : test_loss 4.054019775390625\n",
      "saved model\n",
      "Epoch 80 : train_loss 3.97954033203125\n",
      "Epoch 80 : test_loss 4.00808935546875\n",
      "Epoch 81 : train_loss 3.9676668212890625\n",
      "Epoch 81 : test_loss 4.019449584960937\n",
      "Epoch 82 : train_loss 3.9579629638671876\n",
      "Epoch 82 : test_loss 4.01264453125\n",
      "saved model\n",
      "Epoch 83 : train_loss 3.9469427490234374\n",
      "Epoch 83 : test_loss 3.99371728515625\n",
      "saved model\n",
      "Epoch 84 : train_loss 3.926390576171875\n",
      "Epoch 84 : test_loss 3.9738446044921876\n",
      "saved model\n",
      "Epoch 85 : train_loss 3.9040151123046876\n",
      "Epoch 85 : test_loss 3.959401611328125\n",
      "saved model\n",
      "Epoch 86 : train_loss 3.90594228515625\n",
      "Epoch 86 : test_loss 3.9347694091796876\n",
      "Epoch 87 : train_loss 3.88618154296875\n",
      "Epoch 87 : test_loss 3.94248974609375\n",
      "Epoch 88 : train_loss 3.8742963623046873\n",
      "Epoch 88 : test_loss 3.955048828125\n",
      "saved model\n",
      "Epoch 89 : train_loss 3.8598564453125\n",
      "Epoch 89 : test_loss 3.90630859375\n",
      "saved model\n",
      "Epoch 90 : train_loss 3.8397396240234376\n",
      "Epoch 90 : test_loss 3.899492919921875\n",
      "saved model\n",
      "Epoch 91 : train_loss 3.8262520751953124\n",
      "Epoch 91 : test_loss 3.888739501953125\n",
      "saved model\n",
      "Epoch 92 : train_loss 3.809584619140625\n",
      "Epoch 92 : test_loss 3.873067626953125\n",
      "saved model\n",
      "Epoch 93 : train_loss 3.805234521484375\n",
      "Epoch 93 : test_loss 3.8711923828125\n",
      "saved model\n",
      "Epoch 94 : train_loss 3.7863941284179687\n",
      "Epoch 94 : test_loss 3.860638427734375\n",
      "saved model\n",
      "Epoch 95 : train_loss 3.76054208984375\n",
      "Epoch 95 : test_loss 3.799665771484375\n",
      "Epoch 96 : train_loss 3.7558354370117186\n",
      "Epoch 96 : test_loss 3.8000897216796874\n",
      "Epoch 97 : train_loss 3.7421863525390626\n",
      "Epoch 97 : test_loss 3.81204248046875\n",
      "Epoch 98 : train_loss 3.7421351806640626\n",
      "Epoch 98 : test_loss 3.8013905029296877\n",
      "Epoch 99 : train_loss 3.7264609619140625\n",
      "Epoch 99 : test_loss 3.8013980712890625\n",
      "saved model\n",
      "Epoch 100 : train_loss 3.7125817138671877\n",
      "Epoch 100 : test_loss 3.7299007568359377\n",
      "saved model\n",
      "Epoch 101 : train_loss 3.681182568359375\n",
      "Epoch 101 : test_loss 3.722428955078125\n",
      "Epoch 102 : train_loss 3.6543607177734376\n",
      "Epoch 102 : test_loss 3.7241348876953126\n",
      "saved model\n",
      "Epoch 103 : train_loss 3.6379091430664063\n",
      "Epoch 103 : test_loss 3.72029052734375\n",
      "Epoch 104 : train_loss 3.639941174316406\n",
      "Epoch 104 : test_loss 3.731216796875\n",
      "Epoch 105 : train_loss 3.6546076171875\n",
      "Epoch 105 : test_loss 3.755412841796875\n",
      "Epoch 106 : train_loss 3.657940185546875\n",
      "Epoch 106 : test_loss 3.74070654296875\n",
      "saved model\n",
      "Epoch 107 : train_loss 3.6367972412109375\n",
      "Epoch 107 : test_loss 3.6622325439453123\n",
      "saved model\n",
      "Epoch 108 : train_loss 3.5889147827148435\n",
      "Epoch 108 : test_loss 3.647858154296875\n",
      "saved model\n",
      "Epoch 109 : train_loss 3.5657342407226564\n",
      "Epoch 109 : test_loss 3.62111181640625\n",
      "saved model\n",
      "Epoch 110 : train_loss 3.5549815307617187\n",
      "Epoch 110 : test_loss 3.60495556640625\n",
      "Epoch 111 : train_loss 3.5456638427734375\n",
      "Epoch 111 : test_loss 3.612084228515625\n",
      "saved model\n",
      "Epoch 112 : train_loss 3.554143212890625\n",
      "Epoch 112 : test_loss 3.5962554931640627\n",
      "Epoch 113 : train_loss 3.555290869140625\n",
      "Epoch 113 : test_loss 3.611032958984375\n",
      "Epoch 114 : train_loss 3.556076086425781\n",
      "Epoch 114 : test_loss 3.6252734375\n",
      "saved model\n",
      "Epoch 115 : train_loss 3.54576474609375\n",
      "Epoch 115 : test_loss 3.58269189453125\n",
      "saved model\n",
      "Epoch 116 : train_loss 3.529973620605469\n",
      "Epoch 116 : test_loss 3.5609324951171875\n",
      "saved model\n",
      "Epoch 117 : train_loss 3.4859930419921876\n",
      "Epoch 117 : test_loss 3.525190673828125\n",
      "saved model\n",
      "Epoch 118 : train_loss 3.4477165161132812\n",
      "Epoch 118 : test_loss 3.4690926513671876\n",
      "saved model\n",
      "Epoch 119 : train_loss 3.4246374755859375\n",
      "Epoch 119 : test_loss 3.4521070556640625\n",
      "Epoch 120 : train_loss 3.4101746704101563\n",
      "Epoch 120 : test_loss 3.49210205078125\n",
      "Epoch 121 : train_loss 3.4318875854492186\n",
      "Epoch 121 : test_loss 3.5009993896484377\n",
      "Epoch 122 : train_loss 3.451799206542969\n",
      "Epoch 122 : test_loss 3.527370361328125\n",
      "Epoch 123 : train_loss 3.4329115234375\n",
      "Epoch 123 : test_loss 3.483312744140625\n",
      "saved model\n",
      "Epoch 124 : train_loss 3.3861704956054686\n",
      "Epoch 124 : test_loss 3.42356884765625\n",
      "saved model\n",
      "Epoch 125 : train_loss 3.3459749267578127\n",
      "Epoch 125 : test_loss 3.40175146484375\n",
      "saved model\n",
      "Epoch 126 : train_loss 3.330764318847656\n",
      "Epoch 126 : test_loss 3.3490506591796874\n",
      "Epoch 127 : train_loss 3.3212720458984375\n",
      "Epoch 127 : test_loss 3.3803604736328126\n",
      "Epoch 128 : train_loss 3.326551904296875\n",
      "Epoch 128 : test_loss 3.3820855712890623\n",
      "Epoch 129 : train_loss 3.3373861450195315\n",
      "Epoch 129 : test_loss 3.39071826171875\n",
      "Epoch 130 : train_loss 3.3364201416015624\n",
      "Epoch 130 : test_loss 3.389607421875\n",
      "Epoch 131 : train_loss 3.3434861206054687\n",
      "Epoch 131 : test_loss 3.3968377685546876\n",
      "Epoch 132 : train_loss 3.3346391235351565\n",
      "Epoch 132 : test_loss 3.3795604248046875\n",
      "Epoch 133 : train_loss 3.3222284912109377\n",
      "Epoch 133 : test_loss 3.3746641845703125\n",
      "Epoch 134 : train_loss 3.310361328125\n",
      "Epoch 134 : test_loss 3.3516396484375\n",
      "saved model\n",
      "Epoch 135 : train_loss 3.301781652832031\n",
      "Epoch 135 : test_loss 3.3261287841796876\n",
      "Epoch 136 : train_loss 3.282525244140625\n",
      "Epoch 136 : test_loss 3.32785888671875\n",
      "saved model\n",
      "Epoch 137 : train_loss 3.2571373291015626\n",
      "Epoch 137 : test_loss 3.2918087158203124\n",
      "saved model\n",
      "Epoch 138 : train_loss 3.228223095703125\n",
      "Epoch 138 : test_loss 3.2664876708984374\n",
      "saved model\n",
      "Epoch 139 : train_loss 3.20586787109375\n",
      "Epoch 139 : test_loss 3.2453370361328124\n",
      "saved model\n",
      "Epoch 140 : train_loss 3.1784619506835936\n",
      "Epoch 140 : test_loss 3.2011788330078126\n",
      "saved model\n",
      "Epoch 141 : train_loss 3.146412512207031\n",
      "Epoch 141 : test_loss 3.20080322265625\n",
      "saved model\n",
      "Epoch 142 : train_loss 3.127180212402344\n",
      "Epoch 142 : test_loss 3.170782470703125\n",
      "saved model\n",
      "Epoch 143 : train_loss 3.094967492675781\n",
      "Epoch 143 : test_loss 3.12803271484375\n",
      "saved model\n",
      "Epoch 144 : train_loss 3.077287353515625\n",
      "Epoch 144 : test_loss 3.117466796875\n",
      "saved model\n",
      "Epoch 145 : train_loss 3.0672078491210937\n",
      "Epoch 145 : test_loss 3.108577880859375\n",
      "Epoch 146 : train_loss 3.053829357910156\n",
      "Epoch 146 : test_loss 3.1157783203125\n",
      "Epoch 147 : train_loss 3.059465661621094\n",
      "Epoch 147 : test_loss 3.1503685302734374\n",
      "Epoch 148 : train_loss 3.063706005859375\n",
      "Epoch 148 : test_loss 3.1891685791015627\n",
      "Epoch 149 : train_loss 3.073676171875\n",
      "Epoch 149 : test_loss 3.2051798095703123\n",
      "Epoch 150 : train_loss 3.0699199951171874\n",
      "Epoch 150 : test_loss 3.187625732421875\n",
      "Epoch 151 : train_loss 3.072045764160156\n",
      "Epoch 151 : test_loss 3.192895263671875\n",
      "Epoch 152 : train_loss 3.0573371337890625\n",
      "Epoch 152 : test_loss 3.133780517578125\n",
      "Epoch 153 : train_loss 3.02322265625\n",
      "Epoch 153 : test_loss 3.1162735595703124\n",
      "saved model\n",
      "Epoch 154 : train_loss 2.9756118041992186\n",
      "Epoch 154 : test_loss 3.02254296875\n",
      "saved model\n",
      "Epoch 155 : train_loss 2.9321171142578124\n",
      "Epoch 155 : test_loss 2.979107421875\n",
      "saved model\n",
      "Epoch 156 : train_loss 2.918534875488281\n",
      "Epoch 156 : test_loss 2.9566488037109373\n",
      "saved model\n",
      "Epoch 157 : train_loss 2.9088556640625\n",
      "Epoch 157 : test_loss 2.954235107421875\n",
      "saved model\n",
      "Epoch 158 : train_loss 2.9101688110351565\n",
      "Epoch 158 : test_loss 2.9389796142578124\n",
      "saved model\n",
      "Epoch 159 : train_loss 2.8986923950195314\n",
      "Epoch 159 : test_loss 2.8999197998046875\n",
      "Epoch 160 : train_loss 2.8756933715820314\n",
      "Epoch 160 : test_loss 2.914618896484375\n",
      "Epoch 161 : train_loss 2.8747492797851564\n",
      "Epoch 161 : test_loss 2.959400390625\n",
      "Epoch 162 : train_loss 2.890085656738281\n",
      "Epoch 162 : test_loss 2.954140380859375\n",
      "Epoch 163 : train_loss 2.904394934082031\n",
      "Epoch 163 : test_loss 2.9914359130859376\n",
      "Epoch 164 : train_loss 2.89998642578125\n",
      "Epoch 164 : test_loss 2.9904498291015624\n",
      "Epoch 165 : train_loss 2.890246862792969\n",
      "Epoch 165 : test_loss 2.951479248046875\n",
      "Epoch 166 : train_loss 2.850542102050781\n",
      "Epoch 166 : test_loss 2.900426513671875\n",
      "saved model\n",
      "Epoch 167 : train_loss 2.826396594238281\n",
      "Epoch 167 : test_loss 2.861502685546875\n",
      "Epoch 168 : train_loss 2.8365675659179685\n",
      "Epoch 168 : test_loss 2.87219091796875\n",
      "Epoch 169 : train_loss 2.8609596923828127\n",
      "Epoch 169 : test_loss 2.9114129638671873\n",
      "Epoch 170 : train_loss 2.8778680908203125\n",
      "Epoch 170 : test_loss 2.864532958984375\n",
      "saved model\n",
      "Epoch 171 : train_loss 2.85290625\n",
      "Epoch 171 : test_loss 2.83274609375\n",
      "saved model\n",
      "Epoch 172 : train_loss 2.7981447998046876\n",
      "Epoch 172 : test_loss 2.8194588623046877\n",
      "saved model\n",
      "Epoch 173 : train_loss 2.7494207275390625\n",
      "Epoch 173 : test_loss 2.814739990234375\n",
      "Epoch 174 : train_loss 2.7579760986328123\n",
      "Epoch 174 : test_loss 2.856200927734375\n",
      "Epoch 175 : train_loss 2.7860480224609376\n",
      "Epoch 175 : test_loss 2.91432861328125\n",
      "Epoch 176 : train_loss 2.810486340332031\n",
      "Epoch 176 : test_loss 2.93230322265625\n",
      "Epoch 177 : train_loss 2.816869665527344\n",
      "Epoch 177 : test_loss 2.9048592529296875\n",
      "Epoch 178 : train_loss 2.79622998046875\n",
      "Epoch 178 : test_loss 2.867503662109375\n",
      "Epoch 179 : train_loss 2.7468156127929686\n",
      "Epoch 179 : test_loss 2.821803466796875\n",
      "saved model\n",
      "Epoch 180 : train_loss 2.679237097167969\n",
      "Epoch 180 : test_loss 2.727703369140625\n",
      "saved model\n",
      "Epoch 181 : train_loss 2.618569641113281\n",
      "Epoch 181 : test_loss 2.664367919921875\n",
      "saved model\n",
      "Epoch 182 : train_loss 2.5943641845703125\n",
      "Epoch 182 : test_loss 2.6284776611328127\n",
      "Epoch 183 : train_loss 2.62889658203125\n",
      "Epoch 183 : test_loss 2.650701904296875\n",
      "Epoch 184 : train_loss 2.681896032714844\n",
      "Epoch 184 : test_loss 2.6814364013671876\n",
      "Epoch 185 : train_loss 2.7353893188476563\n",
      "Epoch 185 : test_loss 2.6840521240234376\n",
      "saved model\n",
      "Epoch 186 : train_loss 2.691758752441406\n",
      "Epoch 186 : test_loss 2.6227451171875\n",
      "saved model\n",
      "Epoch 187 : train_loss 2.590489697265625\n",
      "Epoch 187 : test_loss 2.6107547607421875\n",
      "Epoch 188 : train_loss 2.5350042419433594\n",
      "Epoch 188 : test_loss 2.6477239990234374\n",
      "Epoch 189 : train_loss 2.53719853515625\n",
      "Epoch 189 : test_loss 2.6674373779296876\n",
      "Epoch 190 : train_loss 2.60196103515625\n",
      "Epoch 190 : test_loss 2.7628084716796875\n",
      "Epoch 191 : train_loss 2.64313525390625\n",
      "Epoch 191 : test_loss 2.822174560546875\n",
      "Epoch 192 : train_loss 2.69225244140625\n",
      "Epoch 192 : test_loss 2.8866734619140626\n",
      "Epoch 193 : train_loss 2.721941845703125\n",
      "Epoch 193 : test_loss 2.9016619873046876\n",
      "Epoch 194 : train_loss 2.7179322021484373\n",
      "Epoch 194 : test_loss 2.9214422607421877\n",
      "Epoch 195 : train_loss 2.7115999389648437\n",
      "Epoch 195 : test_loss 2.87518115234375\n",
      "Epoch 196 : train_loss 2.6846335815429687\n",
      "Epoch 196 : test_loss 2.820614013671875\n",
      "Epoch 197 : train_loss 2.6478369873046876\n",
      "Epoch 197 : test_loss 2.7566812744140625\n",
      "Epoch 198 : train_loss 2.5769912109375\n",
      "Epoch 198 : test_loss 2.6843580322265623\n",
      "Epoch 199 : train_loss 2.556723681640625\n",
      "Epoch 199 : test_loss 2.675279052734375\n",
      "Epoch 200 : train_loss 2.5415763549804686\n",
      "Epoch 200 : test_loss 2.6720321044921875\n",
      "Epoch 201 : train_loss 2.5319561157226564\n",
      "Epoch 201 : test_loss 2.664525634765625\n",
      "saved model\n",
      "Epoch 202 : train_loss 2.5290313842773435\n",
      "Epoch 202 : test_loss 2.607543212890625\n",
      "Epoch 203 : train_loss 2.518155499267578\n",
      "Epoch 203 : test_loss 2.645902587890625\n",
      "Epoch 204 : train_loss 2.5091410034179686\n",
      "Epoch 204 : test_loss 2.6281346435546875\n",
      "saved model\n",
      "Epoch 205 : train_loss 2.5001416259765623\n",
      "Epoch 205 : test_loss 2.605512939453125\n",
      "saved model\n",
      "Epoch 206 : train_loss 2.4902472839355467\n",
      "Epoch 206 : test_loss 2.601518798828125\n",
      "saved model\n",
      "Epoch 207 : train_loss 2.4841245483398438\n",
      "Epoch 207 : test_loss 2.5925718994140623\n",
      "saved model\n",
      "Epoch 208 : train_loss 2.4749300048828125\n",
      "Epoch 208 : test_loss 2.5922864990234373\n",
      "saved model\n",
      "Epoch 209 : train_loss 2.4682873779296877\n",
      "Epoch 209 : test_loss 2.5728590087890626\n",
      "Epoch 210 : train_loss 2.4422493469238282\n",
      "Epoch 210 : test_loss 2.57514501953125\n",
      "saved model\n",
      "Epoch 211 : train_loss 2.4441458129882814\n",
      "Epoch 211 : test_loss 2.559516357421875\n",
      "saved model\n",
      "Epoch 212 : train_loss 2.431720666503906\n",
      "Epoch 212 : test_loss 2.5301368408203126\n",
      "saved model\n",
      "Epoch 213 : train_loss 2.427775988769531\n",
      "Epoch 213 : test_loss 2.5165008544921874\n",
      "Epoch 214 : train_loss 2.42458427734375\n",
      "Epoch 214 : test_loss 2.5265760498046874\n",
      "saved model\n",
      "Epoch 215 : train_loss 2.4240861450195315\n",
      "Epoch 215 : test_loss 2.5146793212890626\n",
      "saved model\n",
      "Epoch 216 : train_loss 2.4015627807617186\n",
      "Epoch 216 : test_loss 2.4901066284179687\n",
      "saved model\n",
      "Epoch 217 : train_loss 2.4002585266113283\n",
      "Epoch 217 : test_loss 2.475133117675781\n",
      "Epoch 218 : train_loss 2.4012721435546873\n",
      "Epoch 218 : test_loss 2.4773514404296875\n",
      "Epoch 219 : train_loss 2.3857003662109375\n",
      "Epoch 219 : test_loss 2.4861715087890626\n",
      "Epoch 220 : train_loss 2.388375842285156\n",
      "Epoch 220 : test_loss 2.479733154296875\n",
      "saved model\n",
      "Epoch 221 : train_loss 2.3854514221191407\n",
      "Epoch 221 : test_loss 2.4636278076171876\n",
      "saved model\n",
      "Epoch 222 : train_loss 2.373945361328125\n",
      "Epoch 222 : test_loss 2.4492073974609374\n",
      "saved model\n",
      "Epoch 223 : train_loss 2.3786081359863283\n",
      "Epoch 223 : test_loss 2.4465596923828126\n",
      "saved model\n",
      "Epoch 224 : train_loss 2.3730547119140626\n",
      "Epoch 224 : test_loss 2.4346785278320313\n",
      "saved model\n",
      "Epoch 225 : train_loss 2.3627891357421875\n",
      "Epoch 225 : test_loss 2.424201416015625\n",
      "Epoch 226 : train_loss 2.36352783203125\n",
      "Epoch 226 : test_loss 2.426447509765625\n",
      "saved model\n",
      "Epoch 227 : train_loss 2.359285314941406\n",
      "Epoch 227 : test_loss 2.3963707275390624\n",
      "Epoch 228 : train_loss 2.3599918395996093\n",
      "Epoch 228 : test_loss 2.4346865234375\n",
      "Epoch 229 : train_loss 2.3584638549804686\n",
      "Epoch 229 : test_loss 2.4375142822265623\n",
      "Epoch 230 : train_loss 2.3479601623535156\n",
      "Epoch 230 : test_loss 2.4216189575195313\n",
      "Epoch 231 : train_loss 2.3585261840820313\n",
      "Epoch 231 : test_loss 2.4228770751953124\n",
      "Epoch 232 : train_loss 2.352844738769531\n",
      "Epoch 232 : test_loss 2.4070206909179688\n",
      "Epoch 233 : train_loss 2.3462912292480467\n",
      "Epoch 233 : test_loss 2.402639892578125\n",
      "saved model\n",
      "Epoch 234 : train_loss 2.3562711181640625\n",
      "Epoch 234 : test_loss 2.38944775390625\n",
      "Epoch 235 : train_loss 2.3490464294433595\n",
      "Epoch 235 : test_loss 2.419342529296875\n",
      "Epoch 236 : train_loss 2.347537939453125\n",
      "Epoch 236 : test_loss 2.4082294921875\n",
      "Epoch 237 : train_loss 2.3537303588867187\n",
      "Epoch 237 : test_loss 2.3894481201171875\n",
      "saved model\n",
      "Epoch 238 : train_loss 2.3547654907226563\n",
      "Epoch 238 : test_loss 2.384521240234375\n",
      "saved model\n",
      "Epoch 239 : train_loss 2.3418662292480468\n",
      "Epoch 239 : test_loss 2.371782958984375\n",
      "Epoch 240 : train_loss 2.348535217285156\n",
      "Epoch 240 : test_loss 2.3851981201171877\n",
      "Epoch 241 : train_loss 2.34687568359375\n",
      "Epoch 241 : test_loss 2.3807077026367187\n",
      "Epoch 242 : train_loss 2.347038781738281\n",
      "Epoch 242 : test_loss 2.3886695556640625\n",
      "Epoch 243 : train_loss 2.3538470947265626\n",
      "Epoch 243 : test_loss 2.39463134765625\n",
      "Epoch 244 : train_loss 2.3487821533203124\n",
      "Epoch 244 : test_loss 2.3731580810546875\n",
      "saved model\n",
      "Epoch 245 : train_loss 2.3618980590820313\n",
      "Epoch 245 : test_loss 2.3667271728515624\n",
      "Epoch 246 : train_loss 2.3453361572265625\n",
      "Epoch 246 : test_loss 2.431396240234375\n",
      "Epoch 247 : train_loss 2.348159979248047\n",
      "Epoch 247 : test_loss 2.3722135009765624\n",
      "Epoch 248 : train_loss 2.3551667419433593\n",
      "Epoch 248 : test_loss 2.3801766357421874\n",
      "saved model\n",
      "Epoch 249 : train_loss 2.3570059936523435\n",
      "Epoch 249 : test_loss 2.3630097045898437\n",
      "Epoch 250 : train_loss 2.354133508300781\n",
      "Epoch 250 : test_loss 2.3993359375\n",
      "Epoch 251 : train_loss 2.3540969482421876\n",
      "Epoch 251 : test_loss 2.3824715576171873\n",
      "Epoch 252 : train_loss 2.3500689086914064\n",
      "Epoch 252 : test_loss 2.407148193359375\n",
      "Epoch 253 : train_loss 2.3608514587402345\n",
      "Epoch 253 : test_loss 2.3953228759765626\n",
      "Epoch 254 : train_loss 2.351134069824219\n",
      "Epoch 254 : test_loss 2.3910916748046875\n",
      "Epoch 255 : train_loss 2.356692572021484\n",
      "Epoch 255 : test_loss 2.375206237792969\n",
      "Epoch 256 : train_loss 2.3428096435546877\n",
      "Epoch 256 : test_loss 2.4147381591796875\n",
      "Epoch 257 : train_loss 2.359840319824219\n",
      "Epoch 257 : test_loss 2.4041072998046875\n",
      "saved model\n",
      "Epoch 258 : train_loss 2.3477427368164063\n",
      "Epoch 258 : test_loss 2.3531741943359377\n",
      "saved model\n",
      "Epoch 259 : train_loss 2.357281494140625\n",
      "Epoch 259 : test_loss 2.325830810546875\n",
      "Epoch 260 : train_loss 2.3730574096679686\n",
      "Epoch 260 : test_loss 2.3876011962890624\n",
      "Epoch 261 : train_loss 2.3550843994140624\n",
      "Epoch 261 : test_loss 2.3986740112304688\n",
      "Epoch 262 : train_loss 2.3528364318847657\n",
      "Epoch 262 : test_loss 2.3643853759765623\n",
      "Epoch 263 : train_loss 2.3589837524414063\n",
      "Epoch 263 : test_loss 2.3715140380859374\n",
      "Epoch 264 : train_loss 2.346524523925781\n",
      "Epoch 264 : test_loss 2.3595748291015624\n",
      "Epoch 265 : train_loss 2.3486912414550782\n",
      "Epoch 265 : test_loss 2.401647216796875\n",
      "Epoch 266 : train_loss 2.3473748291015624\n",
      "Epoch 266 : test_loss 2.3726907958984373\n",
      "Epoch 267 : train_loss 2.353573907470703\n",
      "Epoch 267 : test_loss 2.3777664794921876\n",
      "Epoch 268 : train_loss 2.3376801513671874\n",
      "Epoch 268 : test_loss 2.3656678466796874\n",
      "Epoch 269 : train_loss 2.340801745605469\n",
      "Epoch 269 : test_loss 2.386876953125\n",
      "Epoch 270 : train_loss 2.3428383911132813\n",
      "Epoch 270 : test_loss 2.368687561035156\n",
      "Epoch 271 : train_loss 2.3393331481933592\n",
      "Epoch 271 : test_loss 2.3719307861328125\n",
      "Epoch 272 : train_loss 2.337168768310547\n",
      "Epoch 272 : test_loss 2.343906982421875\n",
      "Epoch 273 : train_loss 2.339733349609375\n",
      "Epoch 273 : test_loss 2.372639892578125\n",
      "Epoch 274 : train_loss 2.3399857177734376\n",
      "Epoch 274 : test_loss 2.344792236328125\n",
      "Epoch 275 : train_loss 2.3564225341796874\n",
      "Epoch 275 : test_loss 2.384671936035156\n",
      "Epoch 276 : train_loss 2.3479611267089844\n",
      "Epoch 276 : test_loss 2.344139587402344\n",
      "Epoch 277 : train_loss 2.333981719970703\n",
      "Epoch 277 : test_loss 2.40445654296875\n",
      "Epoch 278 : train_loss 2.3409006103515626\n",
      "Epoch 278 : test_loss 2.395984375\n",
      "Epoch 279 : train_loss 2.338202258300781\n",
      "Epoch 279 : test_loss 2.398766540527344\n",
      "Epoch 280 : train_loss 2.3490511108398437\n",
      "Epoch 280 : test_loss 2.3632445068359376\n",
      "Epoch 281 : train_loss 2.3499941650390626\n",
      "Epoch 281 : test_loss 2.38042138671875\n",
      "Epoch 282 : train_loss 2.3427613708496096\n",
      "Epoch 282 : test_loss 2.3571136474609373\n",
      "Epoch 283 : train_loss 2.334366491699219\n",
      "Epoch 283 : test_loss 2.3607222900390625\n",
      "Epoch 284 : train_loss 2.332941290283203\n",
      "Epoch 284 : test_loss 2.3946517944335937\n",
      "Epoch 285 : train_loss 2.3377135498046875\n",
      "Epoch 285 : test_loss 2.3520150146484373\n",
      "Epoch 286 : train_loss 2.339670098876953\n",
      "Epoch 286 : test_loss 2.3568540649414063\n",
      "saved model\n",
      "Epoch 287 : train_loss 2.3385759826660157\n",
      "Epoch 287 : test_loss 2.320059875488281\n",
      "Epoch 288 : train_loss 2.350172119140625\n",
      "Epoch 288 : test_loss 2.3558641357421877\n",
      "Epoch 289 : train_loss 2.351436669921875\n",
      "Epoch 289 : test_loss 2.3650093383789064\n",
      "Epoch 290 : train_loss 2.347006506347656\n",
      "Epoch 290 : test_loss 2.3591993408203127\n",
      "Epoch 291 : train_loss 2.3392168518066407\n",
      "Epoch 291 : test_loss 2.3762657470703124\n",
      "Epoch 292 : train_loss 2.3420330932617186\n",
      "Epoch 292 : test_loss 2.35967431640625\n",
      "Epoch 293 : train_loss 2.3370587890625\n",
      "Epoch 293 : test_loss 2.3513195190429688\n",
      "Epoch 294 : train_loss 2.3375080810546875\n",
      "Epoch 294 : test_loss 2.378050354003906\n",
      "Epoch 295 : train_loss 2.3229829223632814\n",
      "Epoch 295 : test_loss 2.365450256347656\n",
      "Epoch 296 : train_loss 2.3422320190429686\n",
      "Epoch 296 : test_loss 2.3724666748046874\n",
      "Epoch 297 : train_loss 2.3424368957519532\n",
      "Epoch 297 : test_loss 2.3338604736328126\n",
      "Epoch 298 : train_loss 2.342457763671875\n",
      "Epoch 298 : test_loss 2.3686243896484376\n",
      "Epoch 299 : train_loss 2.3470706665039063\n",
      "Epoch 299 : test_loss 2.3671898193359375\n",
      "Epoch 300 : train_loss 2.3381201904296876\n",
      "Epoch 300 : test_loss 2.352278076171875\n",
      "Epoch 301 : train_loss 2.3372884399414064\n",
      "Epoch 301 : test_loss 2.3762716064453127\n",
      "Epoch 302 : train_loss 2.3377335876464844\n",
      "Epoch 302 : test_loss 2.3451171875\n",
      "Epoch 303 : train_loss 2.33944482421875\n",
      "Epoch 303 : test_loss 2.331248229980469\n",
      "Epoch 304 : train_loss 2.346405224609375\n",
      "Epoch 304 : test_loss 2.3513099365234376\n",
      "Epoch 305 : train_loss 2.336780810546875\n",
      "Epoch 305 : test_loss 2.34211181640625\n",
      "Epoch 306 : train_loss 2.347548260498047\n",
      "Epoch 306 : test_loss 2.3456934204101563\n",
      "Epoch 307 : train_loss 2.336309765625\n",
      "Epoch 307 : test_loss 2.364736145019531\n",
      "Epoch 308 : train_loss 2.3374232421875\n",
      "Epoch 308 : test_loss 2.367890625\n",
      "Epoch 309 : train_loss 2.337328009033203\n",
      "Epoch 309 : test_loss 2.3314373779296873\n",
      "Epoch 310 : train_loss 2.341847277832031\n",
      "Epoch 310 : test_loss 2.3596181640625\n",
      "Epoch 311 : train_loss 2.340927331542969\n",
      "Epoch 311 : test_loss 2.3457533569335935\n",
      "Epoch 312 : train_loss 2.3370546264648437\n",
      "Epoch 312 : test_loss 2.3564552001953123\n",
      "Epoch 313 : train_loss 2.344685363769531\n",
      "Epoch 313 : test_loss 2.37500927734375\n",
      "Epoch 314 : train_loss 2.3371256713867186\n",
      "Epoch 314 : test_loss 2.34630224609375\n",
      "Epoch 315 : train_loss 2.345608154296875\n",
      "Epoch 315 : test_loss 2.3694808349609375\n",
      "Epoch 316 : train_loss 2.3369107299804686\n",
      "Epoch 316 : test_loss 2.351473388671875\n",
      "Epoch 317 : train_loss 2.3341701538085937\n",
      "Epoch 317 : test_loss 2.3739556884765625\n",
      "Epoch 318 : train_loss 2.3353413330078125\n",
      "Epoch 318 : test_loss 2.3743336791992187\n",
      "Epoch 319 : train_loss 2.3349750366210937\n",
      "Epoch 319 : test_loss 2.3659749755859374\n",
      "Epoch 320 : train_loss 2.346091552734375\n",
      "Epoch 320 : test_loss 2.3538742065429688\n",
      "Epoch 321 : train_loss 2.3431104858398437\n",
      "Epoch 321 : test_loss 2.386390625\n",
      "Epoch 322 : train_loss 2.333897772216797\n",
      "Epoch 322 : test_loss 2.35333154296875\n",
      "Epoch 323 : train_loss 2.3438090270996095\n",
      "Epoch 323 : test_loss 2.3664898681640625\n",
      "Epoch 324 : train_loss 2.336488024902344\n",
      "Epoch 324 : test_loss 2.370101806640625\n",
      "Epoch 325 : train_loss 2.3310474243164063\n",
      "Epoch 325 : test_loss 2.340945556640625\n",
      "Epoch 326 : train_loss 2.335806048583984\n",
      "Epoch 326 : test_loss 2.389568359375\n",
      "Epoch 327 : train_loss 2.3501338134765626\n",
      "Epoch 327 : test_loss 2.36056103515625\n",
      "Epoch 328 : train_loss 2.3458576110839844\n",
      "Epoch 328 : test_loss 2.386637451171875\n",
      "Epoch 329 : train_loss 2.3418074951171874\n",
      "Epoch 329 : test_loss 2.336597900390625\n",
      "Epoch 330 : train_loss 2.3357657897949218\n",
      "Epoch 330 : test_loss 2.374475158691406\n",
      "Epoch 331 : train_loss 2.3400475219726564\n",
      "Epoch 331 : test_loss 2.373732604980469\n",
      "Epoch 332 : train_loss 2.333273095703125\n",
      "Epoch 332 : test_loss 2.3673814697265625\n",
      "Epoch 333 : train_loss 2.334297613525391\n",
      "Epoch 333 : test_loss 2.365888427734375\n",
      "Epoch 334 : train_loss 2.33836826171875\n",
      "Epoch 334 : test_loss 2.370590393066406\n",
      "Epoch 335 : train_loss 2.3274489318847658\n",
      "Epoch 335 : test_loss 2.366082824707031\n",
      "Epoch 336 : train_loss 2.334991827392578\n",
      "Epoch 336 : test_loss 2.336\n",
      "Epoch 337 : train_loss 2.3417097717285156\n",
      "Epoch 337 : test_loss 2.3518873291015625\n",
      "Epoch 338 : train_loss 2.337442883300781\n",
      "Epoch 338 : test_loss 2.376717224121094\n",
      "Epoch 339 : train_loss 2.340123767089844\n",
      "Epoch 339 : test_loss 2.36314794921875\n",
      "Epoch 340 : train_loss 2.3421477172851564\n",
      "Epoch 340 : test_loss 2.340494567871094\n",
      "Epoch 341 : train_loss 2.338406280517578\n",
      "Epoch 341 : test_loss 2.351385009765625\n",
      "Epoch 342 : train_loss 2.3341753601074218\n",
      "Epoch 342 : test_loss 2.3710589599609375\n",
      "Epoch 343 : train_loss 2.3387117797851564\n",
      "Epoch 343 : test_loss 2.36519921875\n",
      "Epoch 344 : train_loss 2.342399639892578\n",
      "Epoch 344 : test_loss 2.364158935546875\n",
      "Epoch 345 : train_loss 2.332891912841797\n",
      "Epoch 345 : test_loss 2.36077001953125\n",
      "Epoch 346 : train_loss 2.3411968505859373\n",
      "Epoch 346 : test_loss 2.3681961669921874\n",
      "Epoch 347 : train_loss 2.3349003173828127\n",
      "Epoch 347 : test_loss 2.3936136474609375\n",
      "Epoch 348 : train_loss 2.3410319946289064\n",
      "Epoch 348 : test_loss 2.3472908935546877\n",
      "Epoch 349 : train_loss 2.342361901855469\n",
      "Epoch 349 : test_loss 2.369665283203125\n",
      "Epoch 350 : train_loss 2.3305372985839843\n",
      "Epoch 350 : test_loss 2.352604553222656\n",
      "Epoch 351 : train_loss 2.3314736083984373\n",
      "Epoch 351 : test_loss 2.3507090454101562\n",
      "Epoch 352 : train_loss 2.34404306640625\n",
      "Epoch 352 : test_loss 2.3462313842773437\n",
      "Epoch 353 : train_loss 2.3391427001953127\n",
      "Epoch 353 : test_loss 2.3577747802734375\n",
      "Epoch 354 : train_loss 2.328975079345703\n",
      "Epoch 354 : test_loss 2.3830697021484375\n",
      "Epoch 355 : train_loss 2.3314030029296875\n",
      "Epoch 355 : test_loss 2.358957946777344\n",
      "Epoch 356 : train_loss 2.3389508544921873\n",
      "Epoch 356 : test_loss 2.356935791015625\n",
      "Epoch 357 : train_loss 2.331073663330078\n",
      "Epoch 357 : test_loss 2.3739935302734376\n",
      "Epoch 358 : train_loss 2.344403942871094\n",
      "Epoch 358 : test_loss 2.3694556884765623\n",
      "Epoch 359 : train_loss 2.330040637207031\n",
      "Epoch 359 : test_loss 2.3544036865234377\n",
      "Epoch 360 : train_loss 2.34040205078125\n",
      "Epoch 360 : test_loss 2.3472401123046875\n",
      "Epoch 361 : train_loss 2.327673449707031\n",
      "Epoch 361 : test_loss 2.366310302734375\n",
      "Epoch 362 : train_loss 2.337882189941406\n",
      "Epoch 362 : test_loss 2.3717158203125\n",
      "Epoch 363 : train_loss 2.341918310546875\n",
      "Epoch 363 : test_loss 2.3564910888671875\n",
      "saved model\n",
      "Epoch 364 : train_loss 2.333799267578125\n",
      "Epoch 364 : test_loss 2.3159493408203127\n",
      "Epoch 365 : train_loss 2.330628649902344\n",
      "Epoch 365 : test_loss 2.3681129150390623\n",
      "Epoch 366 : train_loss 2.3346572875976563\n",
      "Epoch 366 : test_loss 2.3475911865234376\n",
      "Epoch 367 : train_loss 2.333469970703125\n",
      "Epoch 367 : test_loss 2.3886782836914064\n",
      "Epoch 368 : train_loss 2.3348210205078126\n",
      "Epoch 368 : test_loss 2.375808288574219\n",
      "Epoch 369 : train_loss 2.340504736328125\n",
      "Epoch 369 : test_loss 2.3566865234375\n",
      "Epoch 370 : train_loss 2.335302233886719\n",
      "Epoch 370 : test_loss 2.36797021484375\n",
      "Epoch 371 : train_loss 2.335003674316406\n",
      "Epoch 371 : test_loss 2.3556417846679687\n",
      "Epoch 372 : train_loss 2.329333557128906\n",
      "Epoch 372 : test_loss 2.365929443359375\n",
      "Epoch 373 : train_loss 2.3414381652832033\n",
      "Epoch 373 : test_loss 2.35600048828125\n",
      "Epoch 374 : train_loss 2.3299244262695313\n",
      "Epoch 374 : test_loss 2.3394771728515624\n",
      "Epoch 375 : train_loss 2.3339302490234375\n",
      "Epoch 375 : test_loss 2.3683974609375\n",
      "Epoch 376 : train_loss 2.335709533691406\n",
      "Epoch 376 : test_loss 2.354732421875\n",
      "Epoch 377 : train_loss 2.3281996826171873\n",
      "Epoch 377 : test_loss 2.3642305908203123\n",
      "Epoch 378 : train_loss 2.34051611328125\n",
      "Epoch 378 : test_loss 2.3544830322265624\n",
      "Epoch 379 : train_loss 2.334298974609375\n",
      "Epoch 379 : test_loss 2.3479835205078126\n",
      "Epoch 380 : train_loss 2.34581455078125\n",
      "Epoch 380 : test_loss 2.3368013916015626\n",
      "Epoch 381 : train_loss 2.3373536376953123\n",
      "Epoch 381 : test_loss 2.3572906494140624\n",
      "Epoch 382 : train_loss 2.329616961669922\n",
      "Epoch 382 : test_loss 2.347601745605469\n",
      "Epoch 383 : train_loss 2.331303271484375\n",
      "Epoch 383 : test_loss 2.3511533203125\n",
      "Epoch 384 : train_loss 2.337879150390625\n",
      "Epoch 384 : test_loss 2.3761565551757813\n",
      "Epoch 385 : train_loss 2.3289632934570315\n",
      "Epoch 385 : test_loss 2.3261295166015623\n",
      "Epoch 386 : train_loss 2.3307438293457032\n",
      "Epoch 386 : test_loss 2.3843507080078123\n",
      "Epoch 387 : train_loss 2.332582440185547\n",
      "Epoch 387 : test_loss 2.350098205566406\n",
      "Epoch 388 : train_loss 2.3372838134765623\n",
      "Epoch 388 : test_loss 2.367864013671875\n",
      "Epoch 389 : train_loss 2.343704345703125\n",
      "Epoch 389 : test_loss 2.3630999755859374\n",
      "Epoch 390 : train_loss 2.337779998779297\n",
      "Epoch 390 : test_loss 2.336755126953125\n",
      "Epoch 391 : train_loss 2.333909033203125\n",
      "Epoch 391 : test_loss 2.3392535400390626\n",
      "Epoch 392 : train_loss 2.327551397705078\n",
      "Epoch 392 : test_loss 2.3436575927734373\n",
      "Epoch 393 : train_loss 2.338360095214844\n",
      "Epoch 393 : test_loss 2.3548106689453125\n",
      "Epoch 394 : train_loss 2.327767919921875\n",
      "Epoch 394 : test_loss 2.351110107421875\n",
      "Epoch 395 : train_loss 2.3283915893554687\n",
      "Epoch 395 : test_loss 2.3765052490234373\n",
      "Epoch 396 : train_loss 2.3353821899414062\n",
      "Epoch 396 : test_loss 2.333449035644531\n",
      "Epoch 397 : train_loss 2.3393514892578127\n",
      "Epoch 397 : test_loss 2.3673765869140624\n",
      "Epoch 398 : train_loss 2.3264301513671874\n",
      "Epoch 398 : test_loss 2.33726220703125\n",
      "Epoch 399 : train_loss 2.3233524047851564\n",
      "Epoch 399 : test_loss 2.3243189697265625\n",
      "Epoch 400 : train_loss 2.3280158569335936\n",
      "Epoch 400 : test_loss 2.3336961669921874\n",
      "Epoch 401 : train_loss 2.328348291015625\n",
      "Epoch 401 : test_loss 2.34422314453125\n",
      "Epoch 402 : train_loss 2.3279760986328126\n",
      "Epoch 402 : test_loss 2.35517626953125\n",
      "Epoch 403 : train_loss 2.3359873901367187\n",
      "Epoch 403 : test_loss 2.3431527099609375\n",
      "Epoch 404 : train_loss 2.3293261840820314\n",
      "Epoch 404 : test_loss 2.3586431884765626\n",
      "Epoch 405 : train_loss 2.32249326171875\n",
      "Epoch 405 : test_loss 2.3451285400390627\n",
      "Epoch 406 : train_loss 2.337030749511719\n",
      "Epoch 406 : test_loss 2.362921630859375\n",
      "Epoch 407 : train_loss 2.3302614379882813\n",
      "Epoch 407 : test_loss 2.340264099121094\n",
      "Epoch 408 : train_loss 2.330823004150391\n",
      "Epoch 408 : test_loss 2.3898931884765626\n",
      "Epoch 409 : train_loss 2.3356048706054686\n",
      "Epoch 409 : test_loss 2.3924718627929686\n",
      "Epoch 410 : train_loss 2.33044423828125\n",
      "Epoch 410 : test_loss 2.3737576904296875\n",
      "Epoch 411 : train_loss 2.3371531188964845\n",
      "Epoch 411 : test_loss 2.3675374755859373\n",
      "Epoch 412 : train_loss 2.3300580627441407\n",
      "Epoch 412 : test_loss 2.3591723022460935\n",
      "Epoch 413 : train_loss 2.3347876220703125\n",
      "Epoch 413 : test_loss 2.3691105346679686\n",
      "Epoch 414 : train_loss 2.335809234619141\n",
      "Epoch 414 : test_loss 2.338333984375\n",
      "Epoch 415 : train_loss 2.3303776184082032\n",
      "Epoch 415 : test_loss 2.3494599609375\n",
      "Epoch 416 : train_loss 2.332647839355469\n",
      "Epoch 416 : test_loss 2.378960693359375\n",
      "Epoch 417 : train_loss 2.3417020263671877\n",
      "Epoch 417 : test_loss 2.354842956542969\n",
      "Epoch 418 : train_loss 2.3336265258789064\n",
      "Epoch 418 : test_loss 2.3588270263671873\n",
      "Epoch 419 : train_loss 2.3322144775390625\n",
      "Epoch 419 : test_loss 2.3604888305664065\n",
      "Epoch 420 : train_loss 2.3317200256347657\n",
      "Epoch 420 : test_loss 2.3407655029296874\n",
      "Epoch 421 : train_loss 2.34255859375\n",
      "Epoch 421 : test_loss 2.352013916015625\n",
      "Epoch 422 : train_loss 2.3361444396972657\n",
      "Epoch 422 : test_loss 2.3737470703125\n",
      "Epoch 423 : train_loss 2.3434592895507813\n",
      "Epoch 423 : test_loss 2.388288818359375\n",
      "Epoch 424 : train_loss 2.3287970825195314\n",
      "Epoch 424 : test_loss 2.392766845703125\n",
      "Epoch 425 : train_loss 2.333169940185547\n",
      "Epoch 425 : test_loss 2.354664306640625\n",
      "Epoch 426 : train_loss 2.3343364868164063\n",
      "Epoch 426 : test_loss 2.351569580078125\n",
      "Epoch 427 : train_loss 2.332942468261719\n",
      "Epoch 427 : test_loss 2.347086181640625\n",
      "Epoch 428 : train_loss 2.3339246643066405\n",
      "Epoch 428 : test_loss 2.3685067138671876\n",
      "Epoch 429 : train_loss 2.3343612976074217\n",
      "Epoch 429 : test_loss 2.3226192626953126\n",
      "Epoch 430 : train_loss 2.3226099182128905\n",
      "Epoch 430 : test_loss 2.36868310546875\n",
      "Epoch 431 : train_loss 2.3430689819335937\n",
      "Epoch 431 : test_loss 2.370906982421875\n",
      "Epoch 432 : train_loss 2.341185009765625\n",
      "Epoch 432 : test_loss 2.37231640625\n",
      "Epoch 433 : train_loss 2.333303369140625\n",
      "Epoch 433 : test_loss 2.3640537109375\n",
      "Epoch 434 : train_loss 2.3270566650390627\n",
      "Epoch 434 : test_loss 2.3796612548828127\n",
      "Epoch 435 : train_loss 2.3335214599609375\n",
      "Epoch 435 : test_loss 2.3600980224609374\n",
      "Epoch 436 : train_loss 2.3295939453125\n",
      "Epoch 436 : test_loss 2.325779052734375\n",
      "Epoch 437 : train_loss 2.339546435546875\n",
      "Epoch 437 : test_loss 2.3531988525390624\n",
      "Epoch 438 : train_loss 2.335006103515625\n",
      "Epoch 438 : test_loss 2.3479281005859374\n",
      "Epoch 439 : train_loss 2.3387844299316405\n",
      "Epoch 439 : test_loss 2.35037109375\n",
      "Epoch 440 : train_loss 2.3439011901855467\n",
      "Epoch 440 : test_loss 2.3388211669921875\n",
      "Epoch 441 : train_loss 2.3416368103027345\n",
      "Epoch 441 : test_loss 2.3610577392578125\n",
      "Epoch 442 : train_loss 2.3196456970214845\n",
      "Epoch 442 : test_loss 2.3838236083984374\n",
      "Epoch 443 : train_loss 2.3210857055664063\n",
      "Epoch 443 : test_loss 2.363070068359375\n",
      "Epoch 444 : train_loss 2.3271023071289063\n",
      "Epoch 444 : test_loss 2.323294921875\n",
      "Epoch 445 : train_loss 2.333082879638672\n",
      "Epoch 445 : test_loss 2.3566819458007813\n",
      "Epoch 446 : train_loss 2.324027404785156\n",
      "Epoch 446 : test_loss 2.344586669921875\n",
      "Epoch 447 : train_loss 2.3355212463378905\n",
      "Epoch 447 : test_loss 2.355643615722656\n",
      "Epoch 448 : train_loss 2.336070928955078\n",
      "Epoch 448 : test_loss 2.35339208984375\n",
      "Epoch 449 : train_loss 2.328642431640625\n",
      "Epoch 449 : test_loss 2.3555477294921876\n",
      "Epoch 450 : train_loss 2.3368997253417967\n",
      "Epoch 450 : test_loss 2.3605078735351563\n",
      "Epoch 451 : train_loss 2.3267035400390625\n",
      "Epoch 451 : test_loss 2.37055712890625\n",
      "Epoch 452 : train_loss 2.338903466796875\n",
      "Epoch 452 : test_loss 2.371510559082031\n",
      "Epoch 453 : train_loss 2.328027783203125\n",
      "Epoch 453 : test_loss 2.3472763671875\n",
      "Epoch 454 : train_loss 2.328211962890625\n",
      "Epoch 454 : test_loss 2.354036865234375\n",
      "Epoch 455 : train_loss 2.3266777221679686\n",
      "Epoch 455 : test_loss 2.3529163818359375\n",
      "Epoch 456 : train_loss 2.329997009277344\n",
      "Epoch 456 : test_loss 2.3500252685546874\n",
      "Epoch 457 : train_loss 2.3439136657714843\n",
      "Epoch 457 : test_loss 2.3308604125976564\n",
      "Epoch 458 : train_loss 2.3321950805664065\n",
      "Epoch 458 : test_loss 2.3484984130859377\n",
      "Epoch 459 : train_loss 2.34372744140625\n",
      "Epoch 459 : test_loss 2.351451904296875\n",
      "Epoch 460 : train_loss 2.3297713134765625\n",
      "Epoch 460 : test_loss 2.3206723022460936\n",
      "Epoch 461 : train_loss 2.3222808410644533\n",
      "Epoch 461 : test_loss 2.3690838623046875\n",
      "Epoch 462 : train_loss 2.3300896728515625\n",
      "Epoch 462 : test_loss 2.3324804077148436\n",
      "Epoch 463 : train_loss 2.3313082275390626\n",
      "Epoch 463 : test_loss 2.3573087158203125\n",
      "Epoch 464 : train_loss 2.3368181640625\n",
      "Epoch 464 : test_loss 2.3614896240234375\n",
      "Epoch 465 : train_loss 2.31835673828125\n",
      "Epoch 465 : test_loss 2.34894921875\n",
      "Epoch 466 : train_loss 2.3198835693359374\n",
      "Epoch 466 : test_loss 2.34171875\n",
      "Epoch 467 : train_loss 2.3314683349609373\n",
      "Epoch 467 : test_loss 2.3397342529296874\n",
      "Epoch 468 : train_loss 2.3276779541015626\n",
      "Epoch 468 : test_loss 2.366558837890625\n",
      "Epoch 469 : train_loss 2.3256741943359374\n",
      "Epoch 469 : test_loss 2.3762022705078123\n",
      "Epoch 470 : train_loss 2.3369078979492186\n",
      "Epoch 470 : test_loss 2.3544667358398437\n",
      "Epoch 471 : train_loss 2.3321772399902345\n",
      "Epoch 471 : test_loss 2.353722900390625\n",
      "Epoch 472 : train_loss 2.334646142578125\n",
      "Epoch 472 : test_loss 2.3429810791015626\n",
      "Epoch 473 : train_loss 2.320799084472656\n",
      "Epoch 473 : test_loss 2.3452567138671876\n",
      "Epoch 474 : train_loss 2.326925732421875\n",
      "Epoch 474 : test_loss 2.37975341796875\n",
      "Epoch 475 : train_loss 2.332284753417969\n",
      "Epoch 475 : test_loss 2.3526025390625\n",
      "Epoch 476 : train_loss 2.3351541137695313\n",
      "Epoch 476 : test_loss 2.352289123535156\n",
      "Epoch 477 : train_loss 2.319717810058594\n",
      "Epoch 477 : test_loss 2.3442733154296875\n",
      "Epoch 478 : train_loss 2.325551306152344\n",
      "Epoch 478 : test_loss 2.3681124267578126\n",
      "Epoch 479 : train_loss 2.3237381103515626\n",
      "Epoch 479 : test_loss 2.35575732421875\n",
      "Epoch 480 : train_loss 2.32796396484375\n",
      "Epoch 480 : test_loss 2.331298767089844\n",
      "Epoch 481 : train_loss 2.3348160278320313\n",
      "Epoch 481 : test_loss 2.3635228271484374\n",
      "saved model\n",
      "Epoch 482 : train_loss 2.3215640014648438\n",
      "Epoch 482 : test_loss 2.30957080078125\n",
      "Epoch 483 : train_loss 2.3311081298828125\n",
      "Epoch 483 : test_loss 2.36468359375\n",
      "Epoch 484 : train_loss 2.331264044189453\n",
      "Epoch 484 : test_loss 2.365971130371094\n",
      "Epoch 485 : train_loss 2.321976690673828\n",
      "Epoch 485 : test_loss 2.349037353515625\n",
      "Epoch 486 : train_loss 2.319565350341797\n",
      "Epoch 486 : test_loss 2.3553980712890623\n",
      "Epoch 487 : train_loss 2.331617431640625\n",
      "Epoch 487 : test_loss 2.3574334716796876\n",
      "Epoch 488 : train_loss 2.317320245361328\n",
      "Epoch 488 : test_loss 2.342696472167969\n",
      "Epoch 489 : train_loss 2.320736163330078\n",
      "Epoch 489 : test_loss 2.3613193359375\n",
      "Epoch 490 : train_loss 2.3356429077148437\n",
      "Epoch 490 : test_loss 2.345379150390625\n",
      "Epoch 491 : train_loss 2.3346677490234375\n",
      "Epoch 491 : test_loss 2.327620361328125\n",
      "Epoch 492 : train_loss 2.3208643615722657\n",
      "Epoch 492 : test_loss 2.336845703125\n",
      "Epoch 493 : train_loss 2.3347109375\n",
      "Epoch 493 : test_loss 2.3436109619140626\n",
      "Epoch 494 : train_loss 2.3287313598632813\n",
      "Epoch 494 : test_loss 2.359304931640625\n",
      "Epoch 495 : train_loss 2.327786145019531\n",
      "Epoch 495 : test_loss 2.361327819824219\n",
      "Epoch 496 : train_loss 2.3242857360839846\n",
      "Epoch 496 : test_loss 2.3360296630859376\n",
      "Epoch 497 : train_loss 2.330401336669922\n",
      "Epoch 497 : test_loss 2.362513977050781\n",
      "Epoch 498 : train_loss 2.335682196044922\n",
      "Epoch 498 : test_loss 2.3473410034179687\n",
      "Epoch 499 : train_loss 2.322070788574219\n",
      "Epoch 499 : test_loss 2.3265631103515627\n",
      "Epoch 500 : train_loss 2.330915704345703\n",
      "Epoch 500 : test_loss 2.3411185302734374\n",
      "Epoch 501 : train_loss 2.3292405883789065\n",
      "Epoch 501 : test_loss 2.3732506713867187\n",
      "Epoch 502 : train_loss 2.3322037841796877\n",
      "Epoch 502 : test_loss 2.346262939453125\n",
      "Epoch 503 : train_loss 2.3390316040039063\n",
      "Epoch 503 : test_loss 2.3292135620117187\n",
      "Epoch 504 : train_loss 2.3287948120117186\n",
      "Epoch 504 : test_loss 2.3691446533203124\n",
      "Epoch 505 : train_loss 2.3279029541015626\n",
      "Epoch 505 : test_loss 2.360460998535156\n",
      "Epoch 506 : train_loss 2.3280053100585936\n",
      "Epoch 506 : test_loss 2.3931105346679686\n",
      "Epoch 507 : train_loss 2.3281274291992187\n",
      "Epoch 507 : test_loss 2.350511474609375\n",
      "Epoch 508 : train_loss 2.3256267028808595\n",
      "Epoch 508 : test_loss 2.354467041015625\n",
      "Epoch 509 : train_loss 2.3288899658203124\n",
      "Epoch 509 : test_loss 2.35048974609375\n",
      "Epoch 510 : train_loss 2.318900323486328\n",
      "Epoch 510 : test_loss 2.3461598510742188\n",
      "Epoch 511 : train_loss 2.3193961059570314\n",
      "Epoch 511 : test_loss 2.342774658203125\n",
      "Epoch 512 : train_loss 2.3200080810546875\n",
      "Epoch 512 : test_loss 2.339170715332031\n",
      "Epoch 513 : train_loss 2.3251109924316404\n",
      "Epoch 513 : test_loss 2.3677271118164063\n",
      "Epoch 514 : train_loss 2.319298406982422\n",
      "Epoch 514 : test_loss 2.3586494140625\n",
      "Epoch 515 : train_loss 2.332641662597656\n",
      "Epoch 515 : test_loss 2.356014404296875\n",
      "Epoch 516 : train_loss 2.3352933044433595\n",
      "Epoch 516 : test_loss 2.33598291015625\n",
      "Epoch 517 : train_loss 2.3366157287597655\n",
      "Epoch 517 : test_loss 2.3409222412109374\n",
      "Epoch 518 : train_loss 2.326939453125\n",
      "Epoch 518 : test_loss 2.3485697631835936\n",
      "Epoch 519 : train_loss 2.324365850830078\n",
      "Epoch 519 : test_loss 2.312678466796875\n",
      "Epoch 520 : train_loss 2.338001287841797\n",
      "Epoch 520 : test_loss 2.3261728515625\n",
      "Epoch 521 : train_loss 2.325150732421875\n",
      "Epoch 521 : test_loss 2.363767822265625\n",
      "Epoch 522 : train_loss 2.3278036682128906\n",
      "Epoch 522 : test_loss 2.38124267578125\n",
      "Epoch 523 : train_loss 2.3340943725585936\n",
      "Epoch 523 : test_loss 2.3608007202148436\n",
      "Epoch 524 : train_loss 2.3245513916015623\n",
      "Epoch 524 : test_loss 2.36901416015625\n",
      "Epoch 525 : train_loss 2.3249648010253905\n",
      "Epoch 525 : test_loss 2.3220933227539065\n",
      "Epoch 526 : train_loss 2.334316687011719\n",
      "Epoch 526 : test_loss 2.3521683349609375\n",
      "Epoch 527 : train_loss 2.3204635681152346\n",
      "Epoch 527 : test_loss 2.348326416015625\n",
      "Epoch 528 : train_loss 2.330159637451172\n",
      "Epoch 528 : test_loss 2.338033447265625\n",
      "Epoch 529 : train_loss 2.3221587707519533\n",
      "Epoch 529 : test_loss 2.355116943359375\n",
      "Epoch 530 : train_loss 2.3244455200195313\n",
      "Epoch 530 : test_loss 2.3307108764648437\n",
      "Epoch 531 : train_loss 2.325754815673828\n",
      "Epoch 531 : test_loss 2.3117100830078123\n",
      "Epoch 532 : train_loss 2.3314598693847657\n",
      "Epoch 532 : test_loss 2.35732763671875\n",
      "Epoch 533 : train_loss 2.3168296630859375\n",
      "Epoch 533 : test_loss 2.365822509765625\n",
      "Epoch 534 : train_loss 2.3239952087402345\n",
      "Epoch 534 : test_loss 2.3464296875\n",
      "Epoch 535 : train_loss 2.328108239746094\n",
      "Epoch 535 : test_loss 2.3623590087890625\n",
      "Epoch 536 : train_loss 2.3257796447753907\n",
      "Epoch 536 : test_loss 2.3373128662109375\n",
      "Epoch 537 : train_loss 2.3238252807617186\n",
      "Epoch 537 : test_loss 2.3284793701171873\n",
      "Epoch 538 : train_loss 2.32956884765625\n",
      "Epoch 538 : test_loss 2.3398028564453126\n",
      "Epoch 539 : train_loss 2.3296691284179687\n",
      "Epoch 539 : test_loss 2.3239932250976563\n",
      "Epoch 540 : train_loss 2.3261365539550782\n",
      "Epoch 540 : test_loss 2.3683837890625\n",
      "Epoch 541 : train_loss 2.318666705322266\n",
      "Epoch 541 : test_loss 2.34694384765625\n",
      "Epoch 542 : train_loss 2.319971453857422\n",
      "Epoch 542 : test_loss 2.3656713256835937\n",
      "Epoch 543 : train_loss 2.3190888671875\n",
      "Epoch 543 : test_loss 2.3477135009765626\n",
      "Epoch 544 : train_loss 2.3300579711914065\n",
      "Epoch 544 : test_loss 2.347956970214844\n",
      "Epoch 545 : train_loss 2.3152648193359373\n",
      "Epoch 545 : test_loss 2.3760244140625\n",
      "Epoch 546 : train_loss 2.3363430908203124\n",
      "Epoch 546 : test_loss 2.3931481323242187\n",
      "Epoch 547 : train_loss 2.3295773071289063\n",
      "Epoch 547 : test_loss 2.3294478759765624\n",
      "Epoch 548 : train_loss 2.326725262451172\n",
      "Epoch 548 : test_loss 2.37451318359375\n",
      "Epoch 549 : train_loss 2.329918463134766\n",
      "Epoch 549 : test_loss 2.3806077880859373\n",
      "Epoch 550 : train_loss 2.3247505004882814\n",
      "Epoch 550 : test_loss 2.3315098266601564\n",
      "Epoch 551 : train_loss 2.3235932739257814\n",
      "Epoch 551 : test_loss 2.4018280639648437\n",
      "Epoch 552 : train_loss 2.3188717041015625\n",
      "Epoch 552 : test_loss 2.346343994140625\n",
      "Epoch 553 : train_loss 2.325015771484375\n",
      "Epoch 553 : test_loss 2.3547925415039064\n",
      "Epoch 554 : train_loss 2.321022705078125\n",
      "Epoch 554 : test_loss 2.3643460693359377\n",
      "Epoch 555 : train_loss 2.330830627441406\n",
      "Epoch 555 : test_loss 2.3150960693359375\n",
      "Epoch 556 : train_loss 2.334477667236328\n",
      "Epoch 556 : test_loss 2.339013122558594\n",
      "Epoch 557 : train_loss 2.325886535644531\n",
      "Epoch 557 : test_loss 2.340796875\n",
      "Epoch 558 : train_loss 2.3301703857421874\n",
      "Epoch 558 : test_loss 2.3535195922851564\n",
      "Epoch 559 : train_loss 2.324521813964844\n",
      "Epoch 559 : test_loss 2.36889111328125\n",
      "Epoch 560 : train_loss 2.32077998046875\n",
      "Epoch 560 : test_loss 2.3741884765625\n",
      "Epoch 561 : train_loss 2.322153747558594\n",
      "Epoch 561 : test_loss 2.34803759765625\n",
      "Epoch 562 : train_loss 2.320669140625\n",
      "Epoch 562 : test_loss 2.3792175903320314\n",
      "Epoch 563 : train_loss 2.322377087402344\n",
      "Epoch 563 : test_loss 2.3590909423828124\n",
      "Epoch 564 : train_loss 2.3292613220214844\n",
      "Epoch 564 : test_loss 2.342239807128906\n",
      "Epoch 565 : train_loss 2.3112531982421873\n",
      "Epoch 565 : test_loss 2.3295569458007814\n",
      "Epoch 566 : train_loss 2.322667413330078\n",
      "Epoch 566 : test_loss 2.3438082275390624\n",
      "Epoch 567 : train_loss 2.3484466674804687\n",
      "Epoch 567 : test_loss 2.3596312255859373\n",
      "Epoch 568 : train_loss 2.321978729248047\n",
      "Epoch 568 : test_loss 2.3591588134765624\n",
      "Epoch 569 : train_loss 2.3295852661132814\n",
      "Epoch 569 : test_loss 2.3586124267578126\n",
      "Epoch 570 : train_loss 2.3333204650878905\n",
      "Epoch 570 : test_loss 2.3462404174804687\n",
      "Epoch 571 : train_loss 2.323635089111328\n",
      "Epoch 571 : test_loss 2.3343936767578124\n",
      "Epoch 572 : train_loss 2.329408740234375\n",
      "Epoch 572 : test_loss 2.3785692138671877\n",
      "Epoch 573 : train_loss 2.3327421813964846\n",
      "Epoch 573 : test_loss 2.362613037109375\n",
      "Epoch 574 : train_loss 2.328881396484375\n",
      "Epoch 574 : test_loss 2.3426920166015623\n",
      "Epoch 575 : train_loss 2.324468780517578\n",
      "Epoch 575 : test_loss 2.356182861328125\n",
      "Epoch 576 : train_loss 2.332283758544922\n",
      "Epoch 576 : test_loss 2.3438931884765624\n",
      "Epoch 577 : train_loss 2.321782702636719\n",
      "Epoch 577 : test_loss 2.317903015136719\n",
      "Epoch 578 : train_loss 2.3281696838378907\n",
      "Epoch 578 : test_loss 2.3182618408203126\n",
      "Epoch 579 : train_loss 2.3309171325683593\n",
      "Epoch 579 : test_loss 2.3685345458984375\n",
      "Epoch 580 : train_loss 2.3231982116699217\n",
      "Epoch 580 : test_loss 2.335274169921875\n",
      "Epoch 581 : train_loss 2.324599987792969\n",
      "Epoch 581 : test_loss 2.3434239501953127\n",
      "Epoch 582 : train_loss 2.3235968505859375\n",
      "Epoch 582 : test_loss 2.3399420776367186\n",
      "Epoch 583 : train_loss 2.325978369140625\n",
      "Epoch 583 : test_loss 2.3470228271484377\n",
      "Epoch 584 : train_loss 2.3365862182617185\n",
      "Epoch 584 : test_loss 2.3701546630859376\n",
      "Epoch 585 : train_loss 2.33125830078125\n",
      "Epoch 585 : test_loss 2.369093994140625\n",
      "Epoch 586 : train_loss 2.3209865783691406\n",
      "Epoch 586 : test_loss 2.331805908203125\n",
      "Epoch 587 : train_loss 2.324899737548828\n",
      "Epoch 587 : test_loss 2.36676953125\n",
      "Epoch 588 : train_loss 2.333175762939453\n",
      "Epoch 588 : test_loss 2.3264749755859375\n",
      "Epoch 589 : train_loss 2.334674133300781\n",
      "Epoch 589 : test_loss 2.3548875732421877\n",
      "Epoch 590 : train_loss 2.325515545654297\n",
      "Epoch 590 : test_loss 2.340975341796875\n",
      "Epoch 591 : train_loss 2.3413605529785158\n",
      "Epoch 591 : test_loss 2.348946044921875\n",
      "Epoch 592 : train_loss 2.3223900573730467\n",
      "Epoch 592 : test_loss 2.378439453125\n",
      "Epoch 593 : train_loss 2.325230993652344\n",
      "Epoch 593 : test_loss 2.33207861328125\n",
      "Epoch 594 : train_loss 2.3272594970703127\n",
      "Epoch 594 : test_loss 2.372593811035156\n",
      "Epoch 595 : train_loss 2.324253076171875\n",
      "Epoch 595 : test_loss 2.3489889526367187\n",
      "Epoch 596 : train_loss 2.3293650634765624\n",
      "Epoch 596 : test_loss 2.3332313232421873\n",
      "Epoch 597 : train_loss 2.3204173767089844\n",
      "Epoch 597 : test_loss 2.374081298828125\n",
      "Epoch 598 : train_loss 2.3286165100097658\n",
      "Epoch 598 : test_loss 2.36539453125\n",
      "Epoch 599 : train_loss 2.3233072326660156\n",
      "Epoch 599 : test_loss 2.3485107421875\n",
      "Epoch 600 : train_loss 2.3270281311035155\n",
      "Epoch 600 : test_loss 2.3697476806640627\n",
      "Epoch 601 : train_loss 2.322456262207031\n",
      "Epoch 601 : test_loss 2.3745921630859375\n",
      "Epoch 602 : train_loss 2.3200310668945314\n",
      "Epoch 602 : test_loss 2.339230407714844\n",
      "Epoch 603 : train_loss 2.3235640991210937\n",
      "Epoch 603 : test_loss 2.3492584228515625\n",
      "Epoch 604 : train_loss 2.323426062011719\n",
      "Epoch 604 : test_loss 2.32857177734375\n",
      "Epoch 605 : train_loss 2.324858154296875\n",
      "Epoch 605 : test_loss 2.367869384765625\n",
      "Epoch 606 : train_loss 2.329965875244141\n",
      "Epoch 606 : test_loss 2.377023010253906\n",
      "Epoch 607 : train_loss 2.3293019653320313\n",
      "Epoch 607 : test_loss 2.3776436767578124\n",
      "Epoch 608 : train_loss 2.3332750244140623\n",
      "Epoch 608 : test_loss 2.3747425537109375\n",
      "Epoch 609 : train_loss 2.332821649169922\n",
      "Epoch 609 : test_loss 2.346635498046875\n",
      "Epoch 610 : train_loss 2.325059558105469\n",
      "Epoch 610 : test_loss 2.3582264404296875\n",
      "Epoch 611 : train_loss 2.324487451171875\n",
      "Epoch 611 : test_loss 2.3662662963867187\n",
      "Epoch 612 : train_loss 2.328364208984375\n",
      "Epoch 612 : test_loss 2.3500721435546876\n",
      "Epoch 613 : train_loss 2.3244550842285157\n",
      "Epoch 613 : test_loss 2.3713993530273436\n",
      "Epoch 614 : train_loss 2.3261344482421875\n",
      "Epoch 614 : test_loss 2.36598291015625\n",
      "Epoch 615 : train_loss 2.3211900512695314\n",
      "Epoch 615 : test_loss 2.3551131591796874\n",
      "Epoch 616 : train_loss 2.337872625732422\n",
      "Epoch 616 : test_loss 2.3318641967773437\n",
      "Epoch 617 : train_loss 2.312940478515625\n",
      "Epoch 617 : test_loss 2.331883605957031\n",
      "Epoch 618 : train_loss 2.325077087402344\n",
      "Epoch 618 : test_loss 2.377243957519531\n",
      "Epoch 619 : train_loss 2.3317884521484373\n",
      "Epoch 619 : test_loss 2.357794189453125\n",
      "Epoch 620 : train_loss 2.3198528137207033\n",
      "Epoch 620 : test_loss 2.353273193359375\n",
      "Epoch 621 : train_loss 2.3255742431640627\n",
      "Epoch 621 : test_loss 2.357982666015625\n",
      "Epoch 622 : train_loss 2.3178649719238282\n",
      "Epoch 622 : test_loss 2.3604273071289064\n",
      "Epoch 623 : train_loss 2.3209959533691404\n",
      "Epoch 623 : test_loss 2.38805712890625\n",
      "Epoch 624 : train_loss 2.3230451721191407\n",
      "Epoch 624 : test_loss 2.3638383178710938\n",
      "Epoch 625 : train_loss 2.3287671630859377\n",
      "Epoch 625 : test_loss 2.355589111328125\n",
      "Epoch 626 : train_loss 2.3194320556640626\n",
      "Epoch 626 : test_loss 2.368004333496094\n",
      "Epoch 627 : train_loss 2.323511895751953\n",
      "Epoch 627 : test_loss 2.3420274658203124\n",
      "Epoch 628 : train_loss 2.3264580078125\n",
      "Epoch 628 : test_loss 2.356427978515625\n",
      "Epoch 629 : train_loss 2.3242801330566407\n",
      "Epoch 629 : test_loss 2.390384704589844\n",
      "Epoch 630 : train_loss 2.3259545532226564\n",
      "Epoch 630 : test_loss 2.332052429199219\n",
      "Epoch 631 : train_loss 2.3234252990722655\n",
      "Epoch 631 : test_loss 2.3484112548828127\n",
      "Epoch 632 : train_loss 2.324118273925781\n",
      "Epoch 632 : test_loss 2.3616157836914065\n",
      "Epoch 633 : train_loss 2.3233246826171876\n",
      "Epoch 633 : test_loss 2.3332559814453124\n",
      "Epoch 634 : train_loss 2.3172468872070313\n",
      "Epoch 634 : test_loss 2.31748828125\n",
      "Epoch 635 : train_loss 2.324192578125\n",
      "Epoch 635 : test_loss 2.335383544921875\n",
      "Epoch 636 : train_loss 2.3191248962402344\n",
      "Epoch 636 : test_loss 2.3906854248046874\n",
      "Epoch 637 : train_loss 2.3333929565429687\n",
      "Epoch 637 : test_loss 2.3390372924804685\n",
      "Epoch 638 : train_loss 2.319221856689453\n",
      "Epoch 638 : test_loss 2.3842794189453125\n",
      "Epoch 639 : train_loss 2.315877099609375\n",
      "Epoch 639 : test_loss 2.3411446533203124\n",
      "Epoch 640 : train_loss 2.3234438049316406\n",
      "Epoch 640 : test_loss 2.368937255859375\n",
      "Epoch 641 : train_loss 2.320490966796875\n",
      "Epoch 641 : test_loss 2.3647716064453124\n",
      "Epoch 642 : train_loss 2.3153427612304687\n",
      "Epoch 642 : test_loss 2.3482381591796875\n",
      "Epoch 643 : train_loss 2.331610070800781\n",
      "Epoch 643 : test_loss 2.373005065917969\n",
      "Epoch 644 : train_loss 2.323207141113281\n",
      "Epoch 644 : test_loss 2.312175537109375\n",
      "Epoch 645 : train_loss 2.3229607971191406\n",
      "Epoch 645 : test_loss 2.340173828125\n",
      "Epoch 646 : train_loss 2.3222766845703124\n",
      "Epoch 646 : test_loss 2.375752685546875\n",
      "Epoch 647 : train_loss 2.317102551269531\n",
      "Epoch 647 : test_loss 2.314083740234375\n",
      "Epoch 648 : train_loss 2.3164809448242187\n",
      "Epoch 648 : test_loss 2.325942626953125\n",
      "Epoch 649 : train_loss 2.3239234252929686\n",
      "Epoch 649 : test_loss 2.376174072265625\n",
      "Epoch 650 : train_loss 2.32135146484375\n",
      "Epoch 650 : test_loss 2.335226257324219\n",
      "Epoch 651 : train_loss 2.327766064453125\n",
      "Epoch 651 : test_loss 2.3226646728515625\n",
      "Epoch 652 : train_loss 2.326788690185547\n",
      "Epoch 652 : test_loss 2.3381456298828125\n",
      "Epoch 653 : train_loss 2.332554925537109\n",
      "Epoch 653 : test_loss 2.380688903808594\n",
      "Epoch 654 : train_loss 2.331725634765625\n",
      "Epoch 654 : test_loss 2.3542517700195313\n",
      "Epoch 655 : train_loss 2.3217308837890624\n",
      "Epoch 655 : test_loss 2.370226806640625\n",
      "Epoch 656 : train_loss 2.322645135498047\n",
      "Epoch 656 : test_loss 2.3486669921875\n",
      "Epoch 657 : train_loss 2.322804553222656\n",
      "Epoch 657 : test_loss 2.3413870849609375\n",
      "Epoch 658 : train_loss 2.3247342651367187\n",
      "Epoch 658 : test_loss 2.357398498535156\n",
      "Epoch 659 : train_loss 2.326878631591797\n",
      "Epoch 659 : test_loss 2.3511434326171874\n",
      "Epoch 660 : train_loss 2.3245717651367186\n",
      "Epoch 660 : test_loss 2.346527099609375\n",
      "Epoch 661 : train_loss 2.3258685791015625\n",
      "Epoch 661 : test_loss 2.351722412109375\n",
      "Epoch 662 : train_loss 2.32779169921875\n",
      "Epoch 662 : test_loss 2.3357266845703126\n",
      "Epoch 663 : train_loss 2.326307391357422\n",
      "Epoch 663 : test_loss 2.3520611572265624\n",
      "Epoch 664 : train_loss 2.3238077575683596\n",
      "Epoch 664 : test_loss 2.329961669921875\n",
      "Epoch 665 : train_loss 2.3201520629882815\n",
      "Epoch 665 : test_loss 2.367429504394531\n",
      "Epoch 666 : train_loss 2.316009375\n",
      "Epoch 666 : test_loss 2.35259521484375\n",
      "Epoch 667 : train_loss 2.325686315917969\n",
      "Epoch 667 : test_loss 2.3652157592773437\n",
      "Epoch 668 : train_loss 2.3168899475097655\n",
      "Epoch 668 : test_loss 2.342670166015625\n",
      "Epoch 669 : train_loss 2.3216134094238283\n",
      "Epoch 669 : test_loss 2.3355194702148436\n",
      "Epoch 670 : train_loss 2.322723272705078\n",
      "Epoch 670 : test_loss 2.332956115722656\n",
      "Epoch 671 : train_loss 2.307685919189453\n",
      "Epoch 671 : test_loss 2.3402451171875\n",
      "Epoch 672 : train_loss 2.3059190185546874\n",
      "Epoch 672 : test_loss 2.334010498046875\n",
      "Epoch 673 : train_loss 2.3159929260253906\n",
      "Epoch 673 : test_loss 2.3415133056640625\n",
      "Epoch 674 : train_loss 2.3222034362792967\n",
      "Epoch 674 : test_loss 2.3483255615234375\n",
      "Epoch 675 : train_loss 2.319206768798828\n",
      "Epoch 675 : test_loss 2.3668134765625\n",
      "Epoch 676 : train_loss 2.3267332885742187\n",
      "Epoch 676 : test_loss 2.3598557739257813\n",
      "Epoch 677 : train_loss 2.325084289550781\n",
      "Epoch 677 : test_loss 2.3439407958984373\n",
      "Epoch 678 : train_loss 2.3199632202148437\n",
      "Epoch 678 : test_loss 2.3737933959960937\n",
      "Epoch 679 : train_loss 2.3119036499023435\n",
      "Epoch 679 : test_loss 2.353510498046875\n",
      "Epoch 680 : train_loss 2.3254834533691406\n",
      "Epoch 680 : test_loss 2.350683776855469\n",
      "Epoch 681 : train_loss 2.3238554931640625\n",
      "Epoch 681 : test_loss 2.3422987670898436\n",
      "Epoch 682 : train_loss 2.3179290405273436\n",
      "Epoch 682 : test_loss 2.3475689697265625\n",
      "Epoch 683 : train_loss 2.3151921630859373\n",
      "Epoch 683 : test_loss 2.33930224609375\n",
      "Epoch 684 : train_loss 2.309806042480469\n",
      "Epoch 684 : test_loss 2.3300133056640626\n",
      "Epoch 685 : train_loss 2.3180141357421875\n",
      "Epoch 685 : test_loss 2.3445858154296877\n",
      "Epoch 686 : train_loss 2.3213032836914063\n",
      "Epoch 686 : test_loss 2.3694442138671876\n",
      "Epoch 687 : train_loss 2.3302672119140624\n",
      "Epoch 687 : test_loss 2.358473449707031\n",
      "Epoch 688 : train_loss 2.320046533203125\n",
      "Epoch 688 : test_loss 2.3656240844726564\n",
      "Epoch 689 : train_loss 2.3190031494140624\n",
      "Epoch 689 : test_loss 2.322846435546875\n",
      "Epoch 690 : train_loss 2.31956298828125\n",
      "Epoch 690 : test_loss 2.341807922363281\n",
      "Epoch 691 : train_loss 2.313234265136719\n",
      "Epoch 691 : test_loss 2.3605126953125\n",
      "Epoch 692 : train_loss 2.3202097106933595\n",
      "Epoch 692 : test_loss 2.33499951171875\n",
      "Epoch 693 : train_loss 2.313426397705078\n",
      "Epoch 693 : test_loss 2.362488037109375\n",
      "Epoch 694 : train_loss 2.328311511230469\n",
      "Epoch 694 : test_loss 2.3527744140625\n",
      "Epoch 695 : train_loss 2.330275793457031\n",
      "Epoch 695 : test_loss 2.34169580078125\n",
      "Epoch 696 : train_loss 2.3146421997070314\n",
      "Epoch 696 : test_loss 2.3495675048828124\n",
      "Epoch 697 : train_loss 2.3215612426757812\n",
      "Epoch 697 : test_loss 2.33649609375\n",
      "Epoch 698 : train_loss 2.313399572753906\n",
      "Epoch 698 : test_loss 2.378069396972656\n",
      "Epoch 699 : train_loss 2.316345703125\n",
      "Epoch 699 : test_loss 2.3659224853515624\n",
      "Epoch 700 : train_loss 2.3157021728515623\n",
      "Epoch 700 : test_loss 2.334315673828125\n",
      "Epoch 701 : train_loss 2.3207658325195313\n",
      "Epoch 701 : test_loss 2.3463370361328124\n",
      "Epoch 702 : train_loss 2.3165570556640627\n",
      "Epoch 702 : test_loss 2.3664586181640623\n",
      "Epoch 703 : train_loss 2.323653332519531\n",
      "Epoch 703 : test_loss 2.3606683349609376\n",
      "Epoch 704 : train_loss 2.311224200439453\n",
      "Epoch 704 : test_loss 2.3528424072265626\n",
      "Epoch 705 : train_loss 2.324289758300781\n",
      "Epoch 705 : test_loss 2.354462890625\n",
      "saved model\n",
      "Epoch 706 : train_loss 2.3224987670898436\n",
      "Epoch 706 : test_loss 2.302896240234375\n",
      "Epoch 707 : train_loss 2.3227442993164065\n",
      "Epoch 707 : test_loss 2.3663699951171875\n",
      "Epoch 708 : train_loss 2.3214407958984373\n",
      "Epoch 708 : test_loss 2.3746171875\n",
      "Epoch 709 : train_loss 2.318916778564453\n",
      "Epoch 709 : test_loss 2.3472454833984373\n",
      "Epoch 710 : train_loss 2.3201993896484376\n",
      "Epoch 710 : test_loss 2.3559315185546876\n",
      "Epoch 711 : train_loss 2.3271421630859375\n",
      "Epoch 711 : test_loss 2.330234619140625\n",
      "Epoch 712 : train_loss 2.3142426513671874\n",
      "Epoch 712 : test_loss 2.341077880859375\n",
      "Epoch 713 : train_loss 2.3191464050292967\n",
      "Epoch 713 : test_loss 2.330118408203125\n",
      "Epoch 714 : train_loss 2.319630548095703\n",
      "Epoch 714 : test_loss 2.3222122192382812\n",
      "Epoch 715 : train_loss 2.321333868408203\n",
      "Epoch 715 : test_loss 2.3546353149414063\n",
      "Epoch 716 : train_loss 2.3320953491210936\n",
      "Epoch 716 : test_loss 2.3307105712890626\n",
      "Epoch 717 : train_loss 2.3230653869628908\n",
      "Epoch 717 : test_loss 2.3462880249023437\n",
      "Epoch 718 : train_loss 2.326533312988281\n",
      "Epoch 718 : test_loss 2.3652136840820313\n",
      "Epoch 719 : train_loss 2.311582336425781\n",
      "Epoch 719 : test_loss 2.366796569824219\n",
      "Epoch 720 : train_loss 2.308315771484375\n",
      "Epoch 720 : test_loss 2.3451856689453123\n",
      "Epoch 721 : train_loss 2.3107376342773436\n",
      "Epoch 721 : test_loss 2.3235125732421875\n",
      "Epoch 722 : train_loss 2.319468310546875\n",
      "Epoch 722 : test_loss 2.320956970214844\n",
      "Epoch 723 : train_loss 2.321196514892578\n",
      "Epoch 723 : test_loss 2.325239074707031\n",
      "Epoch 724 : train_loss 2.311010528564453\n",
      "Epoch 724 : test_loss 2.332623718261719\n",
      "Epoch 725 : train_loss 2.3217414794921876\n",
      "Epoch 725 : test_loss 2.3635648193359375\n",
      "Epoch 726 : train_loss 2.3234632690429686\n",
      "Epoch 726 : test_loss 2.350897216796875\n",
      "Epoch 727 : train_loss 2.3186164794921873\n",
      "Epoch 727 : test_loss 2.34473779296875\n",
      "Epoch 728 : train_loss 2.320844140625\n",
      "Epoch 728 : test_loss 2.356232666015625\n",
      "Epoch 729 : train_loss 2.317837384033203\n",
      "Epoch 729 : test_loss 2.365764221191406\n",
      "Epoch 730 : train_loss 2.320122491455078\n",
      "Epoch 730 : test_loss 2.353681396484375\n",
      "Epoch 731 : train_loss 2.3144763427734376\n",
      "Epoch 731 : test_loss 2.330798156738281\n",
      "Epoch 732 : train_loss 2.3189482849121092\n",
      "Epoch 732 : test_loss 2.36082666015625\n",
      "Epoch 733 : train_loss 2.3279948974609375\n",
      "Epoch 733 : test_loss 2.3306842041015625\n",
      "Epoch 734 : train_loss 2.3241156982421876\n",
      "Epoch 734 : test_loss 2.387790710449219\n",
      "Epoch 735 : train_loss 2.313693615722656\n",
      "Epoch 735 : test_loss 2.3321522827148438\n",
      "Epoch 736 : train_loss 2.312045709228516\n",
      "Epoch 736 : test_loss 2.3386852416992188\n",
      "Epoch 737 : train_loss 2.321608544921875\n",
      "Epoch 737 : test_loss 2.3324090576171876\n",
      "Epoch 738 : train_loss 2.3292769470214845\n",
      "Epoch 738 : test_loss 2.357620849609375\n",
      "Epoch 739 : train_loss 2.3265084350585936\n",
      "Epoch 739 : test_loss 2.3433594970703124\n",
      "Epoch 740 : train_loss 2.3201326293945312\n",
      "Epoch 740 : test_loss 2.347947021484375\n",
      "Epoch 741 : train_loss 2.31959375\n",
      "Epoch 741 : test_loss 2.353371826171875\n",
      "Epoch 742 : train_loss 2.320054675292969\n",
      "Epoch 742 : test_loss 2.3331212158203125\n",
      "Epoch 743 : train_loss 2.3172377258300783\n",
      "Epoch 743 : test_loss 2.3494920654296876\n",
      "Epoch 744 : train_loss 2.3249611328125\n",
      "Epoch 744 : test_loss 2.341129821777344\n",
      "Epoch 745 : train_loss 2.327155139160156\n",
      "Epoch 745 : test_loss 2.36745947265625\n",
      "Epoch 746 : train_loss 2.3194579711914063\n",
      "Epoch 746 : test_loss 2.3380986938476562\n",
      "Epoch 747 : train_loss 2.3280780151367186\n",
      "Epoch 747 : test_loss 2.3986412963867187\n",
      "Epoch 748 : train_loss 2.3284189697265627\n",
      "Epoch 748 : test_loss 2.341347595214844\n",
      "Epoch 749 : train_loss 2.3264515380859376\n",
      "Epoch 749 : test_loss 2.34325634765625\n",
      "Epoch 750 : train_loss 2.318754083251953\n",
      "Epoch 750 : test_loss 2.3544107666015623\n",
      "Epoch 751 : train_loss 2.3238529663085936\n",
      "Epoch 751 : test_loss 2.368381103515625\n",
      "Epoch 752 : train_loss 2.3165118774414064\n",
      "Epoch 752 : test_loss 2.3272501220703123\n",
      "Epoch 753 : train_loss 2.3165745971679685\n",
      "Epoch 753 : test_loss 2.352979553222656\n",
      "Epoch 754 : train_loss 2.320008575439453\n",
      "Epoch 754 : test_loss 2.377824951171875\n",
      "Epoch 755 : train_loss 2.3125236389160158\n",
      "Epoch 755 : test_loss 2.3869935302734375\n",
      "Epoch 756 : train_loss 2.3231838012695314\n",
      "Epoch 756 : test_loss 2.3669068603515626\n",
      "Epoch 757 : train_loss 2.3249684936523436\n",
      "Epoch 757 : test_loss 2.3387788696289062\n",
      "Epoch 758 : train_loss 2.313828649902344\n",
      "Epoch 758 : test_loss 2.36204443359375\n",
      "Epoch 759 : train_loss 2.3181933349609376\n",
      "Epoch 759 : test_loss 2.3722427978515626\n",
      "Epoch 760 : train_loss 2.3180278747558596\n",
      "Epoch 760 : test_loss 2.350209289550781\n",
      "Epoch 761 : train_loss 2.3285768005371095\n",
      "Epoch 761 : test_loss 2.3356624145507814\n",
      "Epoch 762 : train_loss 2.3187881591796873\n",
      "Epoch 762 : test_loss 2.3601448974609376\n",
      "Epoch 763 : train_loss 2.324041943359375\n",
      "Epoch 763 : test_loss 2.336412841796875\n",
      "Epoch 764 : train_loss 2.3187788696289062\n",
      "Epoch 764 : test_loss 2.3191029663085936\n",
      "Epoch 765 : train_loss 2.320274267578125\n",
      "Epoch 765 : test_loss 2.320453857421875\n",
      "Epoch 766 : train_loss 2.3133646057128905\n",
      "Epoch 766 : test_loss 2.3681337890625\n",
      "Epoch 767 : train_loss 2.3123079833984375\n",
      "Epoch 767 : test_loss 2.325256103515625\n",
      "Epoch 768 : train_loss 2.321043737792969\n",
      "Epoch 768 : test_loss 2.332873046875\n",
      "Epoch 769 : train_loss 2.3208485290527343\n",
      "Epoch 769 : test_loss 2.3780364990234375\n",
      "Epoch 770 : train_loss 2.312750537109375\n",
      "Epoch 770 : test_loss 2.335752197265625\n",
      "Epoch 771 : train_loss 2.319536749267578\n",
      "Epoch 771 : test_loss 2.3247904052734376\n",
      "Epoch 772 : train_loss 2.3146708251953125\n",
      "Epoch 772 : test_loss 2.332316650390625\n",
      "Epoch 773 : train_loss 2.3219531616210936\n",
      "Epoch 773 : test_loss 2.3583896484375\n",
      "Epoch 774 : train_loss 2.311446484375\n",
      "Epoch 774 : test_loss 2.352439208984375\n",
      "Epoch 775 : train_loss 2.3162209350585936\n",
      "Epoch 775 : test_loss 2.37118896484375\n",
      "Epoch 776 : train_loss 2.3217994384765626\n",
      "Epoch 776 : test_loss 2.3700084228515625\n",
      "Epoch 777 : train_loss 2.319877490234375\n",
      "Epoch 777 : test_loss 2.33311865234375\n",
      "Epoch 778 : train_loss 2.324880358886719\n",
      "Epoch 778 : test_loss 2.336297119140625\n",
      "Epoch 779 : train_loss 2.3203112060546873\n",
      "Epoch 779 : test_loss 2.3405048828125\n",
      "Epoch 780 : train_loss 2.3276239990234373\n",
      "Epoch 780 : test_loss 2.3471931762695313\n",
      "Epoch 781 : train_loss 2.315041162109375\n",
      "Epoch 781 : test_loss 2.3353038940429687\n",
      "Epoch 782 : train_loss 2.3208616943359375\n",
      "Epoch 782 : test_loss 2.330452392578125\n",
      "Epoch 783 : train_loss 2.322216552734375\n",
      "Epoch 783 : test_loss 2.3461038818359374\n",
      "Epoch 784 : train_loss 2.3294479919433595\n",
      "Epoch 784 : test_loss 2.3521130981445313\n",
      "Epoch 785 : train_loss 2.3239434814453124\n",
      "Epoch 785 : test_loss 2.3476490478515624\n",
      "Epoch 786 : train_loss 2.3208705810546877\n",
      "Epoch 786 : test_loss 2.3481802978515627\n",
      "Epoch 787 : train_loss 2.315552966308594\n",
      "Epoch 787 : test_loss 2.334376708984375\n",
      "Epoch 788 : train_loss 2.319542333984375\n",
      "Epoch 788 : test_loss 2.3627582397460936\n",
      "Epoch 789 : train_loss 2.32745908203125\n",
      "Epoch 789 : test_loss 2.356590576171875\n",
      "Epoch 790 : train_loss 2.31515634765625\n",
      "Epoch 790 : test_loss 2.366023376464844\n",
      "Epoch 791 : train_loss 2.326321728515625\n",
      "Epoch 791 : test_loss 2.3571673583984376\n",
      "Epoch 792 : train_loss 2.3161153686523437\n",
      "Epoch 792 : test_loss 2.3529727783203125\n",
      "Epoch 793 : train_loss 2.314561688232422\n",
      "Epoch 793 : test_loss 2.3455814208984376\n",
      "Epoch 794 : train_loss 2.31503544921875\n",
      "Epoch 794 : test_loss 2.366093505859375\n",
      "Epoch 795 : train_loss 2.3117000732421875\n",
      "Epoch 795 : test_loss 2.3465424194335935\n",
      "Epoch 796 : train_loss 2.3311628173828125\n",
      "Epoch 796 : test_loss 2.3758697509765625\n",
      "Epoch 797 : train_loss 2.3135019775390626\n",
      "Epoch 797 : test_loss 2.3623689575195312\n",
      "Epoch 798 : train_loss 2.3137476318359376\n",
      "Epoch 798 : test_loss 2.3263057861328127\n",
      "Epoch 799 : train_loss 2.321219635009766\n",
      "Epoch 799 : test_loss 2.3455267333984375\n",
      "Epoch 800 : train_loss 2.321309259033203\n",
      "Epoch 800 : test_loss 2.365000732421875\n",
      "Epoch 801 : train_loss 2.323059918212891\n",
      "Epoch 801 : test_loss 2.3262198486328125\n",
      "Epoch 802 : train_loss 2.314396179199219\n",
      "Epoch 802 : test_loss 2.353487060546875\n",
      "Epoch 803 : train_loss 2.315358679199219\n",
      "Epoch 803 : test_loss 2.343974365234375\n",
      "Epoch 804 : train_loss 2.3105432006835938\n",
      "Epoch 804 : test_loss 2.388069580078125\n",
      "Epoch 805 : train_loss 2.3161219604492187\n",
      "Epoch 805 : test_loss 2.3618634033203123\n",
      "Epoch 806 : train_loss 2.320856042480469\n",
      "Epoch 806 : test_loss 2.373946533203125\n",
      "Epoch 807 : train_loss 2.311755157470703\n",
      "Epoch 807 : test_loss 2.32396630859375\n",
      "Epoch 808 : train_loss 2.3190946166992186\n",
      "Epoch 808 : test_loss 2.3408203125\n",
      "Epoch 809 : train_loss 2.3157548095703127\n",
      "Epoch 809 : test_loss 2.363322998046875\n",
      "Epoch 810 : train_loss 2.3125470092773437\n",
      "Epoch 810 : test_loss 2.35364990234375\n",
      "Epoch 811 : train_loss 2.3286740844726563\n",
      "Epoch 811 : test_loss 2.3494970703125\n",
      "Epoch 812 : train_loss 2.3193670471191408\n",
      "Epoch 812 : test_loss 2.3155535888671874\n",
      "Epoch 813 : train_loss 2.313851385498047\n",
      "Epoch 813 : test_loss 2.349157409667969\n",
      "Epoch 814 : train_loss 2.309959527587891\n",
      "Epoch 814 : test_loss 2.3496670532226562\n",
      "Epoch 815 : train_loss 2.322004217529297\n",
      "Epoch 815 : test_loss 2.3433369750976563\n",
      "Epoch 816 : train_loss 2.3166557373046874\n",
      "Epoch 816 : test_loss 2.3587197265625\n",
      "Epoch 817 : train_loss 2.3146293579101562\n",
      "Epoch 817 : test_loss 2.324640380859375\n",
      "Epoch 818 : train_loss 2.3043952331542967\n",
      "Epoch 818 : test_loss 2.333802673339844\n",
      "Epoch 819 : train_loss 2.3163489990234374\n",
      "Epoch 819 : test_loss 2.3565759887695314\n",
      "Epoch 820 : train_loss 2.3091610961914064\n",
      "Epoch 820 : test_loss 2.3605640869140627\n",
      "Epoch 821 : train_loss 2.31162412109375\n",
      "Epoch 821 : test_loss 2.356237060546875\n",
      "Epoch 822 : train_loss 2.312405438232422\n",
      "Epoch 822 : test_loss 2.3196175537109376\n",
      "Epoch 823 : train_loss 2.3226829833984377\n",
      "Epoch 823 : test_loss 2.3540736083984375\n",
      "Epoch 824 : train_loss 2.3157718017578124\n",
      "Epoch 824 : test_loss 2.342507568359375\n",
      "Epoch 825 : train_loss 2.3161920837402343\n",
      "Epoch 825 : test_loss 2.3392527465820314\n",
      "Epoch 826 : train_loss 2.3161980590820312\n",
      "Epoch 826 : test_loss 2.34727392578125\n",
      "Epoch 827 : train_loss 2.3099650512695313\n",
      "Epoch 827 : test_loss 2.36669384765625\n",
      "Epoch 828 : train_loss 2.3166427001953127\n",
      "Epoch 828 : test_loss 2.373354248046875\n",
      "Epoch 829 : train_loss 2.316241015625\n",
      "Epoch 829 : test_loss 2.33420458984375\n",
      "Epoch 830 : train_loss 2.318409094238281\n",
      "Epoch 830 : test_loss 2.3546448974609375\n",
      "Epoch 831 : train_loss 2.31715419921875\n",
      "Epoch 831 : test_loss 2.3454283447265625\n",
      "Epoch 832 : train_loss 2.3088641357421875\n",
      "Epoch 832 : test_loss 2.34636572265625\n",
      "Epoch 833 : train_loss 2.33087939453125\n",
      "Epoch 833 : test_loss 2.3375929565429687\n",
      "Epoch 834 : train_loss 2.3159541870117186\n",
      "Epoch 834 : test_loss 2.3462620849609377\n",
      "Epoch 835 : train_loss 2.31829326171875\n",
      "Epoch 835 : test_loss 2.3498199462890623\n",
      "Epoch 836 : train_loss 2.3176867065429687\n",
      "Epoch 836 : test_loss 2.370919677734375\n",
      "Epoch 837 : train_loss 2.3147626586914063\n",
      "Epoch 837 : test_loss 2.35040478515625\n",
      "Epoch 838 : train_loss 2.3204865478515626\n",
      "Epoch 838 : test_loss 2.322970947265625\n",
      "Epoch 839 : train_loss 2.3199368041992185\n",
      "Epoch 839 : test_loss 2.3340903930664063\n",
      "Epoch 840 : train_loss 2.3187215087890625\n",
      "Epoch 840 : test_loss 2.36609619140625\n",
      "Epoch 841 : train_loss 2.315524426269531\n",
      "Epoch 841 : test_loss 2.3078536376953127\n",
      "Epoch 842 : train_loss 2.3202797485351563\n",
      "Epoch 842 : test_loss 2.340757080078125\n",
      "Epoch 843 : train_loss 2.320428063964844\n",
      "Epoch 843 : test_loss 2.339883605957031\n",
      "Epoch 844 : train_loss 2.3199669311523436\n",
      "Epoch 844 : test_loss 2.323758056640625\n",
      "Epoch 845 : train_loss 2.316327941894531\n",
      "Epoch 845 : test_loss 2.3218604736328126\n",
      "Epoch 846 : train_loss 2.3183110046386717\n",
      "Epoch 846 : test_loss 2.36752294921875\n",
      "Epoch 847 : train_loss 2.319037841796875\n",
      "Epoch 847 : test_loss 2.3391578369140626\n",
      "Epoch 848 : train_loss 2.3166644653320314\n",
      "Epoch 848 : test_loss 2.3425433959960937\n",
      "Epoch 849 : train_loss 2.31661728515625\n",
      "Epoch 849 : test_loss 2.3447745361328125\n",
      "Epoch 850 : train_loss 2.323143621826172\n",
      "Epoch 850 : test_loss 2.3357886962890624\n",
      "Epoch 851 : train_loss 2.3177886657714843\n",
      "Epoch 851 : test_loss 2.3294495849609373\n",
      "Epoch 852 : train_loss 2.321988311767578\n",
      "Epoch 852 : test_loss 2.345034423828125\n",
      "Epoch 853 : train_loss 2.3154004150390626\n",
      "Epoch 853 : test_loss 2.3278493041992188\n",
      "Epoch 854 : train_loss 2.309860705566406\n",
      "Epoch 854 : test_loss 2.326814392089844\n",
      "Epoch 855 : train_loss 2.310548522949219\n",
      "Epoch 855 : test_loss 2.342935546875\n",
      "Epoch 856 : train_loss 2.319066632080078\n",
      "Epoch 856 : test_loss 2.35899560546875\n",
      "Epoch 857 : train_loss 2.3066437377929687\n",
      "Epoch 857 : test_loss 2.3367413330078124\n",
      "Epoch 858 : train_loss 2.3183680908203126\n",
      "Epoch 858 : test_loss 2.3340045166015626\n",
      "Epoch 859 : train_loss 2.3163145141601564\n",
      "Epoch 859 : test_loss 2.3374229736328127\n",
      "Epoch 860 : train_loss 2.3194847106933594\n",
      "Epoch 860 : test_loss 2.3423114624023436\n",
      "Epoch 861 : train_loss 2.3153240173339844\n",
      "Epoch 861 : test_loss 2.3479061279296873\n",
      "Epoch 862 : train_loss 2.3180624877929685\n",
      "Epoch 862 : test_loss 2.337994140625\n",
      "Epoch 863 : train_loss 2.3186660339355467\n",
      "Epoch 863 : test_loss 2.3321190185546876\n",
      "Epoch 864 : train_loss 2.312991052246094\n",
      "Epoch 864 : test_loss 2.374808349609375\n",
      "Epoch 865 : train_loss 2.3177887329101563\n",
      "Epoch 865 : test_loss 2.3539613037109377\n",
      "Epoch 866 : train_loss 2.325812170410156\n",
      "Epoch 866 : test_loss 2.3408294677734376\n",
      "Epoch 867 : train_loss 2.3230707763671874\n",
      "Epoch 867 : test_loss 2.3507318115234375\n",
      "Epoch 868 : train_loss 2.3120416015625\n",
      "Epoch 868 : test_loss 2.356326171875\n",
      "Epoch 869 : train_loss 2.3083884643554686\n",
      "Epoch 869 : test_loss 2.3318404541015627\n",
      "Epoch 870 : train_loss 2.3166168701171874\n",
      "Epoch 870 : test_loss 2.3314472045898436\n",
      "Epoch 871 : train_loss 2.3184425048828126\n",
      "Epoch 871 : test_loss 2.3229652099609375\n",
      "Epoch 872 : train_loss 2.3180600769042967\n",
      "Epoch 872 : test_loss 2.324942626953125\n",
      "Epoch 873 : train_loss 2.325553527832031\n",
      "Epoch 873 : test_loss 2.3287972412109377\n",
      "Epoch 874 : train_loss 2.3168745483398436\n",
      "Epoch 874 : test_loss 2.31818505859375\n",
      "Epoch 875 : train_loss 2.3181913146972657\n",
      "Epoch 875 : test_loss 2.341102783203125\n",
      "Epoch 876 : train_loss 2.3289222412109374\n",
      "Epoch 876 : test_loss 2.3366595458984376\n",
      "Epoch 877 : train_loss 2.3175304077148438\n",
      "Epoch 877 : test_loss 2.3580457763671876\n",
      "Epoch 878 : train_loss 2.3063383422851564\n",
      "Epoch 878 : test_loss 2.3459132080078127\n",
      "Epoch 879 : train_loss 2.321642120361328\n",
      "Epoch 879 : test_loss 2.3274546508789062\n",
      "Epoch 880 : train_loss 2.323138818359375\n",
      "Epoch 880 : test_loss 2.3742550659179686\n",
      "Epoch 881 : train_loss 2.320571203613281\n",
      "Epoch 881 : test_loss 2.3436531982421873\n",
      "Epoch 882 : train_loss 2.3115253662109376\n",
      "Epoch 882 : test_loss 2.3548746948242187\n",
      "Epoch 883 : train_loss 2.3125638610839845\n",
      "Epoch 883 : test_loss 2.351618896484375\n",
      "Epoch 884 : train_loss 2.324812872314453\n",
      "Epoch 884 : test_loss 2.3491956787109376\n",
      "Epoch 885 : train_loss 2.3138410522460937\n",
      "Epoch 885 : test_loss 2.3737188720703126\n",
      "Epoch 886 : train_loss 2.3137981384277344\n",
      "Epoch 886 : test_loss 2.337853271484375\n",
      "Epoch 887 : train_loss 2.326884893798828\n",
      "Epoch 887 : test_loss 2.3473927001953125\n",
      "Epoch 888 : train_loss 2.3008389526367186\n",
      "Epoch 888 : test_loss 2.3483255615234375\n",
      "Epoch 889 : train_loss 2.314800109863281\n",
      "Epoch 889 : test_loss 2.377962890625\n",
      "Epoch 890 : train_loss 2.3165923828125\n",
      "Epoch 890 : test_loss 2.35834521484375\n",
      "Epoch 891 : train_loss 2.3198584350585936\n",
      "Epoch 891 : test_loss 2.363839599609375\n",
      "Epoch 892 : train_loss 2.32319580078125\n",
      "Epoch 892 : test_loss 2.3813551025390627\n",
      "Epoch 893 : train_loss 2.315780224609375\n",
      "Epoch 893 : test_loss 2.376362548828125\n",
      "Epoch 894 : train_loss 2.3183714721679687\n",
      "Epoch 894 : test_loss 2.3501761474609375\n",
      "Epoch 895 : train_loss 2.31960380859375\n",
      "Epoch 895 : test_loss 2.3301649169921874\n",
      "Epoch 896 : train_loss 2.317966455078125\n",
      "Epoch 896 : test_loss 2.3100771484375\n",
      "Epoch 897 : train_loss 2.319315264892578\n",
      "Epoch 897 : test_loss 2.3462352905273436\n",
      "Epoch 898 : train_loss 2.3066009521484374\n",
      "Epoch 898 : test_loss 2.3288012084960936\n",
      "Epoch 899 : train_loss 2.3033078735351564\n",
      "Epoch 899 : test_loss 2.3600244140625\n",
      "Epoch 900 : train_loss 2.3233390258789064\n",
      "Epoch 900 : test_loss 2.3252603149414064\n",
      "Epoch 901 : train_loss 2.3241711181640623\n",
      "Epoch 901 : test_loss 2.349643798828125\n",
      "Epoch 902 : train_loss 2.3188113708496094\n",
      "Epoch 902 : test_loss 2.3267271728515624\n",
      "Epoch 903 : train_loss 2.319643811035156\n",
      "Epoch 903 : test_loss 2.3592255859375\n",
      "Epoch 904 : train_loss 2.3117954467773436\n",
      "Epoch 904 : test_loss 2.3739891357421876\n",
      "Epoch 905 : train_loss 2.3184119018554687\n",
      "Epoch 905 : test_loss 2.325313232421875\n",
      "Epoch 906 : train_loss 2.3065529052734375\n",
      "Epoch 906 : test_loss 2.3430487670898437\n",
      "Epoch 907 : train_loss 2.313699963378906\n",
      "Epoch 907 : test_loss 2.3498919677734373\n",
      "Epoch 908 : train_loss 2.315784619140625\n",
      "Epoch 908 : test_loss 2.3670339965820313\n",
      "Epoch 909 : train_loss 2.3200683715820314\n",
      "Epoch 909 : test_loss 2.3368662109375\n",
      "Epoch 910 : train_loss 2.300864367675781\n",
      "Epoch 910 : test_loss 2.3609004516601564\n",
      "Epoch 911 : train_loss 2.3172014892578123\n",
      "Epoch 911 : test_loss 2.340689697265625\n",
      "Epoch 912 : train_loss 2.3129387084960937\n",
      "Epoch 912 : test_loss 2.3270753173828127\n",
      "Epoch 913 : train_loss 2.3156387084960937\n",
      "Epoch 913 : test_loss 2.309554870605469\n",
      "Epoch 914 : train_loss 2.3141507507324217\n",
      "Epoch 914 : test_loss 2.3614830322265625\n",
      "Epoch 915 : train_loss 2.316617297363281\n",
      "Epoch 915 : test_loss 2.3727777709960938\n",
      "Epoch 916 : train_loss 2.312053759765625\n",
      "Epoch 916 : test_loss 2.344179382324219\n",
      "Epoch 917 : train_loss 2.3160956298828124\n",
      "Epoch 917 : test_loss 2.33062890625\n",
      "Epoch 918 : train_loss 2.3152990112304686\n",
      "Epoch 918 : test_loss 2.345845947265625\n",
      "Epoch 919 : train_loss 2.31728916015625\n",
      "Epoch 919 : test_loss 2.338507080078125\n",
      "Epoch 920 : train_loss 2.3165608276367187\n",
      "Epoch 920 : test_loss 2.3412081298828125\n",
      "Epoch 921 : train_loss 2.313529052734375\n",
      "Epoch 921 : test_loss 2.353920654296875\n",
      "Epoch 922 : train_loss 2.3170192749023437\n",
      "Epoch 922 : test_loss 2.37027197265625\n",
      "Epoch 923 : train_loss 2.32888525390625\n",
      "Epoch 923 : test_loss 2.317209228515625\n",
      "Epoch 924 : train_loss 2.318258135986328\n",
      "Epoch 924 : test_loss 2.3316560668945314\n",
      "Epoch 925 : train_loss 2.3089525939941407\n",
      "Epoch 925 : test_loss 2.3435657348632812\n",
      "Epoch 926 : train_loss 2.31612060546875\n",
      "Epoch 926 : test_loss 2.3589495849609374\n",
      "Epoch 927 : train_loss 2.3119845764160156\n",
      "Epoch 927 : test_loss 2.3552598876953126\n",
      "Epoch 928 : train_loss 2.3154912841796875\n",
      "Epoch 928 : test_loss 2.371881103515625\n",
      "Epoch 929 : train_loss 2.3162470703125\n",
      "Epoch 929 : test_loss 2.333720764160156\n",
      "Epoch 930 : train_loss 2.3116714111328127\n",
      "Epoch 930 : test_loss 2.336940185546875\n",
      "Epoch 931 : train_loss 2.3107302673339842\n",
      "Epoch 931 : test_loss 2.36396923828125\n",
      "Epoch 932 : train_loss 2.3181824768066406\n",
      "Epoch 932 : test_loss 2.3428829345703126\n",
      "Epoch 933 : train_loss 2.306195275878906\n",
      "Epoch 933 : test_loss 2.3363331298828127\n",
      "Epoch 934 : train_loss 2.3099641723632813\n",
      "Epoch 934 : test_loss 2.342218017578125\n",
      "Epoch 935 : train_loss 2.318446179199219\n",
      "Epoch 935 : test_loss 2.3333826904296875\n",
      "Epoch 936 : train_loss 2.318029248046875\n",
      "Epoch 936 : test_loss 2.3344852294921874\n",
      "Epoch 937 : train_loss 2.30787529296875\n",
      "Epoch 937 : test_loss 2.308299255371094\n",
      "Epoch 938 : train_loss 2.310282385253906\n",
      "Epoch 938 : test_loss 2.3688990478515626\n",
      "Epoch 939 : train_loss 2.3153704223632814\n",
      "Epoch 939 : test_loss 2.371651916503906\n",
      "Epoch 940 : train_loss 2.310683947753906\n",
      "Epoch 940 : test_loss 2.3441883544921875\n",
      "Epoch 941 : train_loss 2.324414093017578\n",
      "Epoch 941 : test_loss 2.3537623291015626\n",
      "Epoch 942 : train_loss 2.322069665527344\n",
      "Epoch 942 : test_loss 2.34564453125\n",
      "Epoch 943 : train_loss 2.312957421875\n",
      "Epoch 943 : test_loss 2.36997607421875\n",
      "Epoch 944 : train_loss 2.3173927490234374\n",
      "Epoch 944 : test_loss 2.345613037109375\n",
      "Epoch 945 : train_loss 2.3062292358398437\n",
      "Epoch 945 : test_loss 2.3517598876953123\n",
      "Epoch 946 : train_loss 2.309622772216797\n",
      "Epoch 946 : test_loss 2.3486602783203123\n",
      "Epoch 947 : train_loss 2.307128662109375\n",
      "Epoch 947 : test_loss 2.3407908935546873\n",
      "Epoch 948 : train_loss 2.313357958984375\n",
      "Epoch 948 : test_loss 2.343301818847656\n",
      "Epoch 949 : train_loss 2.3224059448242187\n",
      "Epoch 949 : test_loss 2.3474608154296877\n",
      "Epoch 950 : train_loss 2.316236169433594\n",
      "Epoch 950 : test_loss 2.319075439453125\n",
      "Epoch 951 : train_loss 2.30163857421875\n",
      "Epoch 951 : test_loss 2.3458271484375\n",
      "Epoch 952 : train_loss 2.3091929748535156\n",
      "Epoch 952 : test_loss 2.3270362548828123\n",
      "Epoch 953 : train_loss 2.3161898681640625\n",
      "Epoch 953 : test_loss 2.343080322265625\n",
      "Epoch 954 : train_loss 2.318210919189453\n",
      "Epoch 954 : test_loss 2.3598111572265625\n",
      "Epoch 955 : train_loss 2.3157721557617186\n",
      "Epoch 955 : test_loss 2.3321983642578124\n",
      "Epoch 956 : train_loss 2.31286591796875\n",
      "Epoch 956 : test_loss 2.3371083984375\n",
      "Epoch 957 : train_loss 2.3165317993164063\n",
      "Epoch 957 : test_loss 2.353850341796875\n",
      "Epoch 958 : train_loss 2.3055589477539065\n",
      "Epoch 958 : test_loss 2.347616455078125\n",
      "Epoch 959 : train_loss 2.3223846557617187\n",
      "Epoch 959 : test_loss 2.3292402954101563\n",
      "Epoch 960 : train_loss 2.319948132324219\n",
      "Epoch 960 : test_loss 2.3498748168945314\n",
      "Epoch 961 : train_loss 2.3166453918457033\n",
      "Epoch 961 : test_loss 2.3330010986328125\n",
      "Epoch 962 : train_loss 2.304537023925781\n",
      "Epoch 962 : test_loss 2.3358612060546875\n",
      "Epoch 963 : train_loss 2.3018580993652344\n",
      "Epoch 963 : test_loss 2.338688720703125\n",
      "Epoch 964 : train_loss 2.3231090576171876\n",
      "Epoch 964 : test_loss 2.3565458984375\n",
      "Epoch 965 : train_loss 2.313905859375\n",
      "Epoch 965 : test_loss 2.3528807373046874\n",
      "Epoch 966 : train_loss 2.3058144897460937\n",
      "Epoch 966 : test_loss 2.341667663574219\n",
      "Epoch 967 : train_loss 2.3160253662109374\n",
      "Epoch 967 : test_loss 2.3293970947265623\n",
      "Epoch 968 : train_loss 2.3143831787109375\n",
      "Epoch 968 : test_loss 2.330107177734375\n",
      "Epoch 969 : train_loss 2.3122159484863283\n",
      "Epoch 969 : test_loss 2.3325760498046875\n",
      "Epoch 970 : train_loss 2.318935009765625\n",
      "Epoch 970 : test_loss 2.3360574951171875\n",
      "Epoch 971 : train_loss 2.32188671875\n",
      "Epoch 971 : test_loss 2.365557861328125\n",
      "Epoch 972 : train_loss 2.3116290893554687\n",
      "Epoch 972 : test_loss 2.339734924316406\n",
      "Epoch 973 : train_loss 2.3147216064453127\n",
      "Epoch 973 : test_loss 2.3644560546875\n",
      "Epoch 974 : train_loss 2.315938464355469\n",
      "Epoch 974 : test_loss 2.30842919921875\n",
      "Epoch 975 : train_loss 2.317646966552734\n",
      "Epoch 975 : test_loss 2.315027587890625\n",
      "Epoch 976 : train_loss 2.3166728759765625\n",
      "Epoch 976 : test_loss 2.3243333129882813\n",
      "Epoch 977 : train_loss 2.317841979980469\n",
      "Epoch 977 : test_loss 2.374864990234375\n",
      "Epoch 978 : train_loss 2.3088495727539065\n",
      "Epoch 978 : test_loss 2.3244047241210937\n",
      "Epoch 979 : train_loss 2.302464697265625\n",
      "Epoch 979 : test_loss 2.3580877685546877\n",
      "Epoch 980 : train_loss 2.310625378417969\n",
      "Epoch 980 : test_loss 2.32682470703125\n",
      "Epoch 981 : train_loss 2.315081774902344\n",
      "Epoch 981 : test_loss 2.3648533935546876\n",
      "Epoch 982 : train_loss 2.3166240356445313\n",
      "Epoch 982 : test_loss 2.3435433349609376\n",
      "Epoch 983 : train_loss 2.3134704345703123\n",
      "Epoch 983 : test_loss 2.3225361328125\n",
      "Epoch 984 : train_loss 2.3133046020507813\n",
      "Epoch 984 : test_loss 2.348068908691406\n",
      "Epoch 985 : train_loss 2.306183239746094\n",
      "Epoch 985 : test_loss 2.348722900390625\n",
      "Epoch 986 : train_loss 2.3129185180664065\n",
      "Epoch 986 : test_loss 2.3304964599609375\n",
      "Epoch 987 : train_loss 2.3200554443359374\n",
      "Epoch 987 : test_loss 2.348165954589844\n",
      "saved model\n",
      "Epoch 988 : train_loss 2.3147084350585936\n",
      "Epoch 988 : test_loss 2.3016058349609376\n",
      "Epoch 989 : train_loss 2.309798742675781\n",
      "Epoch 989 : test_loss 2.37485205078125\n",
      "Epoch 990 : train_loss 2.318570275878906\n",
      "Epoch 990 : test_loss 2.32685302734375\n",
      "Epoch 991 : train_loss 2.3192990966796874\n",
      "Epoch 991 : test_loss 2.33211328125\n",
      "Epoch 992 : train_loss 2.316765313720703\n",
      "Epoch 992 : test_loss 2.353791748046875\n",
      "Epoch 993 : train_loss 2.2997473510742186\n",
      "Epoch 993 : test_loss 2.3494645385742188\n",
      "Epoch 994 : train_loss 2.30607705078125\n",
      "Epoch 994 : test_loss 2.316249755859375\n",
      "Epoch 995 : train_loss 2.3083445556640627\n",
      "Epoch 995 : test_loss 2.3181243896484376\n",
      "Epoch 996 : train_loss 2.3200194641113283\n",
      "Epoch 996 : test_loss 2.3472866821289062\n",
      "Epoch 997 : train_loss 2.3006461364746094\n",
      "Epoch 997 : test_loss 2.3284773559570313\n",
      "Epoch 998 : train_loss 2.3149315734863283\n",
      "Epoch 998 : test_loss 2.328088195800781\n",
      "Epoch 999 : train_loss 2.3101422119140627\n",
      "Epoch 999 : test_loss 2.3397326049804685\n",
      "Training with 9000\n",
      "saved model\n",
      "Epoch 0 : train_loss 6.009854519314236\n",
      "Epoch 0 : test_loss 5.99307568359375\n",
      "saved model\n",
      "Epoch 1 : train_loss 5.940453016493056\n",
      "Epoch 1 : test_loss 5.90266064453125\n",
      "saved model\n",
      "Epoch 2 : train_loss 5.87342564561632\n",
      "Epoch 2 : test_loss 5.84045166015625\n",
      "saved model\n",
      "Epoch 3 : train_loss 5.800327663845486\n",
      "Epoch 3 : test_loss 5.7701962890625\n",
      "saved model\n",
      "Epoch 4 : train_loss 5.741421196831597\n",
      "Epoch 4 : test_loss 5.693230712890625\n",
      "saved model\n",
      "Epoch 5 : train_loss 5.686394137912326\n",
      "Epoch 5 : test_loss 5.64272802734375\n",
      "saved model\n",
      "Epoch 6 : train_loss 5.6342360161675344\n",
      "Epoch 6 : test_loss 5.58174658203125\n",
      "saved model\n",
      "Epoch 7 : train_loss 5.592243218315972\n",
      "Epoch 7 : test_loss 5.539907470703125\n",
      "saved model\n",
      "Epoch 8 : train_loss 5.557732625325521\n",
      "Epoch 8 : test_loss 5.505019287109375\n",
      "saved model\n",
      "Epoch 9 : train_loss 5.515827623155382\n",
      "Epoch 9 : test_loss 5.47090234375\n",
      "saved model\n",
      "Epoch 10 : train_loss 5.487296020507812\n",
      "Epoch 10 : test_loss 5.43973974609375\n",
      "saved model\n",
      "Epoch 11 : train_loss 5.463171915690104\n",
      "Epoch 11 : test_loss 5.391499755859375\n",
      "saved model\n",
      "Epoch 12 : train_loss 5.431163167317709\n",
      "Epoch 12 : test_loss 5.3905693359375\n",
      "saved model\n",
      "Epoch 13 : train_loss 5.413667412651909\n",
      "Epoch 13 : test_loss 5.348517578125\n",
      "Epoch 14 : train_loss 5.3976504584418405\n",
      "Epoch 14 : test_loss 5.356630859375\n",
      "saved model\n",
      "Epoch 15 : train_loss 5.370918511284723\n",
      "Epoch 15 : test_loss 5.321078857421875\n",
      "saved model\n",
      "Epoch 16 : train_loss 5.352466661241319\n",
      "Epoch 16 : test_loss 5.298748291015625\n",
      "saved model\n",
      "Epoch 17 : train_loss 5.333715616861979\n",
      "Epoch 17 : test_loss 5.28329833984375\n",
      "saved model\n",
      "Epoch 18 : train_loss 5.3090663519965275\n",
      "Epoch 18 : test_loss 5.254775146484375\n",
      "saved model\n",
      "Epoch 19 : train_loss 5.280666395399305\n",
      "Epoch 19 : test_loss 5.217367431640625\n",
      "saved model\n",
      "Epoch 20 : train_loss 5.247317830403646\n",
      "Epoch 20 : test_loss 5.191759765625\n",
      "saved model\n",
      "Epoch 21 : train_loss 5.217829793294271\n",
      "Epoch 21 : test_loss 5.178016357421875\n",
      "saved model\n",
      "Epoch 22 : train_loss 5.195775146484375\n",
      "Epoch 22 : test_loss 5.1471181640625\n",
      "saved model\n",
      "Epoch 23 : train_loss 5.1647113986545135\n",
      "Epoch 23 : test_loss 5.120627807617187\n",
      "saved model\n",
      "Epoch 24 : train_loss 5.145449476453993\n",
      "Epoch 24 : test_loss 5.096921997070313\n",
      "saved model\n",
      "Epoch 25 : train_loss 5.1197186821831595\n",
      "Epoch 25 : test_loss 5.080702270507812\n",
      "saved model\n",
      "Epoch 26 : train_loss 5.1059490559895835\n",
      "Epoch 26 : test_loss 5.07976806640625\n",
      "saved model\n",
      "Epoch 27 : train_loss 5.088853108723958\n",
      "Epoch 27 : test_loss 5.05858984375\n",
      "saved model\n",
      "Epoch 28 : train_loss 5.062451809353298\n",
      "Epoch 28 : test_loss 5.04629296875\n",
      "saved model\n",
      "Epoch 29 : train_loss 5.046539293077257\n",
      "Epoch 29 : test_loss 5.029642578125\n",
      "saved model\n",
      "Epoch 30 : train_loss 5.030174560546875\n",
      "Epoch 30 : test_loss 4.993936157226562\n",
      "saved model\n",
      "Epoch 31 : train_loss 5.019682359483507\n",
      "Epoch 31 : test_loss 4.975996826171875\n",
      "saved model\n",
      "Epoch 32 : train_loss 4.995588324652778\n",
      "Epoch 32 : test_loss 4.959057495117188\n",
      "saved model\n",
      "Epoch 33 : train_loss 4.97510247124566\n",
      "Epoch 33 : test_loss 4.943300170898437\n",
      "saved model\n",
      "Epoch 34 : train_loss 4.950476603190104\n",
      "Epoch 34 : test_loss 4.9130269775390625\n",
      "Epoch 35 : train_loss 4.9373792860243055\n",
      "Epoch 35 : test_loss 4.935181640625\n",
      "saved model\n",
      "Epoch 36 : train_loss 4.91696875\n",
      "Epoch 36 : test_loss 4.9025107421875\n",
      "saved model\n",
      "Epoch 37 : train_loss 4.8984820692274305\n",
      "Epoch 37 : test_loss 4.857564453125\n",
      "saved model\n",
      "Epoch 38 : train_loss 4.880691731770833\n",
      "Epoch 38 : test_loss 4.838168212890625\n",
      "Epoch 39 : train_loss 4.8603173828125\n",
      "Epoch 39 : test_loss 4.84042041015625\n",
      "saved model\n",
      "Epoch 40 : train_loss 4.848937228732639\n",
      "Epoch 40 : test_loss 4.8015556640625\n",
      "saved model\n",
      "Epoch 41 : train_loss 4.8298603515625\n",
      "Epoch 41 : test_loss 4.784281616210937\n",
      "saved model\n",
      "Epoch 42 : train_loss 4.81595647515191\n",
      "Epoch 42 : test_loss 4.780739501953125\n",
      "Epoch 43 : train_loss 4.80463951280382\n",
      "Epoch 43 : test_loss 4.787869506835937\n",
      "saved model\n",
      "Epoch 44 : train_loss 4.780146809895833\n",
      "Epoch 44 : test_loss 4.746552490234375\n",
      "saved model\n",
      "Epoch 45 : train_loss 4.755118801540799\n",
      "Epoch 45 : test_loss 4.712634155273437\n",
      "saved model\n",
      "Epoch 46 : train_loss 4.74339697265625\n",
      "Epoch 46 : test_loss 4.702614990234375\n",
      "saved model\n",
      "Epoch 47 : train_loss 4.7225729709201385\n",
      "Epoch 47 : test_loss 4.682829467773438\n",
      "saved model\n",
      "Epoch 48 : train_loss 4.703973944769965\n",
      "Epoch 48 : test_loss 4.654497680664062\n",
      "saved model\n",
      "Epoch 49 : train_loss 4.68149700249566\n",
      "Epoch 49 : test_loss 4.632080322265625\n",
      "saved model\n",
      "Epoch 50 : train_loss 4.661496175130209\n",
      "Epoch 50 : test_loss 4.62514697265625\n",
      "saved model\n",
      "Epoch 51 : train_loss 4.64291468641493\n",
      "Epoch 51 : test_loss 4.613088989257813\n",
      "saved model\n",
      "Epoch 52 : train_loss 4.627541042751736\n",
      "Epoch 52 : test_loss 4.575409912109375\n",
      "Epoch 53 : train_loss 4.608663425021701\n",
      "Epoch 53 : test_loss 4.593535278320313\n",
      "saved model\n",
      "Epoch 54 : train_loss 4.599518581814236\n",
      "Epoch 54 : test_loss 4.553764038085937\n",
      "saved model\n",
      "Epoch 55 : train_loss 4.580682020399306\n",
      "Epoch 55 : test_loss 4.534773193359375\n",
      "saved model\n",
      "Epoch 56 : train_loss 4.5603878173828125\n",
      "Epoch 56 : test_loss 4.522531127929687\n",
      "saved model\n",
      "Epoch 57 : train_loss 4.541306789822048\n",
      "Epoch 57 : test_loss 4.499157958984375\n",
      "saved model\n",
      "Epoch 58 : train_loss 4.526003173828125\n",
      "Epoch 58 : test_loss 4.480729248046875\n",
      "saved model\n",
      "Epoch 59 : train_loss 4.507005886501736\n",
      "Epoch 59 : test_loss 4.476065795898437\n",
      "saved model\n",
      "Epoch 60 : train_loss 4.4863251953125\n",
      "Epoch 60 : test_loss 4.4583916015625\n",
      "saved model\n",
      "Epoch 61 : train_loss 4.470849107530382\n",
      "Epoch 61 : test_loss 4.44396435546875\n",
      "saved model\n",
      "Epoch 62 : train_loss 4.453393798828125\n",
      "Epoch 62 : test_loss 4.407625244140625\n",
      "saved model\n",
      "Epoch 63 : train_loss 4.432593939887153\n",
      "Epoch 63 : test_loss 4.365729736328125\n",
      "Epoch 64 : train_loss 4.409117770724826\n",
      "Epoch 64 : test_loss 4.369562622070313\n",
      "saved model\n",
      "Epoch 65 : train_loss 4.389633192274306\n",
      "Epoch 65 : test_loss 4.3553583984375\n",
      "saved model\n",
      "Epoch 66 : train_loss 4.373218980577257\n",
      "Epoch 66 : test_loss 4.337857177734375\n",
      "saved model\n",
      "Epoch 67 : train_loss 4.3617039930555555\n",
      "Epoch 67 : test_loss 4.31731884765625\n",
      "saved model\n",
      "Epoch 68 : train_loss 4.343859849717882\n",
      "Epoch 68 : test_loss 4.315413818359375\n",
      "saved model\n",
      "Epoch 69 : train_loss 4.327508734809028\n",
      "Epoch 69 : test_loss 4.298845581054687\n",
      "saved model\n",
      "Epoch 70 : train_loss 4.307756713867187\n",
      "Epoch 70 : test_loss 4.24181396484375\n",
      "saved model\n",
      "Epoch 71 : train_loss 4.292551554361979\n",
      "Epoch 71 : test_loss 4.231154541015625\n",
      "saved model\n",
      "Epoch 72 : train_loss 4.26273786078559\n",
      "Epoch 72 : test_loss 4.230600341796875\n",
      "saved model\n",
      "Epoch 73 : train_loss 4.24554408094618\n",
      "Epoch 73 : test_loss 4.20966162109375\n",
      "saved model\n",
      "Epoch 74 : train_loss 4.228173366970486\n",
      "Epoch 74 : test_loss 4.199556640625\n",
      "saved model\n",
      "Epoch 75 : train_loss 4.2089632161458335\n",
      "Epoch 75 : test_loss 4.178533203125\n",
      "saved model\n",
      "Epoch 76 : train_loss 4.190367757161458\n",
      "Epoch 76 : test_loss 4.154521240234375\n",
      "saved model\n",
      "Epoch 77 : train_loss 4.174635552300347\n",
      "Epoch 77 : test_loss 4.14425830078125\n",
      "saved model\n",
      "Epoch 78 : train_loss 4.1564090033637155\n",
      "Epoch 78 : test_loss 4.131427978515625\n",
      "saved model\n",
      "Epoch 79 : train_loss 4.129172729492187\n",
      "Epoch 79 : test_loss 4.117205322265625\n",
      "saved model\n",
      "Epoch 80 : train_loss 4.126994411892361\n",
      "Epoch 80 : test_loss 4.0929150390625\n",
      "saved model\n",
      "Epoch 81 : train_loss 4.105377631293403\n",
      "Epoch 81 : test_loss 4.0873720703125\n",
      "saved model\n",
      "Epoch 82 : train_loss 4.0838784044053815\n",
      "Epoch 82 : test_loss 4.068058349609375\n",
      "saved model\n",
      "Epoch 83 : train_loss 4.062063015407986\n",
      "Epoch 83 : test_loss 4.044135009765625\n",
      "saved model\n",
      "Epoch 84 : train_loss 4.040926961263021\n",
      "Epoch 84 : test_loss 4.0210673828125\n",
      "saved model\n",
      "Epoch 85 : train_loss 4.023550903320312\n",
      "Epoch 85 : test_loss 4.013111938476563\n",
      "saved model\n",
      "Epoch 86 : train_loss 4.008033704969618\n",
      "Epoch 86 : test_loss 4.0043408203125\n",
      "saved model\n",
      "Epoch 87 : train_loss 3.9969012586805555\n",
      "Epoch 87 : test_loss 3.978878662109375\n",
      "Epoch 88 : train_loss 3.9773936767578126\n",
      "Epoch 88 : test_loss 3.990509765625\n",
      "saved model\n",
      "Epoch 89 : train_loss 3.954734293619792\n",
      "Epoch 89 : test_loss 3.95774755859375\n",
      "saved model\n",
      "Epoch 90 : train_loss 3.9359151746961807\n",
      "Epoch 90 : test_loss 3.920575439453125\n",
      "saved model\n",
      "Epoch 91 : train_loss 3.920997517903646\n",
      "Epoch 91 : test_loss 3.90976123046875\n",
      "saved model\n",
      "Epoch 92 : train_loss 3.912251478407118\n",
      "Epoch 92 : test_loss 3.8911435546875\n",
      "saved model\n",
      "Epoch 93 : train_loss 3.8938633490668404\n",
      "Epoch 93 : test_loss 3.8895186767578127\n",
      "saved model\n",
      "Epoch 94 : train_loss 3.869626953125\n",
      "Epoch 94 : test_loss 3.8719041748046874\n",
      "saved model\n",
      "Epoch 95 : train_loss 3.847775634765625\n",
      "Epoch 95 : test_loss 3.859292724609375\n",
      "saved model\n",
      "Epoch 96 : train_loss 3.835162584092882\n",
      "Epoch 96 : test_loss 3.8461788330078126\n",
      "Epoch 97 : train_loss 3.8175592312282984\n",
      "Epoch 97 : test_loss 3.8552647705078127\n",
      "saved model\n",
      "Epoch 98 : train_loss 3.8088853488498264\n",
      "Epoch 98 : test_loss 3.843043212890625\n",
      "saved model\n",
      "Epoch 99 : train_loss 3.789472683376736\n",
      "Epoch 99 : test_loss 3.7987998046875\n",
      "saved model\n",
      "Epoch 100 : train_loss 3.77153668891059\n",
      "Epoch 100 : test_loss 3.780135498046875\n",
      "saved model\n",
      "Epoch 101 : train_loss 3.7592937689887154\n",
      "Epoch 101 : test_loss 3.7789619140625\n",
      "saved model\n",
      "Epoch 102 : train_loss 3.743414808485243\n",
      "Epoch 102 : test_loss 3.730843017578125\n",
      "saved model\n",
      "Epoch 103 : train_loss 3.7269811469184027\n",
      "Epoch 103 : test_loss 3.72465478515625\n",
      "saved model\n",
      "Epoch 104 : train_loss 3.6930023057725694\n",
      "Epoch 104 : test_loss 3.685392333984375\n",
      "saved model\n",
      "Epoch 105 : train_loss 3.6646617024739583\n",
      "Epoch 105 : test_loss 3.6801561279296875\n",
      "saved model\n",
      "Epoch 106 : train_loss 3.6563243069118925\n",
      "Epoch 106 : test_loss 3.636520263671875\n",
      "Epoch 107 : train_loss 3.657904758029514\n",
      "Epoch 107 : test_loss 3.6825767822265627\n",
      "Epoch 108 : train_loss 3.6455558607313368\n",
      "Epoch 108 : test_loss 3.653370849609375\n",
      "saved model\n",
      "Epoch 109 : train_loss 3.621802992078993\n",
      "Epoch 109 : test_loss 3.620015625\n",
      "saved model\n",
      "Epoch 110 : train_loss 3.5855631306966145\n",
      "Epoch 110 : test_loss 3.60931982421875\n",
      "saved model\n",
      "Epoch 111 : train_loss 3.571952311197917\n",
      "Epoch 111 : test_loss 3.5945078125\n",
      "saved model\n",
      "Epoch 112 : train_loss 3.5837723388671874\n",
      "Epoch 112 : test_loss 3.59012841796875\n",
      "Epoch 113 : train_loss 3.566422797309028\n",
      "Epoch 113 : test_loss 3.59457666015625\n",
      "saved model\n",
      "Epoch 114 : train_loss 3.5430454983181425\n",
      "Epoch 114 : test_loss 3.5612288818359374\n",
      "saved model\n",
      "Epoch 115 : train_loss 3.516788831922743\n",
      "Epoch 115 : test_loss 3.5313143310546873\n",
      "saved model\n",
      "Epoch 116 : train_loss 3.4967247585720487\n",
      "Epoch 116 : test_loss 3.52215966796875\n",
      "saved model\n",
      "Epoch 117 : train_loss 3.479695366753472\n",
      "Epoch 117 : test_loss 3.49012060546875\n",
      "Epoch 118 : train_loss 3.471205891927083\n",
      "Epoch 118 : test_loss 3.4960194091796875\n",
      "Epoch 119 : train_loss 3.475151902940538\n",
      "Epoch 119 : test_loss 3.505657470703125\n",
      "saved model\n",
      "Epoch 120 : train_loss 3.4656794094509547\n",
      "Epoch 120 : test_loss 3.481610595703125\n",
      "saved model\n",
      "Epoch 121 : train_loss 3.4329472384982638\n",
      "Epoch 121 : test_loss 3.3878258056640624\n",
      "saved model\n",
      "Epoch 122 : train_loss 3.3839546508789065\n",
      "Epoch 122 : test_loss 3.3769111328125\n",
      "saved model\n",
      "Epoch 123 : train_loss 3.356821051703559\n",
      "Epoch 123 : test_loss 3.33964404296875\n",
      "Epoch 124 : train_loss 3.3603871459960937\n",
      "Epoch 124 : test_loss 3.37276806640625\n",
      "Epoch 125 : train_loss 3.3558834431966145\n",
      "Epoch 125 : test_loss 3.345101806640625\n",
      "Epoch 126 : train_loss 3.353485337999132\n",
      "Epoch 126 : test_loss 3.352982666015625\n",
      "Epoch 127 : train_loss 3.3386568671332464\n",
      "Epoch 127 : test_loss 3.3464666748046876\n",
      "Epoch 128 : train_loss 3.3376478068033855\n",
      "Epoch 128 : test_loss 3.3770811767578124\n",
      "saved model\n",
      "Epoch 129 : train_loss 3.3238697848849825\n",
      "Epoch 129 : test_loss 3.3282811279296873\n",
      "saved model\n",
      "Epoch 130 : train_loss 3.292156934950087\n",
      "Epoch 130 : test_loss 3.2786746826171873\n",
      "saved model\n",
      "Epoch 131 : train_loss 3.243564120822483\n",
      "Epoch 131 : test_loss 3.225314453125\n",
      "saved model\n",
      "Epoch 132 : train_loss 3.2154964463975695\n",
      "Epoch 132 : test_loss 3.2023076171875\n",
      "saved model\n",
      "Epoch 133 : train_loss 3.195398390028212\n",
      "Epoch 133 : test_loss 3.2017408447265625\n",
      "saved model\n",
      "Epoch 134 : train_loss 3.1701111450195314\n",
      "Epoch 134 : test_loss 3.1896400146484374\n",
      "saved model\n",
      "Epoch 135 : train_loss 3.1658108656141493\n",
      "Epoch 135 : test_loss 3.1877919921875\n",
      "Epoch 136 : train_loss 3.1873258124457466\n",
      "Epoch 136 : test_loss 3.21897314453125\n",
      "Epoch 137 : train_loss 3.2054038831922744\n",
      "Epoch 137 : test_loss 3.2287000732421873\n",
      "Epoch 138 : train_loss 3.1986262953016493\n",
      "Epoch 138 : test_loss 3.21235693359375\n",
      "saved model\n",
      "Epoch 139 : train_loss 3.166694342719184\n",
      "Epoch 139 : test_loss 3.1309942626953124\n",
      "saved model\n",
      "Epoch 140 : train_loss 3.1111751098632814\n",
      "Epoch 140 : test_loss 3.0849210205078124\n",
      "saved model\n",
      "Epoch 141 : train_loss 3.0752565307617186\n",
      "Epoch 141 : test_loss 3.065505615234375\n",
      "saved model\n",
      "Epoch 142 : train_loss 3.0427554796006944\n",
      "Epoch 142 : test_loss 3.030993408203125\n",
      "Epoch 143 : train_loss 3.022418680826823\n",
      "Epoch 143 : test_loss 3.0313079833984373\n",
      "Epoch 144 : train_loss 3.041081332736545\n",
      "Epoch 144 : test_loss 3.0349761962890627\n",
      "Epoch 145 : train_loss 3.0773262396918404\n",
      "Epoch 145 : test_loss 3.0959078369140625\n",
      "Epoch 146 : train_loss 3.0818178236219618\n",
      "Epoch 146 : test_loss 3.0653504638671873\n",
      "saved model\n",
      "Epoch 147 : train_loss 3.04130768500434\n",
      "Epoch 147 : test_loss 3.00465576171875\n",
      "saved model\n",
      "Epoch 148 : train_loss 3.0009643961588544\n",
      "Epoch 148 : test_loss 2.9911689453125\n",
      "saved model\n",
      "Epoch 149 : train_loss 2.9601640082465277\n",
      "Epoch 149 : test_loss 2.940931640625\n",
      "Epoch 150 : train_loss 2.928219489203559\n",
      "Epoch 150 : test_loss 2.94181640625\n",
      "Epoch 151 : train_loss 2.922872545030382\n",
      "Epoch 151 : test_loss 2.959803955078125\n",
      "Epoch 152 : train_loss 2.952744093153212\n",
      "Epoch 152 : test_loss 3.0201339111328127\n",
      "Epoch 153 : train_loss 2.990429144965278\n",
      "Epoch 153 : test_loss 3.083030517578125\n",
      "Epoch 154 : train_loss 2.985743116590712\n",
      "Epoch 154 : test_loss 3.0018448486328126\n",
      "saved model\n",
      "Epoch 155 : train_loss 2.918413302951389\n",
      "Epoch 155 : test_loss 2.911030029296875\n",
      "saved model\n",
      "Epoch 156 : train_loss 2.8491416965060763\n",
      "Epoch 156 : test_loss 2.837093017578125\n",
      "Epoch 157 : train_loss 2.814742858886719\n",
      "Epoch 157 : test_loss 2.855624267578125\n",
      "Epoch 158 : train_loss 2.8573799438476564\n",
      "Epoch 158 : test_loss 2.973087158203125\n",
      "Epoch 159 : train_loss 2.897286661783854\n",
      "Epoch 159 : test_loss 2.9726444091796873\n",
      "Epoch 160 : train_loss 2.881335205078125\n",
      "Epoch 160 : test_loss 2.8639149169921874\n",
      "saved model\n",
      "Epoch 161 : train_loss 2.828048387315538\n",
      "Epoch 161 : test_loss 2.8095052490234376\n",
      "Epoch 162 : train_loss 2.791009521484375\n",
      "Epoch 162 : test_loss 2.8211514892578125\n",
      "Epoch 163 : train_loss 2.7948817070855037\n",
      "Epoch 163 : test_loss 2.8504283447265624\n",
      "Epoch 164 : train_loss 2.8016986490885416\n",
      "Epoch 164 : test_loss 2.8834022216796873\n",
      "Epoch 165 : train_loss 2.818544684516059\n",
      "Epoch 165 : test_loss 2.920055419921875\n",
      "Epoch 166 : train_loss 2.822359870062934\n",
      "Epoch 166 : test_loss 2.8994150390625\n",
      "Epoch 167 : train_loss 2.7992366672092013\n",
      "Epoch 167 : test_loss 2.8400052490234375\n",
      "saved model\n",
      "Epoch 168 : train_loss 2.7470912679036457\n",
      "Epoch 168 : test_loss 2.7631212158203127\n",
      "saved model\n",
      "Epoch 169 : train_loss 2.6867245415581595\n",
      "Epoch 169 : test_loss 2.666849853515625\n",
      "saved model\n",
      "Epoch 170 : train_loss 2.628609598795573\n",
      "Epoch 170 : test_loss 2.621179443359375\n",
      "saved model\n",
      "Epoch 171 : train_loss 2.6184954087999133\n",
      "Epoch 171 : test_loss 2.61845068359375\n",
      "Epoch 172 : train_loss 2.6564123874240453\n",
      "Epoch 172 : test_loss 2.654928955078125\n",
      "Epoch 173 : train_loss 2.7126109551323783\n",
      "Epoch 173 : test_loss 2.734590576171875\n",
      "Epoch 174 : train_loss 2.727264431423611\n",
      "Epoch 174 : test_loss 2.6920458984375\n",
      "saved model\n",
      "Epoch 175 : train_loss 2.676060051812066\n",
      "Epoch 175 : test_loss 2.6127921142578123\n",
      "saved model\n",
      "Epoch 176 : train_loss 2.615945848253038\n",
      "Epoch 176 : test_loss 2.57723583984375\n",
      "saved model\n",
      "Epoch 177 : train_loss 2.557762444390191\n",
      "Epoch 177 : test_loss 2.5050244750976565\n",
      "Epoch 178 : train_loss 2.522143371582031\n",
      "Epoch 178 : test_loss 2.5244334716796875\n",
      "Epoch 179 : train_loss 2.5154854804144966\n",
      "Epoch 179 : test_loss 2.55191796875\n",
      "Epoch 180 : train_loss 2.5228505859375\n",
      "Epoch 180 : test_loss 2.570423583984375\n",
      "Epoch 181 : train_loss 2.540897989908854\n",
      "Epoch 181 : test_loss 2.57293310546875\n",
      "Epoch 182 : train_loss 2.571917236328125\n",
      "Epoch 182 : test_loss 2.627266845703125\n",
      "Epoch 183 : train_loss 2.582060038248698\n",
      "Epoch 183 : test_loss 2.6192818603515624\n",
      "Epoch 184 : train_loss 2.5773683064778647\n",
      "Epoch 184 : test_loss 2.5834780883789064\n",
      "Epoch 185 : train_loss 2.5449132758246527\n",
      "Epoch 185 : test_loss 2.525089294433594\n",
      "saved model\n",
      "Epoch 186 : train_loss 2.495547858344184\n",
      "Epoch 186 : test_loss 2.4412171630859376\n",
      "saved model\n",
      "Epoch 187 : train_loss 2.440593254937066\n",
      "Epoch 187 : test_loss 2.421593994140625\n",
      "saved model\n",
      "Epoch 188 : train_loss 2.383455349392361\n",
      "Epoch 188 : test_loss 2.387248229980469\n",
      "saved model\n",
      "Epoch 189 : train_loss 2.3655697360568575\n",
      "Epoch 189 : test_loss 2.3554892578125\n",
      "Epoch 190 : train_loss 2.369472744411892\n",
      "Epoch 190 : test_loss 2.4108642578125\n",
      "Epoch 191 : train_loss 2.3986688435872394\n",
      "Epoch 191 : test_loss 2.382481750488281\n",
      "Epoch 192 : train_loss 2.4241682603624133\n",
      "Epoch 192 : test_loss 2.4650897216796874\n",
      "Epoch 193 : train_loss 2.4200657348632815\n",
      "Epoch 193 : test_loss 2.4494655151367186\n",
      "Epoch 194 : train_loss 2.408009446885851\n",
      "Epoch 194 : test_loss 2.373349365234375\n",
      "saved model\n",
      "Epoch 195 : train_loss 2.379428141276042\n",
      "Epoch 195 : test_loss 2.3343092041015625\n",
      "saved model\n",
      "Epoch 196 : train_loss 2.34168753390842\n",
      "Epoch 196 : test_loss 2.282664855957031\n",
      "saved model\n",
      "Epoch 197 : train_loss 2.2958787909613716\n",
      "Epoch 197 : test_loss 2.2306135864257812\n",
      "saved model\n",
      "Epoch 198 : train_loss 2.2670743476019966\n",
      "Epoch 198 : test_loss 2.21117822265625\n",
      "saved model\n",
      "Epoch 199 : train_loss 2.2409896104600695\n",
      "Epoch 199 : test_loss 2.1551190185546876\n",
      "Epoch 200 : train_loss 2.2288066745334203\n",
      "Epoch 200 : test_loss 2.201468505859375\n",
      "Epoch 201 : train_loss 2.22811767578125\n",
      "Epoch 201 : test_loss 2.22210498046875\n",
      "Epoch 202 : train_loss 2.2467152370876735\n",
      "Epoch 202 : test_loss 2.280833251953125\n",
      "Epoch 203 : train_loss 2.2975577053493925\n",
      "Epoch 203 : test_loss 2.3377532958984375\n",
      "Epoch 204 : train_loss 2.329647759331597\n",
      "Epoch 204 : test_loss 2.317885498046875\n",
      "Epoch 205 : train_loss 2.3078671875\n",
      "Epoch 205 : test_loss 2.2844268798828127\n",
      "Epoch 206 : train_loss 2.2628424886067706\n",
      "Epoch 206 : test_loss 2.2322568359375\n",
      "Epoch 207 : train_loss 2.2313992580837674\n",
      "Epoch 207 : test_loss 2.238640563964844\n",
      "Epoch 208 : train_loss 2.2311076185438368\n",
      "Epoch 208 : test_loss 2.243443115234375\n",
      "Epoch 209 : train_loss 2.2361284925672744\n",
      "Epoch 209 : test_loss 2.26548388671875\n",
      "Epoch 210 : train_loss 2.2482708808051215\n",
      "Epoch 210 : test_loss 2.207308044433594\n",
      "Epoch 211 : train_loss 2.2429802856445313\n",
      "Epoch 211 : test_loss 2.2166767578125\n",
      "Epoch 212 : train_loss 2.244548834906684\n",
      "Epoch 212 : test_loss 2.2408436889648438\n",
      "Epoch 213 : train_loss 2.221400187174479\n",
      "Epoch 213 : test_loss 2.2267666015625\n",
      "Epoch 214 : train_loss 2.2039550103081598\n",
      "Epoch 214 : test_loss 2.1985736083984375\n",
      "Epoch 215 : train_loss 2.183621419270833\n",
      "Epoch 215 : test_loss 2.1778580322265624\n",
      "saved model\n",
      "Epoch 216 : train_loss 2.164987636990017\n",
      "Epoch 216 : test_loss 2.1133631591796873\n",
      "Epoch 217 : train_loss 2.1423292982313367\n",
      "Epoch 217 : test_loss 2.129130859375\n",
      "saved model\n",
      "Epoch 218 : train_loss 2.1213258531358505\n",
      "Epoch 218 : test_loss 2.083007019042969\n",
      "saved model\n",
      "Epoch 219 : train_loss 2.094696790907118\n",
      "Epoch 219 : test_loss 2.0797568359375\n",
      "saved model\n",
      "Epoch 220 : train_loss 2.070477756076389\n",
      "Epoch 220 : test_loss 2.0630572509765623\n",
      "saved model\n",
      "Epoch 221 : train_loss 2.055624260796441\n",
      "Epoch 221 : test_loss 2.0387828369140624\n",
      "saved model\n",
      "Epoch 222 : train_loss 2.041446607801649\n",
      "Epoch 222 : test_loss 2.020031005859375\n",
      "saved model\n",
      "Epoch 223 : train_loss 2.0277428317599826\n",
      "Epoch 223 : test_loss 1.9968374633789063\n",
      "Epoch 224 : train_loss 2.0116656155056423\n",
      "Epoch 224 : test_loss 2.011852355957031\n",
      "saved model\n",
      "Epoch 225 : train_loss 2.0040372382269966\n",
      "Epoch 225 : test_loss 1.9847935791015625\n",
      "saved model\n",
      "Epoch 226 : train_loss 1.9979580417209202\n",
      "Epoch 226 : test_loss 1.9706858520507813\n",
      "Epoch 227 : train_loss 1.9934075385199652\n",
      "Epoch 227 : test_loss 1.9856387939453124\n",
      "Epoch 228 : train_loss 1.9869439629448784\n",
      "Epoch 228 : test_loss 1.9773577880859374\n",
      "Epoch 229 : train_loss 1.9911671074761286\n",
      "Epoch 229 : test_loss 1.982669921875\n",
      "Epoch 230 : train_loss 1.9808710055881076\n",
      "Epoch 230 : test_loss 1.985594482421875\n",
      "Epoch 231 : train_loss 1.9746150783962673\n",
      "Epoch 231 : test_loss 1.988206787109375\n",
      "Epoch 232 : train_loss 1.9801115383572048\n",
      "Epoch 232 : test_loss 1.977876220703125\n",
      "Epoch 233 : train_loss 1.9720469021267362\n",
      "Epoch 233 : test_loss 1.977824951171875\n",
      "Epoch 234 : train_loss 1.9770181477864583\n",
      "Epoch 234 : test_loss 1.9793009643554687\n",
      "Epoch 235 : train_loss 1.9810598822699652\n",
      "Epoch 235 : test_loss 1.9996254272460938\n",
      "Epoch 236 : train_loss 1.987036349826389\n",
      "Epoch 236 : test_loss 1.9976094970703124\n",
      "Epoch 237 : train_loss 1.9882179497612846\n",
      "Epoch 237 : test_loss 1.9786663818359376\n",
      "Epoch 238 : train_loss 1.97333544921875\n",
      "Epoch 238 : test_loss 1.98024462890625\n",
      "saved model\n",
      "Epoch 239 : train_loss 1.9882833591037326\n",
      "Epoch 239 : test_loss 1.9630118408203125\n",
      "saved model\n",
      "Epoch 240 : train_loss 1.9802245212131075\n",
      "Epoch 240 : test_loss 1.961027099609375\n",
      "Epoch 241 : train_loss 1.9882591281467015\n",
      "Epoch 241 : test_loss 1.9942793579101563\n",
      "Epoch 242 : train_loss 1.9770003051757812\n",
      "Epoch 242 : test_loss 1.9835224609375\n",
      "Epoch 243 : train_loss 1.9755638563368056\n",
      "Epoch 243 : test_loss 1.9916395263671876\n",
      "Epoch 244 : train_loss 1.9843208618164063\n",
      "Epoch 244 : test_loss 1.9703031005859375\n",
      "saved model\n",
      "Epoch 245 : train_loss 1.9971824883355034\n",
      "Epoch 245 : test_loss 1.9565081787109375\n",
      "Epoch 246 : train_loss 1.9825172322591147\n",
      "Epoch 246 : test_loss 2.0167138671875\n",
      "Epoch 247 : train_loss 1.9848695407443577\n",
      "Epoch 247 : test_loss 1.988524658203125\n",
      "Epoch 248 : train_loss 1.9724573635525173\n",
      "Epoch 248 : test_loss 1.9643095703125\n",
      "Epoch 249 : train_loss 1.978744852701823\n",
      "Epoch 249 : test_loss 2.00317431640625\n",
      "Epoch 250 : train_loss 1.976709228515625\n",
      "Epoch 250 : test_loss 1.9644723510742188\n",
      "Epoch 251 : train_loss 1.976553697374132\n",
      "Epoch 251 : test_loss 1.9761806640625\n",
      "Epoch 252 : train_loss 1.9755744018554688\n",
      "Epoch 252 : test_loss 2.00114453125\n",
      "Epoch 253 : train_loss 1.977320271809896\n",
      "Epoch 253 : test_loss 1.9820577392578125\n",
      "Epoch 254 : train_loss 1.972427266438802\n",
      "Epoch 254 : test_loss 1.9848988647460937\n",
      "saved model\n",
      "Epoch 255 : train_loss 1.971336697048611\n",
      "Epoch 255 : test_loss 1.9373643798828124\n",
      "Epoch 256 : train_loss 1.971792697482639\n",
      "Epoch 256 : test_loss 1.963915771484375\n",
      "Epoch 257 : train_loss 1.9768277655707465\n",
      "Epoch 257 : test_loss 1.9536251220703125\n",
      "Epoch 258 : train_loss 1.9780885959201389\n",
      "Epoch 258 : test_loss 1.973544677734375\n",
      "Epoch 259 : train_loss 1.979354051378038\n",
      "Epoch 259 : test_loss 2.0016160888671877\n",
      "Epoch 260 : train_loss 1.97069974093967\n",
      "Epoch 260 : test_loss 1.9977020263671874\n",
      "Epoch 261 : train_loss 1.9753533664279515\n",
      "Epoch 261 : test_loss 1.9816888427734376\n",
      "Epoch 262 : train_loss 1.9684173990885416\n",
      "Epoch 262 : test_loss 2.0149918212890623\n",
      "Epoch 263 : train_loss 1.9708896213107638\n",
      "Epoch 263 : test_loss 1.9695162353515625\n",
      "Epoch 264 : train_loss 1.9674176635742187\n",
      "Epoch 264 : test_loss 1.9979126586914062\n",
      "Epoch 265 : train_loss 1.9585743950737846\n",
      "Epoch 265 : test_loss 1.995832275390625\n",
      "Epoch 266 : train_loss 1.9703959621853298\n",
      "Epoch 266 : test_loss 2.014256164550781\n",
      "Epoch 267 : train_loss 1.9688437974717883\n",
      "Epoch 267 : test_loss 1.9825765991210937\n",
      "Epoch 268 : train_loss 1.9698045789930556\n",
      "Epoch 268 : test_loss 1.9611771240234375\n",
      "Epoch 269 : train_loss 1.9653218519422744\n",
      "Epoch 269 : test_loss 1.9562047729492187\n",
      "Epoch 270 : train_loss 1.9617103542751737\n",
      "Epoch 270 : test_loss 1.955740234375\n",
      "Epoch 271 : train_loss 1.9686545817057293\n",
      "Epoch 271 : test_loss 1.9833501586914062\n",
      "Epoch 272 : train_loss 1.9701459147135416\n",
      "Epoch 272 : test_loss 1.9830167846679687\n",
      "Epoch 273 : train_loss 1.9613536105685765\n",
      "Epoch 273 : test_loss 1.9837926025390624\n",
      "Epoch 274 : train_loss 1.9592858479817707\n",
      "Epoch 274 : test_loss 1.9527368774414062\n",
      "Epoch 275 : train_loss 1.9635672268337674\n",
      "Epoch 275 : test_loss 1.9803912963867187\n",
      "Epoch 276 : train_loss 1.9704105156792535\n",
      "Epoch 276 : test_loss 1.9761463623046875\n",
      "Epoch 277 : train_loss 1.9695226643880208\n",
      "Epoch 277 : test_loss 1.9892095947265624\n",
      "Epoch 278 : train_loss 1.959232699924045\n",
      "Epoch 278 : test_loss 1.95879296875\n",
      "Epoch 279 : train_loss 1.9656809624565972\n",
      "Epoch 279 : test_loss 1.9489991455078124\n",
      "Epoch 280 : train_loss 1.9629901462131076\n",
      "Epoch 280 : test_loss 1.987714111328125\n",
      "Epoch 281 : train_loss 1.9641148342556423\n",
      "Epoch 281 : test_loss 1.975458251953125\n",
      "Epoch 282 : train_loss 1.968337205674913\n",
      "Epoch 282 : test_loss 1.9570985107421874\n",
      "saved model\n",
      "Epoch 283 : train_loss 1.9655578477647568\n",
      "Epoch 283 : test_loss 1.9330361938476563\n",
      "Epoch 284 : train_loss 1.9653590291341145\n",
      "Epoch 284 : test_loss 1.9639492797851563\n",
      "Epoch 285 : train_loss 1.9659105767144098\n",
      "Epoch 285 : test_loss 1.9686363525390624\n",
      "Epoch 286 : train_loss 1.9742620442708334\n",
      "Epoch 286 : test_loss 1.9962550048828125\n",
      "Epoch 287 : train_loss 1.9602761908637152\n",
      "Epoch 287 : test_loss 1.9525885009765624\n",
      "Epoch 288 : train_loss 1.9641699557834202\n",
      "Epoch 288 : test_loss 1.9541791381835938\n",
      "saved model\n",
      "Epoch 289 : train_loss 1.9615648396809895\n",
      "Epoch 289 : test_loss 1.9278463745117187\n",
      "Epoch 290 : train_loss 1.9653489312065973\n",
      "Epoch 290 : test_loss 1.9732521362304687\n",
      "Epoch 291 : train_loss 1.9666387736002604\n",
      "Epoch 291 : test_loss 1.9741304321289062\n",
      "Epoch 292 : train_loss 1.9641500786675348\n",
      "Epoch 292 : test_loss 1.9652915649414062\n",
      "Epoch 293 : train_loss 1.967124504937066\n",
      "Epoch 293 : test_loss 1.9700787353515625\n",
      "Epoch 294 : train_loss 1.9615533582899305\n",
      "Epoch 294 : test_loss 2.016267822265625\n",
      "Epoch 295 : train_loss 1.9620690578884548\n",
      "Epoch 295 : test_loss 1.9572222900390626\n",
      "Epoch 296 : train_loss 1.9650265367296007\n",
      "Epoch 296 : test_loss 1.9674032592773438\n",
      "Epoch 297 : train_loss 1.9630258246527779\n",
      "Epoch 297 : test_loss 1.9786157836914062\n",
      "Epoch 298 : train_loss 1.9567913343641492\n",
      "Epoch 298 : test_loss 1.9640919189453125\n",
      "Epoch 299 : train_loss 1.9740794270833333\n",
      "Epoch 299 : test_loss 1.973796142578125\n",
      "Epoch 300 : train_loss 1.9629495103624133\n",
      "Epoch 300 : test_loss 1.9405071411132813\n",
      "Epoch 301 : train_loss 1.960686258951823\n",
      "Epoch 301 : test_loss 1.96178662109375\n",
      "saved model\n",
      "Epoch 302 : train_loss 1.9678894517686631\n",
      "Epoch 302 : test_loss 1.9202019653320312\n",
      "Epoch 303 : train_loss 1.9557383490668403\n",
      "Epoch 303 : test_loss 1.9503129272460937\n",
      "Epoch 304 : train_loss 1.9640986938476563\n",
      "Epoch 304 : test_loss 1.9958987426757813\n",
      "Epoch 305 : train_loss 1.968527845594618\n",
      "Epoch 305 : test_loss 1.9783314208984375\n",
      "Epoch 306 : train_loss 1.961429694281684\n",
      "Epoch 306 : test_loss 1.99445556640625\n",
      "Epoch 307 : train_loss 1.9532637057834201\n",
      "Epoch 307 : test_loss 1.939897216796875\n",
      "Epoch 308 : train_loss 1.9604696451822916\n",
      "Epoch 308 : test_loss 1.9586325073242188\n",
      "Epoch 309 : train_loss 1.9512700873480904\n",
      "Epoch 309 : test_loss 1.9882105712890625\n",
      "Epoch 310 : train_loss 1.9565354071723091\n",
      "Epoch 310 : test_loss 1.938760986328125\n",
      "Epoch 311 : train_loss 1.9602861531575522\n",
      "Epoch 311 : test_loss 1.980502685546875\n",
      "Epoch 312 : train_loss 1.9673838433159723\n",
      "Epoch 312 : test_loss 1.9943273315429688\n",
      "Epoch 313 : train_loss 1.9543322007921007\n",
      "Epoch 313 : test_loss 1.9650515747070312\n",
      "Epoch 314 : train_loss 1.9641904296875\n",
      "Epoch 314 : test_loss 1.934531982421875\n",
      "Epoch 315 : train_loss 1.9607847696940104\n",
      "Epoch 315 : test_loss 1.9541349487304687\n",
      "Epoch 316 : train_loss 1.9556489868164062\n",
      "Epoch 316 : test_loss 1.9445762939453124\n",
      "Epoch 317 : train_loss 1.9554317559136285\n",
      "Epoch 317 : test_loss 1.9788897705078126\n",
      "Epoch 318 : train_loss 1.9637427435980903\n",
      "Epoch 318 : test_loss 1.9618383178710936\n",
      "Epoch 319 : train_loss 1.9549009467230902\n",
      "Epoch 319 : test_loss 1.9416399536132813\n",
      "Epoch 320 : train_loss 1.9594700656467015\n",
      "Epoch 320 : test_loss 1.9525673828125\n",
      "Epoch 321 : train_loss 1.9532296142578125\n",
      "Epoch 321 : test_loss 1.96722119140625\n",
      "Epoch 322 : train_loss 1.9676489868164062\n",
      "Epoch 322 : test_loss 1.9469385986328125\n",
      "Epoch 323 : train_loss 1.961976813422309\n",
      "Epoch 323 : test_loss 1.9481907958984375\n",
      "Epoch 324 : train_loss 1.9573290405273438\n",
      "Epoch 324 : test_loss 1.9456588745117187\n",
      "Epoch 325 : train_loss 1.966694315592448\n",
      "Epoch 325 : test_loss 1.9486641845703125\n",
      "Epoch 326 : train_loss 1.9576051601833768\n",
      "Epoch 326 : test_loss 1.9564503173828125\n",
      "Epoch 327 : train_loss 1.9589865315755208\n",
      "Epoch 327 : test_loss 1.9621231689453125\n",
      "Epoch 328 : train_loss 1.965318345811632\n",
      "Epoch 328 : test_loss 1.971400390625\n",
      "Epoch 329 : train_loss 1.9579007161458333\n",
      "Epoch 329 : test_loss 1.984410888671875\n",
      "Epoch 330 : train_loss 1.9682333577473958\n",
      "Epoch 330 : test_loss 1.9662723388671874\n",
      "Epoch 331 : train_loss 1.97131441921658\n",
      "Epoch 331 : test_loss 1.92899658203125\n",
      "Epoch 332 : train_loss 1.960503668891059\n",
      "Epoch 332 : test_loss 1.9671451416015624\n",
      "Epoch 333 : train_loss 1.9572527398003472\n",
      "Epoch 333 : test_loss 1.9534158935546875\n",
      "Epoch 334 : train_loss 1.9656505398220485\n",
      "Epoch 334 : test_loss 1.932029541015625\n",
      "Epoch 335 : train_loss 1.9625025431315104\n",
      "Epoch 335 : test_loss 1.9747537231445313\n",
      "Epoch 336 : train_loss 1.9657134941948784\n",
      "Epoch 336 : test_loss 1.9795982666015626\n",
      "Epoch 337 : train_loss 1.9573596801757813\n",
      "Epoch 337 : test_loss 1.9617006225585938\n",
      "Epoch 338 : train_loss 1.9595176730685764\n",
      "Epoch 338 : test_loss 1.975498291015625\n",
      "Epoch 339 : train_loss 1.96714456515842\n",
      "Epoch 339 : test_loss 1.9760257568359374\n",
      "Epoch 340 : train_loss 1.9577313096788194\n",
      "Epoch 340 : test_loss 2.0169679565429686\n",
      "saved model\n",
      "Epoch 341 : train_loss 1.956301540798611\n",
      "Epoch 341 : test_loss 1.9158411865234375\n",
      "Epoch 342 : train_loss 1.9641207953559028\n",
      "Epoch 342 : test_loss 2.00124560546875\n",
      "Epoch 343 : train_loss 1.9604781290690103\n",
      "Epoch 343 : test_loss 1.97030029296875\n",
      "Epoch 344 : train_loss 1.9615107421875\n",
      "Epoch 344 : test_loss 1.9535087890625\n",
      "Epoch 345 : train_loss 1.9547569647894965\n",
      "Epoch 345 : test_loss 1.945729736328125\n",
      "Epoch 346 : train_loss 1.9563121270073784\n",
      "Epoch 346 : test_loss 1.9877864990234375\n",
      "Epoch 347 : train_loss 1.9624363471137152\n",
      "Epoch 347 : test_loss 1.9684820556640625\n",
      "Epoch 348 : train_loss 1.9555257025824653\n",
      "Epoch 348 : test_loss 1.9682382202148438\n",
      "Epoch 349 : train_loss 1.9563488498263888\n",
      "Epoch 349 : test_loss 1.959511962890625\n",
      "Epoch 350 : train_loss 1.9593667127821182\n",
      "Epoch 350 : test_loss 1.951689697265625\n",
      "Epoch 351 : train_loss 1.9542519802517362\n",
      "Epoch 351 : test_loss 1.9928059692382813\n",
      "Epoch 352 : train_loss 1.9550021497938368\n",
      "Epoch 352 : test_loss 1.9467791748046874\n",
      "Epoch 353 : train_loss 1.9576029934353298\n",
      "Epoch 353 : test_loss 1.9827525024414063\n",
      "Epoch 354 : train_loss 1.9587272338867188\n",
      "Epoch 354 : test_loss 1.96489892578125\n",
      "Epoch 355 : train_loss 1.9607372504340277\n",
      "Epoch 355 : test_loss 1.9585244140625\n",
      "Epoch 356 : train_loss 1.9526258409288195\n",
      "Epoch 356 : test_loss 1.9886738891601563\n",
      "Epoch 357 : train_loss 1.9566565551757813\n",
      "Epoch 357 : test_loss 2.0142833251953123\n",
      "Epoch 358 : train_loss 1.9592920464409722\n",
      "Epoch 358 : test_loss 1.9240785522460937\n",
      "Epoch 359 : train_loss 1.9563140326605903\n",
      "Epoch 359 : test_loss 1.9833870239257811\n",
      "Epoch 360 : train_loss 1.9557208387586806\n",
      "Epoch 360 : test_loss 1.944183837890625\n",
      "Epoch 361 : train_loss 1.9562716878255209\n",
      "Epoch 361 : test_loss 1.9370213623046875\n",
      "Epoch 362 : train_loss 1.9572637464735243\n",
      "Epoch 362 : test_loss 1.9920870361328125\n",
      "Epoch 363 : train_loss 1.957259806315104\n",
      "Epoch 363 : test_loss 1.9556998291015626\n",
      "Epoch 364 : train_loss 1.9619956529405382\n",
      "Epoch 364 : test_loss 1.9524937133789062\n",
      "Epoch 365 : train_loss 1.9608858167860244\n",
      "Epoch 365 : test_loss 1.9544632568359375\n",
      "Epoch 366 : train_loss 1.96072314453125\n",
      "Epoch 366 : test_loss 1.9791053466796875\n",
      "Epoch 367 : train_loss 1.9534181857638888\n",
      "Epoch 367 : test_loss 1.9520530395507814\n",
      "Epoch 368 : train_loss 1.956148145887587\n",
      "Epoch 368 : test_loss 1.9735661010742187\n",
      "Epoch 369 : train_loss 1.9568262125651041\n",
      "Epoch 369 : test_loss 1.9546994018554686\n",
      "Epoch 370 : train_loss 1.9685460951063367\n",
      "Epoch 370 : test_loss 2.010886962890625\n",
      "Epoch 371 : train_loss 1.9493055555555556\n",
      "Epoch 371 : test_loss 1.9443236083984374\n",
      "Epoch 372 : train_loss 1.96103662109375\n",
      "Epoch 372 : test_loss 1.973596435546875\n",
      "Epoch 373 : train_loss 1.9606559719509549\n",
      "Epoch 373 : test_loss 1.936419921875\n",
      "Epoch 374 : train_loss 1.9580128919813369\n",
      "Epoch 374 : test_loss 1.989424560546875\n",
      "Epoch 375 : train_loss 1.9650316501193577\n",
      "Epoch 375 : test_loss 1.9986682739257813\n",
      "Epoch 376 : train_loss 1.9512623765733508\n",
      "Epoch 376 : test_loss 1.9640911865234374\n",
      "Epoch 377 : train_loss 1.9528362291124133\n",
      "Epoch 377 : test_loss 1.9710255126953125\n",
      "Epoch 378 : train_loss 1.9521385565863716\n",
      "Epoch 378 : test_loss 1.96232763671875\n",
      "Epoch 379 : train_loss 1.954099846733941\n",
      "Epoch 379 : test_loss 1.9396527709960938\n",
      "Epoch 380 : train_loss 1.9634629584418404\n",
      "Epoch 380 : test_loss 1.9197342529296875\n",
      "Epoch 381 : train_loss 1.962062750922309\n",
      "Epoch 381 : test_loss 1.9616302490234374\n",
      "Epoch 382 : train_loss 1.9658617689344617\n",
      "Epoch 382 : test_loss 1.975948486328125\n",
      "Epoch 383 : train_loss 1.9633133205837674\n",
      "Epoch 383 : test_loss 1.9516654052734375\n",
      "Epoch 384 : train_loss 1.9570598551432292\n",
      "Epoch 384 : test_loss 1.9195093994140624\n",
      "Epoch 385 : train_loss 1.955693589952257\n",
      "Epoch 385 : test_loss 1.9444708251953124\n",
      "Epoch 386 : train_loss 1.9512049831814235\n",
      "Epoch 386 : test_loss 1.9539877319335937\n",
      "Epoch 387 : train_loss 1.9602767808702257\n",
      "Epoch 387 : test_loss 1.9550941772460937\n",
      "Epoch 388 : train_loss 1.9536824815538194\n",
      "Epoch 388 : test_loss 1.940649169921875\n",
      "Epoch 389 : train_loss 1.9568344930013022\n",
      "Epoch 389 : test_loss 1.983502197265625\n",
      "Epoch 390 : train_loss 1.9574094509548612\n",
      "Epoch 390 : test_loss 1.9495702514648436\n",
      "Epoch 391 : train_loss 1.9542771063910591\n",
      "Epoch 391 : test_loss 1.961673828125\n",
      "Epoch 392 : train_loss 1.9528843519422743\n",
      "Epoch 392 : test_loss 1.9512034301757812\n",
      "Epoch 393 : train_loss 1.9637247653537326\n",
      "Epoch 393 : test_loss 1.954805908203125\n",
      "Epoch 394 : train_loss 1.9646625569661458\n",
      "Epoch 394 : test_loss 1.9522096557617188\n",
      "Epoch 395 : train_loss 1.9494957817925347\n",
      "Epoch 395 : test_loss 1.9385218505859374\n",
      "Epoch 396 : train_loss 1.9508620130750869\n",
      "Epoch 396 : test_loss 1.9329207153320314\n",
      "Epoch 397 : train_loss 1.9536729668511286\n",
      "Epoch 397 : test_loss 1.95733984375\n",
      "Epoch 398 : train_loss 1.9546683213975695\n",
      "Epoch 398 : test_loss 1.9392103271484376\n",
      "Epoch 399 : train_loss 1.956569363064236\n",
      "Epoch 399 : test_loss 1.9797279663085938\n",
      "Epoch 400 : train_loss 1.955285400390625\n",
      "Epoch 400 : test_loss 1.9625812377929688\n",
      "Epoch 401 : train_loss 1.9495977240668403\n",
      "Epoch 401 : test_loss 1.9684541015625\n",
      "Epoch 402 : train_loss 1.9533125610351563\n",
      "Epoch 402 : test_loss 1.968509765625\n",
      "Epoch 403 : train_loss 1.9563674248589409\n",
      "Epoch 403 : test_loss 1.9426484375\n",
      "Epoch 404 : train_loss 1.9560689968532987\n",
      "Epoch 404 : test_loss 1.9416793212890624\n",
      "Epoch 405 : train_loss 1.958900682237413\n",
      "Epoch 405 : test_loss 1.9956690673828126\n",
      "Epoch 406 : train_loss 1.9520540093315972\n",
      "Epoch 406 : test_loss 1.9388539428710938\n",
      "Epoch 407 : train_loss 1.9592555406358507\n",
      "Epoch 407 : test_loss 1.9521903076171876\n",
      "Epoch 408 : train_loss 1.9532083401150173\n",
      "Epoch 408 : test_loss 1.97449853515625\n",
      "Epoch 409 : train_loss 1.9524539523654514\n",
      "Epoch 409 : test_loss 1.9354547119140626\n",
      "Epoch 410 : train_loss 1.9603351508246527\n",
      "Epoch 410 : test_loss 1.9553955688476563\n",
      "Epoch 411 : train_loss 1.9528536716037326\n",
      "Epoch 411 : test_loss 1.9485411376953126\n",
      "Epoch 412 : train_loss 1.948831312391493\n",
      "Epoch 412 : test_loss 1.9825875244140625\n",
      "Epoch 413 : train_loss 1.9560524495442708\n",
      "Epoch 413 : test_loss 1.9847347412109375\n",
      "Epoch 414 : train_loss 1.9570117594401042\n",
      "Epoch 414 : test_loss 1.9775631103515625\n",
      "Epoch 415 : train_loss 1.9538306613498264\n",
      "Epoch 415 : test_loss 1.9240189208984375\n",
      "Epoch 416 : train_loss 1.951679958767361\n",
      "Epoch 416 : test_loss 1.9750086669921876\n",
      "Epoch 417 : train_loss 1.9584810994466146\n",
      "Epoch 417 : test_loss 1.94582421875\n",
      "Epoch 418 : train_loss 1.9569515991210937\n",
      "Epoch 418 : test_loss 1.9713701171875\n",
      "Epoch 419 : train_loss 1.961273173014323\n",
      "Epoch 419 : test_loss 1.960551513671875\n",
      "Epoch 420 : train_loss 1.9527262030707466\n",
      "Epoch 420 : test_loss 1.9469544067382814\n",
      "Epoch 421 : train_loss 1.9427283460828992\n",
      "Epoch 421 : test_loss 1.9460908203125\n",
      "Epoch 422 : train_loss 1.9554586520724826\n",
      "Epoch 422 : test_loss 1.9315576782226562\n",
      "Epoch 423 : train_loss 1.956240227593316\n",
      "Epoch 423 : test_loss 1.9214395141601563\n",
      "Epoch 424 : train_loss 1.9614621039496527\n",
      "Epoch 424 : test_loss 1.9526936645507813\n",
      "Epoch 425 : train_loss 1.9567323404947916\n",
      "Epoch 425 : test_loss 1.939116455078125\n",
      "Epoch 426 : train_loss 1.9552844577365451\n",
      "Epoch 426 : test_loss 1.9720411987304687\n",
      "Epoch 427 : train_loss 1.9522651977539063\n",
      "Epoch 427 : test_loss 1.941251953125\n",
      "Epoch 428 : train_loss 1.9511026950412327\n",
      "Epoch 428 : test_loss 1.968038818359375\n",
      "Epoch 429 : train_loss 1.954113505045573\n",
      "Epoch 429 : test_loss 1.9501893920898437\n",
      "Epoch 430 : train_loss 1.9520634765625\n",
      "Epoch 430 : test_loss 1.941105712890625\n",
      "Epoch 431 : train_loss 1.9522689819335937\n",
      "Epoch 431 : test_loss 1.9796239013671875\n",
      "Epoch 432 : train_loss 1.9483374565972222\n",
      "Epoch 432 : test_loss 1.9351922607421874\n",
      "Epoch 433 : train_loss 1.9479566175672742\n",
      "Epoch 433 : test_loss 1.9631051635742187\n",
      "Epoch 434 : train_loss 1.9505241495768229\n",
      "Epoch 434 : test_loss 1.9578447875976563\n",
      "Epoch 435 : train_loss 1.9549281887478298\n",
      "Epoch 435 : test_loss 1.947037353515625\n",
      "Epoch 436 : train_loss 1.9558099839952257\n",
      "Epoch 436 : test_loss 1.9746607666015625\n",
      "Epoch 437 : train_loss 1.9576340738932292\n",
      "Epoch 437 : test_loss 1.9663130493164063\n",
      "Epoch 438 : train_loss 1.9525132378472223\n",
      "Epoch 438 : test_loss 1.916494873046875\n",
      "Epoch 439 : train_loss 1.9509327528211806\n",
      "Epoch 439 : test_loss 1.959669921875\n",
      "Epoch 440 : train_loss 1.9549573974609376\n",
      "Epoch 440 : test_loss 1.9615303955078125\n",
      "Epoch 441 : train_loss 1.9566108262803819\n",
      "Epoch 441 : test_loss 1.9864482421875\n",
      "Epoch 442 : train_loss 1.9595805392795138\n",
      "Epoch 442 : test_loss 1.9630927734375\n",
      "Epoch 443 : train_loss 1.9486863098144531\n",
      "Epoch 443 : test_loss 1.9554669189453124\n",
      "Epoch 444 : train_loss 1.954041022406684\n",
      "Epoch 444 : test_loss 1.9448837280273437\n",
      "Epoch 445 : train_loss 1.9524006551106772\n",
      "Epoch 445 : test_loss 1.965235107421875\n",
      "Epoch 446 : train_loss 1.9526098022460938\n",
      "Epoch 446 : test_loss 1.9415054931640625\n",
      "Epoch 447 : train_loss 1.9531124131944444\n",
      "Epoch 447 : test_loss 1.946377685546875\n",
      "Epoch 448 : train_loss 1.9447723456488715\n",
      "Epoch 448 : test_loss 1.9653605346679688\n",
      "Epoch 449 : train_loss 1.9567124430338543\n",
      "Epoch 449 : test_loss 1.9377677001953124\n",
      "Epoch 450 : train_loss 1.9559171820746528\n",
      "Epoch 450 : test_loss 1.9516936645507812\n",
      "Epoch 451 : train_loss 1.9559515787760418\n",
      "Epoch 451 : test_loss 1.9584693603515626\n",
      "Epoch 452 : train_loss 1.9463666110568576\n",
      "Epoch 452 : test_loss 1.949045166015625\n",
      "Epoch 453 : train_loss 1.943591566297743\n",
      "Epoch 453 : test_loss 1.9415335083007812\n",
      "Epoch 454 : train_loss 1.954506340874566\n",
      "Epoch 454 : test_loss 1.9237607421875\n",
      "Epoch 455 : train_loss 1.9511595526801215\n",
      "Epoch 455 : test_loss 1.9405814208984375\n",
      "Epoch 456 : train_loss 1.9536968451605903\n",
      "Epoch 456 : test_loss 1.9663796997070313\n",
      "Epoch 457 : train_loss 1.952391377766927\n",
      "Epoch 457 : test_loss 1.95482568359375\n",
      "Epoch 458 : train_loss 1.9584329833984375\n",
      "Epoch 458 : test_loss 1.9347763061523438\n",
      "Epoch 459 : train_loss 1.9581315443250868\n",
      "Epoch 459 : test_loss 1.91612744140625\n",
      "Epoch 460 : train_loss 1.9562892591688368\n",
      "Epoch 460 : test_loss 1.942917236328125\n",
      "Epoch 461 : train_loss 1.9447235921223958\n",
      "Epoch 461 : test_loss 1.9614155883789062\n",
      "Epoch 462 : train_loss 1.9590858357747396\n",
      "Epoch 462 : test_loss 1.959722900390625\n",
      "saved model\n",
      "Epoch 463 : train_loss 1.9470916951497397\n",
      "Epoch 463 : test_loss 1.9108175048828124\n",
      "Epoch 464 : train_loss 1.9523328653971355\n",
      "Epoch 464 : test_loss 1.956178466796875\n",
      "Epoch 465 : train_loss 1.9526229587131076\n",
      "Epoch 465 : test_loss 1.9516929931640625\n",
      "Epoch 466 : train_loss 1.9454629448784722\n",
      "Epoch 466 : test_loss 1.9491264038085938\n",
      "Epoch 467 : train_loss 1.9488170572916668\n",
      "Epoch 467 : test_loss 1.97497998046875\n",
      "Epoch 468 : train_loss 1.9477103135850695\n",
      "Epoch 468 : test_loss 1.9681180419921875\n",
      "Epoch 469 : train_loss 1.9464639146592881\n",
      "Epoch 469 : test_loss 1.9408994140625\n",
      "Epoch 470 : train_loss 1.949450934516059\n",
      "Epoch 470 : test_loss 1.959062255859375\n",
      "Epoch 471 : train_loss 1.9542006089952257\n",
      "Epoch 471 : test_loss 1.9509844970703125\n",
      "Epoch 472 : train_loss 1.9491069539388022\n",
      "Epoch 472 : test_loss 1.943450927734375\n",
      "Epoch 473 : train_loss 1.9383182373046874\n",
      "Epoch 473 : test_loss 1.9497510986328126\n",
      "Epoch 474 : train_loss 1.9479859822591146\n",
      "Epoch 474 : test_loss 2.0146878662109375\n",
      "Epoch 475 : train_loss 1.949439432779948\n",
      "Epoch 475 : test_loss 1.9485209350585937\n",
      "Epoch 476 : train_loss 1.9496240030924479\n",
      "Epoch 476 : test_loss 1.958666015625\n",
      "Epoch 477 : train_loss 1.951208747016059\n",
      "Epoch 477 : test_loss 1.9652101440429688\n",
      "Epoch 478 : train_loss 1.9459611273871529\n",
      "Epoch 478 : test_loss 1.9532679443359375\n",
      "Epoch 479 : train_loss 1.9525822347005208\n",
      "Epoch 479 : test_loss 1.9148994140625\n",
      "Epoch 480 : train_loss 1.9507630818684896\n",
      "Epoch 480 : test_loss 1.93816845703125\n",
      "Epoch 481 : train_loss 1.9461593695746529\n",
      "Epoch 481 : test_loss 1.963196533203125\n",
      "Epoch 482 : train_loss 1.9502617662217883\n",
      "Epoch 482 : test_loss 1.9897183837890624\n",
      "Epoch 483 : train_loss 1.9459591064453126\n",
      "Epoch 483 : test_loss 1.9596725463867188\n",
      "Epoch 484 : train_loss 1.949707017686632\n",
      "Epoch 484 : test_loss 1.9268885498046875\n",
      "Epoch 485 : train_loss 1.9515544162326388\n",
      "Epoch 485 : test_loss 1.9609442138671875\n",
      "Epoch 486 : train_loss 1.9492266235351563\n",
      "Epoch 486 : test_loss 1.9660069580078126\n",
      "Epoch 487 : train_loss 1.9549249945746527\n",
      "Epoch 487 : test_loss 1.9293455810546876\n",
      "Epoch 488 : train_loss 1.9431712239583334\n",
      "Epoch 488 : test_loss 1.9579793701171875\n",
      "Epoch 489 : train_loss 1.946990729437934\n",
      "Epoch 489 : test_loss 1.9506796875\n",
      "Epoch 490 : train_loss 1.945160407172309\n",
      "Epoch 490 : test_loss 1.93875439453125\n",
      "Epoch 491 : train_loss 1.9439793023003473\n",
      "Epoch 491 : test_loss 1.98888818359375\n",
      "Epoch 492 : train_loss 1.9537452867296008\n",
      "Epoch 492 : test_loss 1.9337825317382813\n",
      "Epoch 493 : train_loss 1.944788309733073\n",
      "Epoch 493 : test_loss 1.94716064453125\n",
      "Epoch 494 : train_loss 1.9469939914279515\n",
      "Epoch 494 : test_loss 1.96443896484375\n",
      "Epoch 495 : train_loss 1.9424307183159721\n",
      "Epoch 495 : test_loss 1.94781103515625\n",
      "Epoch 496 : train_loss 1.9551280721028645\n",
      "Epoch 496 : test_loss 1.9446697387695313\n",
      "Epoch 497 : train_loss 1.9369794921875\n",
      "Epoch 497 : test_loss 1.93289892578125\n",
      "Epoch 498 : train_loss 1.9464752943250867\n",
      "Epoch 498 : test_loss 1.9414435424804688\n",
      "Epoch 499 : train_loss 1.9459990709092883\n",
      "Epoch 499 : test_loss 1.9615528564453124\n",
      "Epoch 500 : train_loss 1.945094740125868\n",
      "Epoch 500 : test_loss 1.9503902587890625\n",
      "Epoch 501 : train_loss 1.9509980943467882\n",
      "Epoch 501 : test_loss 1.95053125\n",
      "Epoch 502 : train_loss 1.9412323201497397\n",
      "Epoch 502 : test_loss 1.9292337646484374\n",
      "Epoch 503 : train_loss 1.9519328816731771\n",
      "Epoch 503 : test_loss 1.9120951538085937\n",
      "Epoch 504 : train_loss 1.951152838812934\n",
      "Epoch 504 : test_loss 1.929089599609375\n",
      "Epoch 505 : train_loss 1.9440027601453993\n",
      "Epoch 505 : test_loss 1.9382455444335938\n",
      "Epoch 506 : train_loss 1.9449225192599826\n",
      "Epoch 506 : test_loss 1.964878173828125\n",
      "Epoch 507 : train_loss 1.9484145711263021\n",
      "Epoch 507 : test_loss 1.949462158203125\n",
      "Epoch 508 : train_loss 1.9450287746853299\n",
      "Epoch 508 : test_loss 1.9345892333984376\n",
      "Epoch 509 : train_loss 1.9446819661458334\n",
      "Epoch 509 : test_loss 1.9741546630859375\n",
      "Epoch 510 : train_loss 1.950060526529948\n",
      "Epoch 510 : test_loss 1.9419161376953125\n",
      "Epoch 511 : train_loss 1.9430958048502605\n",
      "Epoch 511 : test_loss 1.9446257934570312\n",
      "Epoch 512 : train_loss 1.9442743123372397\n",
      "Epoch 512 : test_loss 1.9659970703125\n",
      "Epoch 513 : train_loss 1.9424264865451388\n",
      "Epoch 513 : test_loss 1.9381368408203126\n",
      "Epoch 514 : train_loss 1.9413564317491319\n",
      "Epoch 514 : test_loss 1.9440150146484374\n",
      "Epoch 515 : train_loss 1.9453241441514757\n",
      "Epoch 515 : test_loss 1.9719468994140625\n",
      "Epoch 516 : train_loss 1.9380753241644966\n",
      "Epoch 516 : test_loss 1.9370870361328125\n",
      "Epoch 517 : train_loss 1.9423880547417536\n",
      "Epoch 517 : test_loss 1.9325466918945313\n",
      "Epoch 518 : train_loss 1.946464111328125\n",
      "Epoch 518 : test_loss 1.965529541015625\n",
      "Epoch 519 : train_loss 1.9405745713975695\n",
      "Epoch 519 : test_loss 1.9570280151367188\n",
      "Epoch 520 : train_loss 1.952915757921007\n",
      "Epoch 520 : test_loss 1.9896329956054688\n",
      "Epoch 521 : train_loss 1.9474372016059027\n",
      "Epoch 521 : test_loss 1.9271978149414062\n",
      "Epoch 522 : train_loss 1.947281500922309\n",
      "Epoch 522 : test_loss 1.9272178344726563\n",
      "Epoch 523 : train_loss 1.9475604858398436\n",
      "Epoch 523 : test_loss 1.949270263671875\n",
      "Epoch 524 : train_loss 1.9477689615885416\n",
      "Epoch 524 : test_loss 1.9433189697265625\n",
      "Epoch 525 : train_loss 1.9411719902886284\n",
      "Epoch 525 : test_loss 1.9641279907226563\n",
      "Epoch 526 : train_loss 1.939219957139757\n",
      "Epoch 526 : test_loss 1.9828777465820313\n",
      "Epoch 527 : train_loss 1.949476060655382\n",
      "Epoch 527 : test_loss 1.987573974609375\n",
      "Epoch 528 : train_loss 1.9386975979275174\n",
      "Epoch 528 : test_loss 1.968626220703125\n",
      "Epoch 529 : train_loss 1.946754150390625\n",
      "Epoch 529 : test_loss 1.9636813354492189\n",
      "Epoch 530 : train_loss 1.9419493069118923\n",
      "Epoch 530 : test_loss 1.968896484375\n",
      "Epoch 531 : train_loss 1.9393408949110242\n",
      "Epoch 531 : test_loss 1.9696702880859376\n",
      "Epoch 532 : train_loss 1.9486595730251737\n",
      "Epoch 532 : test_loss 1.96559375\n",
      "Epoch 533 : train_loss 1.9401167534722221\n",
      "Epoch 533 : test_loss 1.9713115234375\n",
      "Epoch 534 : train_loss 1.9465992702907986\n",
      "Epoch 534 : test_loss 1.9398162841796875\n",
      "Epoch 535 : train_loss 1.94304000515408\n",
      "Epoch 535 : test_loss 1.9785467529296874\n",
      "Epoch 536 : train_loss 1.9510881754557292\n",
      "Epoch 536 : test_loss 1.9441015625\n",
      "Epoch 537 : train_loss 1.9448147447374131\n",
      "Epoch 537 : test_loss 1.950538818359375\n",
      "Epoch 538 : train_loss 1.9486363796657986\n",
      "Epoch 538 : test_loss 1.9599765625\n",
      "Epoch 539 : train_loss 1.9519660034179687\n",
      "Epoch 539 : test_loss 1.9283892822265625\n",
      "Epoch 540 : train_loss 1.9503883260091146\n",
      "Epoch 540 : test_loss 1.944138916015625\n",
      "Epoch 541 : train_loss 1.9495438028971355\n",
      "Epoch 541 : test_loss 1.9533402099609376\n",
      "Epoch 542 : train_loss 1.9512153252495659\n",
      "Epoch 542 : test_loss 1.9361144409179687\n",
      "Epoch 543 : train_loss 1.9470992160373264\n",
      "Epoch 543 : test_loss 1.9796815795898437\n",
      "Epoch 544 : train_loss 1.9484453667534722\n",
      "Epoch 544 : test_loss 1.9682294311523438\n",
      "Epoch 545 : train_loss 1.9379426710340713\n",
      "Epoch 545 : test_loss 1.979284912109375\n",
      "Epoch 546 : train_loss 1.9525729302300348\n",
      "Epoch 546 : test_loss 1.9115274658203125\n",
      "Epoch 547 : train_loss 1.947659417046441\n",
      "Epoch 547 : test_loss 1.947827392578125\n",
      "Epoch 548 : train_loss 1.9572915581597221\n",
      "Epoch 548 : test_loss 1.9368050537109376\n",
      "Epoch 549 : train_loss 1.9477273763020833\n",
      "Epoch 549 : test_loss 1.9485859375\n",
      "Epoch 550 : train_loss 1.9484480251736112\n",
      "Epoch 550 : test_loss 1.9331937255859375\n",
      "Epoch 551 : train_loss 1.9544663425021702\n",
      "Epoch 551 : test_loss 1.9747903442382813\n",
      "Epoch 552 : train_loss 1.9393117336697048\n",
      "Epoch 552 : test_loss 1.9223069458007813\n",
      "Epoch 553 : train_loss 1.9410827975802951\n",
      "Epoch 553 : test_loss 1.9929074096679686\n",
      "Epoch 554 : train_loss 1.9495786878797743\n",
      "Epoch 554 : test_loss 1.94422216796875\n",
      "Epoch 555 : train_loss 1.949678419325087\n",
      "Epoch 555 : test_loss 1.9225975341796875\n",
      "Epoch 556 : train_loss 1.9475439927842881\n",
      "Epoch 556 : test_loss 1.960062744140625\n",
      "Epoch 557 : train_loss 1.9482091946072049\n",
      "Epoch 557 : test_loss 1.9279171752929687\n",
      "saved model\n",
      "Epoch 558 : train_loss 1.9377332424587674\n",
      "Epoch 558 : test_loss 1.9102135620117187\n",
      "Epoch 559 : train_loss 1.9492820298936633\n",
      "Epoch 559 : test_loss 1.9733421630859376\n",
      "Epoch 560 : train_loss 1.9568732367621529\n",
      "Epoch 560 : test_loss 1.9487696533203125\n",
      "saved model\n",
      "Epoch 561 : train_loss 1.944995395236545\n",
      "Epoch 561 : test_loss 1.9077800903320312\n",
      "Epoch 562 : train_loss 1.9425269097222222\n",
      "Epoch 562 : test_loss 1.9683470458984376\n",
      "Epoch 563 : train_loss 1.9465703599717883\n",
      "Epoch 563 : test_loss 1.940929931640625\n",
      "Epoch 564 : train_loss 1.950110846625434\n",
      "Epoch 564 : test_loss 1.9614675903320313\n",
      "Epoch 565 : train_loss 1.9425866292317708\n",
      "Epoch 565 : test_loss 1.931941650390625\n",
      "Epoch 566 : train_loss 1.9469864569769966\n",
      "Epoch 566 : test_loss 1.9327374877929688\n",
      "Epoch 567 : train_loss 1.9470392116970485\n",
      "Epoch 567 : test_loss 1.9203731689453125\n",
      "Epoch 568 : train_loss 1.9429344957139756\n",
      "Epoch 568 : test_loss 1.9677171630859376\n",
      "Epoch 569 : train_loss 1.9453077663845486\n",
      "Epoch 569 : test_loss 1.921912109375\n",
      "Epoch 570 : train_loss 1.9482330796983507\n",
      "Epoch 570 : test_loss 1.9180922241210938\n",
      "Epoch 571 : train_loss 1.941799031575521\n",
      "Epoch 571 : test_loss 1.960784912109375\n",
      "Epoch 572 : train_loss 1.9502491658528647\n",
      "Epoch 572 : test_loss 1.9342032470703125\n",
      "Epoch 573 : train_loss 1.951195061577691\n",
      "Epoch 573 : test_loss 1.99039306640625\n",
      "Epoch 574 : train_loss 1.9437793375651042\n",
      "Epoch 574 : test_loss 1.9495228271484375\n",
      "Epoch 575 : train_loss 1.9541624077690973\n",
      "Epoch 575 : test_loss 1.921485107421875\n",
      "Epoch 576 : train_loss 1.9418536309136285\n",
      "Epoch 576 : test_loss 1.9339630126953125\n",
      "saved model\n",
      "Epoch 577 : train_loss 1.9483997395833332\n",
      "Epoch 577 : test_loss 1.9068507080078125\n",
      "Epoch 578 : train_loss 1.9472687038845486\n",
      "Epoch 578 : test_loss 1.9491362915039063\n",
      "Epoch 579 : train_loss 1.9420428534613716\n",
      "Epoch 579 : test_loss 1.9542516479492187\n",
      "Epoch 580 : train_loss 1.9404124755859375\n",
      "Epoch 580 : test_loss 1.949925048828125\n",
      "Epoch 581 : train_loss 1.942019788953993\n",
      "Epoch 581 : test_loss 1.9486058959960937\n",
      "Epoch 582 : train_loss 1.9485222778320312\n",
      "Epoch 582 : test_loss 1.9408626098632813\n",
      "Epoch 583 : train_loss 1.9457915547688802\n",
      "Epoch 583 : test_loss 1.9111998291015626\n",
      "Epoch 584 : train_loss 1.9425299682617188\n",
      "Epoch 584 : test_loss 1.9163491821289063\n",
      "Epoch 585 : train_loss 1.9467571411132814\n",
      "Epoch 585 : test_loss 1.9455015869140626\n",
      "Epoch 586 : train_loss 1.9434062025282117\n",
      "Epoch 586 : test_loss 1.9603560180664064\n",
      "saved model\n",
      "Epoch 587 : train_loss 1.9458650716145833\n",
      "Epoch 587 : test_loss 1.8864169311523438\n",
      "Epoch 588 : train_loss 1.947105177137587\n",
      "Epoch 588 : test_loss 1.925909912109375\n",
      "Epoch 589 : train_loss 1.9394569295247397\n",
      "Epoch 589 : test_loss 1.97882373046875\n",
      "Epoch 590 : train_loss 1.9398837483723959\n",
      "Epoch 590 : test_loss 1.9202393798828126\n",
      "Epoch 591 : train_loss 1.943079840766059\n",
      "Epoch 591 : test_loss 1.9389972534179687\n",
      "Epoch 592 : train_loss 1.9473886176215278\n",
      "Epoch 592 : test_loss 1.9776065673828125\n",
      "Epoch 593 : train_loss 1.9435606350368924\n",
      "Epoch 593 : test_loss 1.9648179321289063\n",
      "Epoch 594 : train_loss 1.939625969780816\n",
      "Epoch 594 : test_loss 1.9114927978515626\n",
      "Epoch 595 : train_loss 1.9396248372395832\n",
      "Epoch 595 : test_loss 1.938647216796875\n",
      "Epoch 596 : train_loss 1.9386070556640624\n",
      "Epoch 596 : test_loss 1.9316319580078125\n",
      "Epoch 597 : train_loss 1.94668997531467\n",
      "Epoch 597 : test_loss 1.9087054443359375\n",
      "Epoch 598 : train_loss 1.94346484375\n",
      "Epoch 598 : test_loss 1.9447871704101563\n",
      "Epoch 599 : train_loss 1.9461422865125868\n",
      "Epoch 599 : test_loss 1.938526123046875\n",
      "Epoch 600 : train_loss 1.94044970703125\n",
      "Epoch 600 : test_loss 1.9556671142578126\n",
      "Epoch 601 : train_loss 1.949285895453559\n",
      "Epoch 601 : test_loss 1.938477783203125\n",
      "Epoch 602 : train_loss 1.9387470431857639\n",
      "Epoch 602 : test_loss 1.964111328125\n",
      "Epoch 603 : train_loss 1.9502874959309895\n",
      "Epoch 603 : test_loss 1.9601796875\n",
      "Epoch 604 : train_loss 1.9435586547851562\n",
      "Epoch 604 : test_loss 1.976031494140625\n",
      "Epoch 605 : train_loss 1.950474894205729\n",
      "Epoch 605 : test_loss 1.9547620849609375\n",
      "Epoch 606 : train_loss 1.9442192993164062\n",
      "Epoch 606 : test_loss 1.9402545166015626\n",
      "Epoch 607 : train_loss 1.9355750935872396\n",
      "Epoch 607 : test_loss 1.9562088623046876\n",
      "Epoch 608 : train_loss 1.942363301595052\n",
      "Epoch 608 : test_loss 1.9377092895507813\n",
      "Epoch 609 : train_loss 1.9397380710177952\n",
      "Epoch 609 : test_loss 1.9175422973632812\n",
      "Epoch 610 : train_loss 1.946026095920139\n",
      "Epoch 610 : test_loss 1.976761474609375\n",
      "Epoch 611 : train_loss 1.9393683607313368\n",
      "Epoch 611 : test_loss 1.936998046875\n",
      "Epoch 612 : train_loss 1.9372243109809029\n",
      "Epoch 612 : test_loss 1.9551079711914063\n",
      "Epoch 613 : train_loss 1.9445197007921007\n",
      "Epoch 613 : test_loss 1.9589474487304688\n",
      "Epoch 614 : train_loss 1.9395103963216145\n",
      "Epoch 614 : test_loss 1.9402137451171875\n",
      "Epoch 615 : train_loss 1.9406259290907117\n",
      "Epoch 615 : test_loss 1.91251171875\n",
      "Epoch 616 : train_loss 1.9384702758789063\n",
      "Epoch 616 : test_loss 1.968534912109375\n",
      "Epoch 617 : train_loss 1.9369170328776042\n",
      "Epoch 617 : test_loss 1.968248779296875\n",
      "Epoch 618 : train_loss 1.9491240912543404\n",
      "Epoch 618 : test_loss 1.9266006469726562\n",
      "Epoch 619 : train_loss 1.9431072319878473\n",
      "Epoch 619 : test_loss 1.9463077392578125\n",
      "Epoch 620 : train_loss 1.9417506646050346\n",
      "Epoch 620 : test_loss 1.9613084716796876\n",
      "Epoch 621 : train_loss 1.9472444729275173\n",
      "Epoch 621 : test_loss 1.94520361328125\n",
      "Epoch 622 : train_loss 1.9345418429904513\n",
      "Epoch 622 : test_loss 1.9064431762695313\n",
      "Epoch 623 : train_loss 1.940079060872396\n",
      "Epoch 623 : test_loss 1.9525655517578124\n",
      "Epoch 624 : train_loss 1.948210205078125\n",
      "Epoch 624 : test_loss 1.9123668212890625\n",
      "Epoch 625 : train_loss 1.9458907267252603\n",
      "Epoch 625 : test_loss 1.9647083129882812\n",
      "Epoch 626 : train_loss 1.9435241021050347\n",
      "Epoch 626 : test_loss 1.9365034790039062\n",
      "Epoch 627 : train_loss 1.9360415785047742\n",
      "Epoch 627 : test_loss 1.9091107177734374\n",
      "Epoch 628 : train_loss 1.9410111897786457\n",
      "Epoch 628 : test_loss 1.913840576171875\n",
      "Epoch 629 : train_loss 1.9399908989800347\n",
      "Epoch 629 : test_loss 1.9196552124023438\n",
      "Epoch 630 : train_loss 1.9469955783420139\n",
      "Epoch 630 : test_loss 1.906302001953125\n",
      "Epoch 631 : train_loss 1.9400438368055555\n",
      "Epoch 631 : test_loss 1.9656104736328126\n",
      "Epoch 632 : train_loss 1.946863762749566\n",
      "Epoch 632 : test_loss 2.0001455688476564\n",
      "Epoch 633 : train_loss 1.9472001546223958\n",
      "Epoch 633 : test_loss 1.924985107421875\n",
      "Epoch 634 : train_loss 1.937511969672309\n",
      "Epoch 634 : test_loss 1.9903365478515624\n",
      "Epoch 635 : train_loss 1.9344898817274305\n",
      "Epoch 635 : test_loss 1.93561865234375\n",
      "Epoch 636 : train_loss 1.9482622816297743\n",
      "Epoch 636 : test_loss 1.9760928344726563\n",
      "Epoch 637 : train_loss 1.947412353515625\n",
      "Epoch 637 : test_loss 1.914003662109375\n",
      "Epoch 638 : train_loss 1.9402069363064236\n",
      "Epoch 638 : test_loss 1.977655517578125\n",
      "Epoch 639 : train_loss 1.9411029730902778\n",
      "Epoch 639 : test_loss 1.9053167114257812\n",
      "Epoch 640 : train_loss 1.9380860934787327\n",
      "Epoch 640 : test_loss 1.91293212890625\n",
      "Epoch 641 : train_loss 1.9356682942708334\n",
      "Epoch 641 : test_loss 1.9487786865234376\n",
      "Epoch 642 : train_loss 1.9374937065972222\n",
      "Epoch 642 : test_loss 1.9313092041015625\n",
      "Epoch 643 : train_loss 1.937727281358507\n",
      "Epoch 643 : test_loss 1.914779296875\n",
      "Epoch 644 : train_loss 1.9353055962456598\n",
      "Epoch 644 : test_loss 1.9639827880859375\n",
      "Epoch 645 : train_loss 1.9441500447591147\n",
      "Epoch 645 : test_loss 1.9306904907226563\n",
      "Epoch 646 : train_loss 1.9384950493706596\n",
      "Epoch 646 : test_loss 1.9409432373046875\n",
      "Epoch 647 : train_loss 1.9411762017144096\n",
      "Epoch 647 : test_loss 1.931719482421875\n",
      "Epoch 648 : train_loss 1.944570041232639\n",
      "Epoch 648 : test_loss 1.9163172607421874\n",
      "Epoch 649 : train_loss 1.9385844048394096\n",
      "Epoch 649 : test_loss 1.94803271484375\n",
      "Epoch 650 : train_loss 1.9342456868489584\n",
      "Epoch 650 : test_loss 1.923364990234375\n",
      "Epoch 651 : train_loss 1.9384634704589845\n",
      "Epoch 651 : test_loss 1.9325531005859375\n",
      "Epoch 652 : train_loss 1.9409929741753473\n",
      "Epoch 652 : test_loss 1.9305028076171875\n",
      "Epoch 653 : train_loss 1.9496605902777777\n",
      "Epoch 653 : test_loss 1.9519821166992188\n",
      "Epoch 654 : train_loss 1.9478140801323784\n",
      "Epoch 654 : test_loss 1.9441678466796875\n",
      "Epoch 655 : train_loss 1.9357516682942708\n",
      "Epoch 655 : test_loss 1.9392449340820312\n",
      "Epoch 656 : train_loss 1.9432026095920139\n",
      "Epoch 656 : test_loss 1.9139552001953124\n",
      "Epoch 657 : train_loss 1.9410015733506945\n",
      "Epoch 657 : test_loss 1.9446030883789063\n",
      "Epoch 658 : train_loss 1.9392398952907985\n",
      "Epoch 658 : test_loss 1.9658350830078124\n",
      "Epoch 659 : train_loss 1.9456334228515626\n",
      "Epoch 659 : test_loss 1.9263381958007812\n",
      "Epoch 660 : train_loss 1.9419051242404515\n",
      "Epoch 660 : test_loss 1.9554239501953126\n",
      "Epoch 661 : train_loss 1.9399195081922742\n",
      "Epoch 661 : test_loss 1.9259855346679688\n",
      "Epoch 662 : train_loss 1.9415749986436632\n",
      "Epoch 662 : test_loss 1.9166043090820313\n",
      "Epoch 663 : train_loss 1.9433360934787327\n",
      "Epoch 663 : test_loss 1.9290760498046875\n",
      "Epoch 664 : train_loss 1.9373434176974826\n",
      "Epoch 664 : test_loss 1.9810054931640626\n",
      "Epoch 665 : train_loss 1.9430791490342882\n",
      "Epoch 665 : test_loss 1.9298629150390625\n",
      "Epoch 666 : train_loss 1.9388910658094618\n",
      "Epoch 666 : test_loss 1.9188311157226563\n",
      "Epoch 667 : train_loss 1.941541002061632\n",
      "Epoch 667 : test_loss 1.903010986328125\n",
      "Epoch 668 : train_loss 1.9464695977105035\n",
      "Epoch 668 : test_loss 1.9273478393554688\n",
      "Epoch 669 : train_loss 1.9370091078016494\n",
      "Epoch 669 : test_loss 1.9464910888671876\n",
      "Epoch 670 : train_loss 1.9424723239474826\n",
      "Epoch 670 : test_loss 1.9457028198242188\n",
      "Epoch 671 : train_loss 1.9375839911566841\n",
      "Epoch 671 : test_loss 1.9354588623046876\n",
      "Epoch 672 : train_loss 1.937691616482205\n",
      "Epoch 672 : test_loss 1.9479671630859374\n",
      "Epoch 673 : train_loss 1.9457658962673612\n",
      "Epoch 673 : test_loss 1.9271865234375\n",
      "Epoch 674 : train_loss 1.9407105034722223\n",
      "Epoch 674 : test_loss 1.9544656982421875\n",
      "Epoch 675 : train_loss 1.9354706556532117\n",
      "Epoch 675 : test_loss 1.9187936401367187\n",
      "Epoch 676 : train_loss 1.9399991183810763\n",
      "Epoch 676 : test_loss 1.9613001708984374\n",
      "Epoch 677 : train_loss 1.9353683810763889\n",
      "Epoch 677 : test_loss 1.9471956787109375\n",
      "Epoch 678 : train_loss 1.9396878729926215\n",
      "Epoch 678 : test_loss 1.9250823364257812\n",
      "Epoch 679 : train_loss 1.9386746080186632\n",
      "Epoch 679 : test_loss 1.9676260375976562\n",
      "Epoch 680 : train_loss 1.948653591579861\n",
      "Epoch 680 : test_loss 1.9323985595703126\n",
      "Epoch 681 : train_loss 1.935237318250868\n",
      "Epoch 681 : test_loss 1.941276123046875\n",
      "Epoch 682 : train_loss 1.9431128607855903\n",
      "Epoch 682 : test_loss 1.9443082885742187\n",
      "Epoch 683 : train_loss 1.9383802761501736\n",
      "Epoch 683 : test_loss 1.9641881103515626\n",
      "Epoch 684 : train_loss 1.9321384616427952\n",
      "Epoch 684 : test_loss 1.9515975341796874\n",
      "Epoch 685 : train_loss 1.9509853108723958\n",
      "Epoch 685 : test_loss 1.945921630859375\n",
      "Epoch 686 : train_loss 1.944208014594184\n",
      "Epoch 686 : test_loss 1.92864599609375\n",
      "Epoch 687 : train_loss 1.937338148328993\n",
      "Epoch 687 : test_loss 1.9148905639648437\n",
      "Epoch 688 : train_loss 1.9394942152235244\n",
      "Epoch 688 : test_loss 1.9290172119140625\n",
      "Epoch 689 : train_loss 1.9373566080729168\n",
      "Epoch 689 : test_loss 1.9454854736328124\n",
      "Epoch 690 : train_loss 1.9383355238172744\n",
      "Epoch 690 : test_loss 1.9315650634765624\n",
      "Epoch 691 : train_loss 1.9481050889756943\n",
      "Epoch 691 : test_loss 1.9111204833984374\n",
      "Epoch 692 : train_loss 1.9491507636176215\n",
      "Epoch 692 : test_loss 1.9440963745117188\n",
      "Epoch 693 : train_loss 1.9421911146375868\n",
      "Epoch 693 : test_loss 1.9462701416015624\n",
      "Epoch 694 : train_loss 1.938434543185764\n",
      "Epoch 694 : test_loss 1.9409019775390626\n",
      "Epoch 695 : train_loss 1.9394244452582465\n",
      "Epoch 695 : test_loss 1.9220006713867188\n",
      "Epoch 696 : train_loss 1.943289055718316\n",
      "Epoch 696 : test_loss 1.9191761474609375\n",
      "Epoch 697 : train_loss 1.936017320421007\n",
      "Epoch 697 : test_loss 1.9602327270507813\n",
      "Epoch 698 : train_loss 1.9355518866644965\n",
      "Epoch 698 : test_loss 1.93917626953125\n",
      "Epoch 699 : train_loss 1.9411606716579861\n",
      "Epoch 699 : test_loss 1.9483086547851562\n",
      "Epoch 700 : train_loss 1.9370150892469618\n",
      "Epoch 700 : test_loss 1.9487241821289063\n",
      "Epoch 701 : train_loss 1.9322635091145834\n",
      "Epoch 701 : test_loss 1.96565380859375\n",
      "Epoch 702 : train_loss 1.937793436686198\n",
      "Epoch 702 : test_loss 1.964244384765625\n",
      "Epoch 703 : train_loss 1.9399306165907118\n",
      "Epoch 703 : test_loss 1.9225785522460936\n",
      "Epoch 704 : train_loss 1.9418130967881944\n",
      "Epoch 704 : test_loss 1.9706578369140626\n",
      "Epoch 705 : train_loss 1.937590799967448\n",
      "Epoch 705 : test_loss 1.9397139892578126\n",
      "Epoch 706 : train_loss 1.937399173312717\n",
      "Epoch 706 : test_loss 1.94339794921875\n",
      "Epoch 707 : train_loss 1.936287333170573\n",
      "Epoch 707 : test_loss 1.9452220458984375\n",
      "Epoch 708 : train_loss 1.9431357557508682\n",
      "Epoch 708 : test_loss 1.9599228515625\n",
      "Epoch 709 : train_loss 1.9442577446831597\n",
      "Epoch 709 : test_loss 1.9089114990234375\n",
      "Epoch 710 : train_loss 1.9375865546332465\n",
      "Epoch 710 : test_loss 1.9424576416015624\n",
      "Epoch 711 : train_loss 1.9366522081163196\n",
      "Epoch 711 : test_loss 1.9429869995117188\n",
      "Epoch 712 : train_loss 1.9343893093532987\n",
      "Epoch 712 : test_loss 1.9138655395507813\n",
      "Epoch 713 : train_loss 1.932639906141493\n",
      "Epoch 713 : test_loss 1.9589083251953125\n",
      "Epoch 714 : train_loss 1.9335765109592014\n",
      "Epoch 714 : test_loss 1.9383358154296875\n",
      "Epoch 715 : train_loss 1.9326726684570312\n",
      "Epoch 715 : test_loss 1.9523948364257813\n",
      "Epoch 716 : train_loss 1.9302599622938368\n",
      "Epoch 716 : test_loss 1.937366943359375\n",
      "Epoch 717 : train_loss 1.9370426093207465\n",
      "Epoch 717 : test_loss 1.9179141845703125\n",
      "Epoch 718 : train_loss 1.9359790513780382\n",
      "Epoch 718 : test_loss 1.9450086669921876\n",
      "Epoch 719 : train_loss 1.9337171630859376\n",
      "Epoch 719 : test_loss 1.9292844848632813\n",
      "Epoch 720 : train_loss 1.938755859375\n",
      "Epoch 720 : test_loss 1.9273182983398438\n",
      "Epoch 721 : train_loss 1.9388472629123263\n",
      "Epoch 721 : test_loss 1.9632230834960938\n",
      "Epoch 722 : train_loss 1.9323221028645834\n",
      "Epoch 722 : test_loss 1.9128157958984375\n",
      "Epoch 723 : train_loss 1.931524149576823\n",
      "Epoch 723 : test_loss 1.9387454833984374\n",
      "Epoch 724 : train_loss 1.9376603088378905\n",
      "Epoch 724 : test_loss 1.908561767578125\n",
      "Epoch 725 : train_loss 1.9358616943359375\n",
      "Epoch 725 : test_loss 1.9323818969726563\n",
      "Epoch 726 : train_loss 1.9445293714735243\n",
      "Epoch 726 : test_loss 1.9364874877929688\n",
      "Epoch 727 : train_loss 1.9401493191189236\n",
      "Epoch 727 : test_loss 1.9008116455078126\n",
      "Epoch 728 : train_loss 1.9399373508029514\n",
      "Epoch 728 : test_loss 1.94506298828125\n",
      "Epoch 729 : train_loss 1.935075398763021\n",
      "Epoch 729 : test_loss 1.9488474731445313\n",
      "Epoch 730 : train_loss 1.9333565809461806\n",
      "Epoch 730 : test_loss 1.9283787841796876\n",
      "Epoch 731 : train_loss 1.9471057739257813\n",
      "Epoch 731 : test_loss 1.9093687744140626\n",
      "Epoch 732 : train_loss 1.9342250366210938\n",
      "Epoch 732 : test_loss 1.9458695068359375\n",
      "Epoch 733 : train_loss 1.938080844455295\n",
      "Epoch 733 : test_loss 1.9383402099609375\n",
      "Epoch 734 : train_loss 1.9334955579969617\n",
      "Epoch 734 : test_loss 1.9462845458984375\n",
      "Epoch 735 : train_loss 1.940043707953559\n",
      "Epoch 735 : test_loss 1.91971142578125\n",
      "Epoch 736 : train_loss 1.9346933017306858\n",
      "Epoch 736 : test_loss 1.9567054443359375\n",
      "Epoch 737 : train_loss 1.9361637166341146\n",
      "Epoch 737 : test_loss 1.9584763793945312\n",
      "Epoch 738 : train_loss 1.9384479370117187\n",
      "Epoch 738 : test_loss 1.9228375244140625\n",
      "Epoch 739 : train_loss 1.9383247545030382\n",
      "Epoch 739 : test_loss 1.9517745971679688\n",
      "Epoch 740 : train_loss 1.9458126763237846\n",
      "Epoch 740 : test_loss 1.929392578125\n",
      "Epoch 741 : train_loss 1.9370960015190972\n",
      "Epoch 741 : test_loss 1.93257177734375\n",
      "Epoch 742 : train_loss 1.9420988023546006\n",
      "Epoch 742 : test_loss 1.950818603515625\n",
      "Epoch 743 : train_loss 1.9266469184027777\n",
      "Epoch 743 : test_loss 1.9833919677734375\n",
      "Epoch 744 : train_loss 1.9379488661024304\n",
      "Epoch 744 : test_loss 1.9670799560546874\n",
      "Epoch 745 : train_loss 1.9290981784396701\n",
      "Epoch 745 : test_loss 1.95272314453125\n",
      "Epoch 746 : train_loss 1.9429747382269966\n",
      "Epoch 746 : test_loss 1.9308119506835937\n",
      "Epoch 747 : train_loss 1.936447519938151\n",
      "Epoch 747 : test_loss 1.9481112670898437\n",
      "Epoch 748 : train_loss 1.936418206108941\n",
      "Epoch 748 : test_loss 1.935688720703125\n",
      "Epoch 749 : train_loss 1.937446282280816\n",
      "Epoch 749 : test_loss 1.93482275390625\n",
      "Epoch 750 : train_loss 1.9331766221788194\n",
      "Epoch 750 : test_loss 1.8948941040039062\n",
      "Epoch 751 : train_loss 1.9321232231987848\n",
      "Epoch 751 : test_loss 1.9041636352539062\n",
      "Epoch 752 : train_loss 1.9347033759223091\n",
      "Epoch 752 : test_loss 1.9284990234375\n",
      "Epoch 753 : train_loss 1.9367227715386284\n",
      "Epoch 753 : test_loss 1.97430126953125\n",
      "Epoch 754 : train_loss 1.9315074259440104\n",
      "Epoch 754 : test_loss 1.93918994140625\n",
      "Epoch 755 : train_loss 1.9456262885199653\n",
      "Epoch 755 : test_loss 1.933125\n",
      "Epoch 756 : train_loss 1.9328079325358072\n",
      "Epoch 756 : test_loss 1.9443724975585936\n",
      "Epoch 757 : train_loss 1.9422295328776042\n",
      "Epoch 757 : test_loss 1.95557080078125\n",
      "Epoch 758 : train_loss 1.9342928059895834\n",
      "Epoch 758 : test_loss 1.942087890625\n",
      "Epoch 759 : train_loss 1.9300997517903646\n",
      "Epoch 759 : test_loss 1.9184291381835938\n",
      "Epoch 760 : train_loss 1.9315196533203125\n",
      "Epoch 760 : test_loss 1.941381103515625\n",
      "Epoch 761 : train_loss 1.9350638427734375\n",
      "Epoch 761 : test_loss 1.9462506713867187\n",
      "Epoch 762 : train_loss 1.9417068684895833\n",
      "Epoch 762 : test_loss 1.9245604248046875\n",
      "Epoch 763 : train_loss 1.938210951063368\n",
      "Epoch 763 : test_loss 1.96779638671875\n",
      "Epoch 764 : train_loss 1.934426506890191\n",
      "Epoch 764 : test_loss 1.9381002197265624\n",
      "Epoch 765 : train_loss 1.9328797403971354\n",
      "Epoch 765 : test_loss 1.9591688842773438\n",
      "Epoch 766 : train_loss 1.9386585625542534\n",
      "Epoch 766 : test_loss 1.965811767578125\n",
      "Epoch 767 : train_loss 1.9340029127332898\n",
      "Epoch 767 : test_loss 1.9314393310546876\n",
      "Epoch 768 : train_loss 1.9383342895507814\n",
      "Epoch 768 : test_loss 1.9275938720703125\n",
      "Epoch 769 : train_loss 1.9280321112738714\n",
      "Epoch 769 : test_loss 1.9525006103515625\n",
      "Epoch 770 : train_loss 1.9377256334092883\n",
      "Epoch 770 : test_loss 1.9252789916992188\n",
      "Epoch 771 : train_loss 1.9418709988064236\n",
      "Epoch 771 : test_loss 1.9324346923828124\n",
      "Epoch 772 : train_loss 1.9309648844401042\n",
      "Epoch 772 : test_loss 1.929099365234375\n",
      "Epoch 773 : train_loss 1.9358204210069445\n",
      "Epoch 773 : test_loss 1.9198318481445313\n",
      "Epoch 774 : train_loss 1.9441974555121528\n",
      "Epoch 774 : test_loss 1.9190811767578124\n",
      "Epoch 775 : train_loss 1.940823031955295\n",
      "Epoch 775 : test_loss 1.9115460205078125\n",
      "Epoch 776 : train_loss 1.9391892971462674\n",
      "Epoch 776 : test_loss 1.940974609375\n",
      "Epoch 777 : train_loss 1.9348849690755208\n",
      "Epoch 777 : test_loss 1.9222196044921875\n",
      "Epoch 778 : train_loss 1.9379974161783855\n",
      "Epoch 778 : test_loss 1.924315673828125\n",
      "Epoch 779 : train_loss 1.9346780192057291\n",
      "Epoch 779 : test_loss 1.923400390625\n",
      "Epoch 780 : train_loss 1.9381373155381945\n",
      "Epoch 780 : test_loss 1.9198494262695311\n",
      "Epoch 781 : train_loss 1.927980716281467\n",
      "Epoch 781 : test_loss 1.9458032836914063\n",
      "Epoch 782 : train_loss 1.936615227593316\n",
      "Epoch 782 : test_loss 1.9262521362304688\n",
      "Epoch 783 : train_loss 1.9284922892252605\n",
      "Epoch 783 : test_loss 1.94990625\n",
      "Epoch 784 : train_loss 1.9342078179253472\n",
      "Epoch 784 : test_loss 1.947543701171875\n",
      "Epoch 785 : train_loss 1.932091539171007\n",
      "Epoch 785 : test_loss 1.90781396484375\n",
      "Epoch 786 : train_loss 1.932984585232205\n",
      "Epoch 786 : test_loss 1.933920166015625\n",
      "Epoch 787 : train_loss 1.935025349934896\n",
      "Epoch 787 : test_loss 1.9113628540039063\n",
      "Epoch 788 : train_loss 1.9372697550455729\n",
      "Epoch 788 : test_loss 1.945763916015625\n",
      "Epoch 789 : train_loss 1.9312735968695747\n",
      "Epoch 789 : test_loss 1.9458822021484374\n",
      "Epoch 790 : train_loss 1.9309497748480904\n",
      "Epoch 790 : test_loss 1.90971728515625\n",
      "Epoch 791 : train_loss 1.9350863172743056\n",
      "Epoch 791 : test_loss 1.9270694580078125\n",
      "Epoch 792 : train_loss 1.9284210069444445\n",
      "Epoch 792 : test_loss 1.8994696044921875\n",
      "Epoch 793 : train_loss 1.9378798285590277\n",
      "Epoch 793 : test_loss 1.9182183227539062\n",
      "Epoch 794 : train_loss 1.9412083808051215\n",
      "Epoch 794 : test_loss 1.9431395263671876\n",
      "Epoch 795 : train_loss 1.9308246256510417\n",
      "Epoch 795 : test_loss 1.9195767822265626\n",
      "Epoch 796 : train_loss 1.933224344889323\n",
      "Epoch 796 : test_loss 1.942203125\n",
      "Epoch 797 : train_loss 1.9436540934244793\n",
      "Epoch 797 : test_loss 1.9301806640625\n",
      "Epoch 798 : train_loss 1.9305513576931423\n",
      "Epoch 798 : test_loss 1.9028267822265625\n",
      "Epoch 799 : train_loss 1.9312799614800347\n",
      "Epoch 799 : test_loss 1.9490123291015624\n",
      "Epoch 800 : train_loss 1.9340666436089409\n",
      "Epoch 800 : test_loss 1.9612960205078125\n",
      "Epoch 801 : train_loss 1.9309515109592015\n",
      "Epoch 801 : test_loss 1.9458109741210938\n",
      "Epoch 802 : train_loss 1.925685282389323\n",
      "Epoch 802 : test_loss 1.9415365600585937\n",
      "Epoch 803 : train_loss 1.9331441311306423\n",
      "Epoch 803 : test_loss 1.9449534912109374\n",
      "Epoch 804 : train_loss 1.9382688598632813\n",
      "Epoch 804 : test_loss 1.9476370239257812\n",
      "Epoch 805 : train_loss 1.9414001803927952\n",
      "Epoch 805 : test_loss 1.93520068359375\n",
      "Epoch 806 : train_loss 1.928509738498264\n",
      "Epoch 806 : test_loss 1.9246700439453126\n",
      "Epoch 807 : train_loss 1.9292708265516494\n",
      "Epoch 807 : test_loss 1.93269775390625\n",
      "Epoch 808 : train_loss 1.9384153238932291\n",
      "Epoch 808 : test_loss 1.9263361206054688\n",
      "Epoch 809 : train_loss 1.927753431532118\n",
      "Epoch 809 : test_loss 1.947612548828125\n",
      "Epoch 810 : train_loss 1.9325736355251737\n",
      "Epoch 810 : test_loss 1.918040283203125\n",
      "Epoch 811 : train_loss 1.9359678615993923\n",
      "Epoch 811 : test_loss 1.9609404296875\n",
      "Epoch 812 : train_loss 1.9258028361002604\n",
      "Epoch 812 : test_loss 1.9417921142578125\n",
      "Epoch 813 : train_loss 1.9399048326280381\n",
      "Epoch 813 : test_loss 1.922614501953125\n",
      "Epoch 814 : train_loss 1.9354507921006945\n",
      "Epoch 814 : test_loss 1.9255781860351562\n",
      "Epoch 815 : train_loss 1.935395284016927\n",
      "Epoch 815 : test_loss 1.9609761352539063\n",
      "Epoch 816 : train_loss 1.9317179158528646\n",
      "Epoch 816 : test_loss 1.9346949462890626\n",
      "Epoch 817 : train_loss 1.9325418497721354\n",
      "Epoch 817 : test_loss 1.9484988403320314\n",
      "Epoch 818 : train_loss 1.9321975708007812\n",
      "Epoch 818 : test_loss 1.9299508666992187\n",
      "Epoch 819 : train_loss 1.9345362141927083\n",
      "Epoch 819 : test_loss 1.9328397216796875\n",
      "Epoch 820 : train_loss 1.9265779486762153\n",
      "Epoch 820 : test_loss 1.9377064208984376\n",
      "Epoch 821 : train_loss 1.9286305881076389\n",
      "Epoch 821 : test_loss 1.9444957275390624\n",
      "Epoch 822 : train_loss 1.9339794311523437\n",
      "Epoch 822 : test_loss 1.9381643676757812\n",
      "Epoch 823 : train_loss 1.938438991970486\n",
      "Epoch 823 : test_loss 1.947717041015625\n",
      "Epoch 824 : train_loss 1.9381237318250868\n",
      "Epoch 824 : test_loss 1.9401173706054688\n",
      "Epoch 825 : train_loss 1.9267691379123264\n",
      "Epoch 825 : test_loss 1.9303431396484374\n",
      "Epoch 826 : train_loss 1.9296624009874133\n",
      "Epoch 826 : test_loss 1.9757342529296875\n",
      "Epoch 827 : train_loss 1.9315437282986112\n",
      "Epoch 827 : test_loss 1.9249498291015625\n",
      "Epoch 828 : train_loss 1.9441297234429253\n",
      "Epoch 828 : test_loss 1.9503564453125\n",
      "Epoch 829 : train_loss 1.9349061754014758\n",
      "Epoch 829 : test_loss 1.9237239990234376\n",
      "Epoch 830 : train_loss 1.930409905327691\n",
      "Epoch 830 : test_loss 1.9260205078125\n",
      "Epoch 831 : train_loss 1.925435824924045\n",
      "Epoch 831 : test_loss 1.94046826171875\n",
      "Epoch 832 : train_loss 1.9360750800238715\n",
      "Epoch 832 : test_loss 1.9385036010742187\n",
      "Epoch 833 : train_loss 1.931344712999132\n",
      "Epoch 833 : test_loss 1.913982177734375\n",
      "Epoch 834 : train_loss 1.9320516221788195\n",
      "Epoch 834 : test_loss 1.9583427734375\n",
      "Epoch 835 : train_loss 1.932384033203125\n",
      "Epoch 835 : test_loss 1.948726806640625\n",
      "Epoch 836 : train_loss 1.9363873494466146\n",
      "Epoch 836 : test_loss 1.9334542236328125\n",
      "Epoch 837 : train_loss 1.9326153564453126\n",
      "Epoch 837 : test_loss 1.9119654541015625\n",
      "Epoch 838 : train_loss 1.933011942545573\n",
      "Epoch 838 : test_loss 1.9343287353515626\n",
      "Epoch 839 : train_loss 1.936503200954861\n",
      "Epoch 839 : test_loss 1.9223471069335938\n",
      "Epoch 840 : train_loss 1.926893836127387\n",
      "Epoch 840 : test_loss 1.910793212890625\n",
      "Epoch 841 : train_loss 1.9382929280598957\n",
      "Epoch 841 : test_loss 1.9036422119140626\n",
      "Epoch 842 : train_loss 1.9348156263563367\n",
      "Epoch 842 : test_loss 1.94356689453125\n",
      "Epoch 843 : train_loss 1.936659193250868\n",
      "Epoch 843 : test_loss 1.9635609741210938\n",
      "Epoch 844 : train_loss 1.9327767673068577\n",
      "Epoch 844 : test_loss 1.9213028564453125\n",
      "Epoch 845 : train_loss 1.9330160590277778\n",
      "Epoch 845 : test_loss 1.9450427856445311\n",
      "Epoch 846 : train_loss 1.933396016438802\n",
      "Epoch 846 : test_loss 1.946064453125\n",
      "Epoch 847 : train_loss 1.9339632432725695\n",
      "Epoch 847 : test_loss 1.9438040161132812\n",
      "Epoch 848 : train_loss 1.9369282972547743\n",
      "Epoch 848 : test_loss 1.904978271484375\n",
      "Epoch 849 : train_loss 1.93370556640625\n",
      "Epoch 849 : test_loss 1.9253568115234374\n",
      "Epoch 850 : train_loss 1.9305265502929687\n",
      "Epoch 850 : test_loss 1.9387027587890624\n",
      "Epoch 851 : train_loss 1.9322176378038194\n",
      "Epoch 851 : test_loss 1.915010498046875\n",
      "Epoch 852 : train_loss 1.9308022800021702\n",
      "Epoch 852 : test_loss 1.9040518798828125\n",
      "Epoch 853 : train_loss 1.9348823581271701\n",
      "Epoch 853 : test_loss 1.9308461303710938\n",
      "Epoch 854 : train_loss 1.9305913628472222\n",
      "Epoch 854 : test_loss 1.942869873046875\n",
      "Epoch 855 : train_loss 1.929749789767795\n",
      "Epoch 855 : test_loss 1.9420199584960938\n",
      "Epoch 856 : train_loss 1.9285796983506944\n",
      "Epoch 856 : test_loss 1.9374593505859374\n",
      "Epoch 857 : train_loss 1.9312882690429687\n",
      "Epoch 857 : test_loss 1.9467421875\n",
      "Epoch 858 : train_loss 1.9305412122938368\n",
      "Epoch 858 : test_loss 1.9415223388671874\n",
      "Epoch 859 : train_loss 1.9244872368706598\n",
      "Epoch 859 : test_loss 1.9380974731445313\n",
      "Epoch 860 : train_loss 1.9298809882269965\n",
      "Epoch 860 : test_loss 1.9192928466796875\n",
      "Epoch 861 : train_loss 1.9212828979492187\n",
      "Epoch 861 : test_loss 1.9127950439453125\n",
      "Epoch 862 : train_loss 1.9340956420898439\n",
      "Epoch 862 : test_loss 1.92045849609375\n",
      "Epoch 863 : train_loss 1.9307733696831597\n",
      "Epoch 863 : test_loss 1.9299791870117187\n",
      "Epoch 864 : train_loss 1.9262510850694445\n",
      "Epoch 864 : test_loss 1.931152587890625\n",
      "Epoch 865 : train_loss 1.9292181125217014\n",
      "Epoch 865 : test_loss 1.9427376098632811\n",
      "Epoch 866 : train_loss 1.9351227349175346\n",
      "Epoch 866 : test_loss 1.9793604736328125\n",
      "Epoch 867 : train_loss 1.9288665974934895\n",
      "Epoch 867 : test_loss 1.9399503173828125\n",
      "Epoch 868 : train_loss 1.9350133192274306\n",
      "Epoch 868 : test_loss 1.9471214599609374\n",
      "Epoch 869 : train_loss 1.9227781439887153\n",
      "Epoch 869 : test_loss 1.9339188232421876\n",
      "Epoch 870 : train_loss 1.9243947380913629\n",
      "Epoch 870 : test_loss 1.9255895385742188\n",
      "Epoch 871 : train_loss 1.9279061414930556\n",
      "Epoch 871 : test_loss 1.920535888671875\n",
      "Epoch 872 : train_loss 1.9321830715603299\n",
      "Epoch 872 : test_loss 1.958574462890625\n",
      "Epoch 873 : train_loss 1.930362284342448\n",
      "Epoch 873 : test_loss 1.9376063232421874\n",
      "Epoch 874 : train_loss 1.924006354437934\n",
      "Epoch 874 : test_loss 1.9214835815429687\n",
      "Epoch 875 : train_loss 1.9269671156141492\n",
      "Epoch 875 : test_loss 1.914291015625\n",
      "Epoch 876 : train_loss 1.936318122016059\n",
      "Epoch 876 : test_loss 1.9584130859375\n",
      "Epoch 877 : train_loss 1.9245053914388022\n",
      "Epoch 877 : test_loss 1.9281077270507812\n",
      "Epoch 878 : train_loss 1.9316780802408855\n",
      "Epoch 878 : test_loss 1.9425745849609375\n",
      "Epoch 879 : train_loss 1.9312412109375\n",
      "Epoch 879 : test_loss 1.9078514404296876\n",
      "Epoch 880 : train_loss 1.9381505533854166\n",
      "Epoch 880 : test_loss 1.939506591796875\n",
      "Epoch 881 : train_loss 1.9345080159505208\n",
      "Epoch 881 : test_loss 1.9437354736328125\n",
      "Epoch 882 : train_loss 1.9310188869900173\n",
      "Epoch 882 : test_loss 1.922498046875\n",
      "Epoch 883 : train_loss 1.9297788967556424\n",
      "Epoch 883 : test_loss 1.9475455932617187\n",
      "Epoch 884 : train_loss 1.9354517686631945\n",
      "Epoch 884 : test_loss 1.9472935791015624\n",
      "Epoch 885 : train_loss 1.943408454047309\n",
      "Epoch 885 : test_loss 1.9520093994140626\n",
      "Epoch 886 : train_loss 1.9346719970703126\n",
      "Epoch 886 : test_loss 1.9212721557617187\n",
      "Epoch 887 : train_loss 1.9249580688476562\n",
      "Epoch 887 : test_loss 1.9121220703125\n",
      "Epoch 888 : train_loss 1.9241807183159723\n",
      "Epoch 888 : test_loss 1.9063564453125\n",
      "Epoch 889 : train_loss 1.9345679796006945\n",
      "Epoch 889 : test_loss 1.9383692626953124\n",
      "Epoch 890 : train_loss 1.9285067816840278\n",
      "Epoch 890 : test_loss 1.941732177734375\n",
      "Epoch 891 : train_loss 1.925071770562066\n",
      "Epoch 891 : test_loss 1.9072464599609376\n",
      "Epoch 892 : train_loss 1.928464633517795\n",
      "Epoch 892 : test_loss 1.9551963500976564\n",
      "Epoch 893 : train_loss 1.9297918972439236\n",
      "Epoch 893 : test_loss 1.9259561767578126\n",
      "Epoch 894 : train_loss 1.9260082465277777\n",
      "Epoch 894 : test_loss 1.9006931762695312\n",
      "Epoch 895 : train_loss 1.9309112752278645\n",
      "Epoch 895 : test_loss 1.9297783203125\n",
      "Epoch 896 : train_loss 1.9276763983832466\n",
      "Epoch 896 : test_loss 1.919480712890625\n",
      "Epoch 897 : train_loss 1.927188747829861\n",
      "Epoch 897 : test_loss 1.9308732299804687\n",
      "Epoch 898 : train_loss 1.9290161743164063\n",
      "Epoch 898 : test_loss 1.926546142578125\n",
      "Epoch 899 : train_loss 1.9310427720811632\n",
      "Epoch 899 : test_loss 1.9174285888671876\n",
      "Epoch 900 : train_loss 1.928415025499132\n",
      "Epoch 900 : test_loss 1.92298681640625\n",
      "Epoch 901 : train_loss 1.9326527506510416\n",
      "Epoch 901 : test_loss 1.94801220703125\n",
      "Epoch 902 : train_loss 1.9330225796169704\n",
      "Epoch 902 : test_loss 1.960090087890625\n",
      "Epoch 903 : train_loss 1.940620808919271\n",
      "Epoch 903 : test_loss 1.9586077880859376\n",
      "Epoch 904 : train_loss 1.927002685546875\n",
      "Epoch 904 : test_loss 1.9223267822265624\n",
      "Epoch 905 : train_loss 1.925891615125868\n",
      "Epoch 905 : test_loss 1.9168975830078125\n",
      "Epoch 906 : train_loss 1.9283212280273438\n",
      "Epoch 906 : test_loss 1.915771484375\n",
      "Epoch 907 : train_loss 1.9251748996310765\n",
      "Epoch 907 : test_loss 1.9375606689453124\n",
      "Epoch 908 : train_loss 1.9321425577799478\n",
      "Epoch 908 : test_loss 1.9490266723632812\n",
      "Epoch 909 : train_loss 1.933846408420139\n",
      "Epoch 909 : test_loss 1.9390465087890625\n",
      "Epoch 910 : train_loss 1.92630810546875\n",
      "Epoch 910 : test_loss 1.9070343627929687\n",
      "Epoch 911 : train_loss 1.9354280870225695\n",
      "Epoch 911 : test_loss 1.920830810546875\n",
      "Epoch 912 : train_loss 1.9283523186577691\n",
      "Epoch 912 : test_loss 1.9226395263671876\n",
      "Epoch 913 : train_loss 1.9319542236328124\n",
      "Epoch 913 : test_loss 1.950225341796875\n",
      "Epoch 914 : train_loss 1.92254541015625\n",
      "Epoch 914 : test_loss 1.9420507202148438\n",
      "Epoch 915 : train_loss 1.9255170491536457\n",
      "Epoch 915 : test_loss 1.9362843017578124\n",
      "Epoch 916 : train_loss 1.9264849717881944\n",
      "Epoch 916 : test_loss 1.9223212280273438\n",
      "Epoch 917 : train_loss 1.9220698038736979\n",
      "Epoch 917 : test_loss 1.9563543701171875\n",
      "Epoch 918 : train_loss 1.9284285142686632\n",
      "Epoch 918 : test_loss 1.9133643798828126\n",
      "Epoch 919 : train_loss 1.9360107014973957\n",
      "Epoch 919 : test_loss 1.943869140625\n",
      "Epoch 920 : train_loss 1.9307308688693576\n",
      "Epoch 920 : test_loss 1.9270703125\n",
      "Epoch 921 : train_loss 1.9278084309895833\n",
      "Epoch 921 : test_loss 1.9243411865234374\n",
      "Epoch 922 : train_loss 1.931230238172743\n",
      "Epoch 922 : test_loss 1.94188134765625\n",
      "Epoch 923 : train_loss 1.9304628295898438\n",
      "Epoch 923 : test_loss 1.9522786865234374\n",
      "Epoch 924 : train_loss 1.9236980116102431\n",
      "Epoch 924 : test_loss 1.9327603149414063\n",
      "Epoch 925 : train_loss 1.9212222663031684\n",
      "Epoch 925 : test_loss 1.953766357421875\n",
      "Epoch 926 : train_loss 1.9349781697591146\n",
      "Epoch 926 : test_loss 1.923638671875\n",
      "Epoch 927 : train_loss 1.932928229437934\n",
      "Epoch 927 : test_loss 1.9889276733398438\n",
      "Epoch 928 : train_loss 1.9252474636501735\n",
      "Epoch 928 : test_loss 1.8880479736328124\n",
      "Epoch 929 : train_loss 1.9346839938693576\n",
      "Epoch 929 : test_loss 1.9417604370117187\n",
      "Epoch 930 : train_loss 1.9201578843858507\n",
      "Epoch 930 : test_loss 1.9324075927734374\n",
      "Epoch 931 : train_loss 1.9292040269639756\n",
      "Epoch 931 : test_loss 1.9334310302734374\n",
      "Epoch 932 : train_loss 1.926470418294271\n",
      "Epoch 932 : test_loss 1.9249974975585937\n",
      "Epoch 933 : train_loss 1.9295608249240452\n",
      "Epoch 933 : test_loss 1.924502685546875\n",
      "Epoch 934 : train_loss 1.9325601060655382\n",
      "Epoch 934 : test_loss 1.9464268798828126\n",
      "Epoch 935 : train_loss 1.928909437391493\n",
      "Epoch 935 : test_loss 1.9238333129882812\n",
      "Epoch 936 : train_loss 1.9284287109375\n",
      "Epoch 936 : test_loss 1.9201627807617188\n",
      "Epoch 937 : train_loss 1.929697245279948\n",
      "Epoch 937 : test_loss 1.9514214477539062\n",
      "Epoch 938 : train_loss 1.9213541395399305\n",
      "Epoch 938 : test_loss 1.9132904052734374\n",
      "Epoch 939 : train_loss 1.9355537448459201\n",
      "Epoch 939 : test_loss 1.9234971923828126\n",
      "Epoch 940 : train_loss 1.9251327175564237\n",
      "Epoch 940 : test_loss 1.931026611328125\n",
      "Epoch 941 : train_loss 1.9337750651041667\n",
      "Epoch 941 : test_loss 1.92598193359375\n",
      "Epoch 942 : train_loss 1.930984361436632\n",
      "Epoch 942 : test_loss 1.9301010131835938\n",
      "Epoch 943 : train_loss 1.929776869032118\n",
      "Epoch 943 : test_loss 1.931694580078125\n",
      "Epoch 944 : train_loss 1.9249045884874132\n",
      "Epoch 944 : test_loss 1.942436767578125\n",
      "Epoch 945 : train_loss 1.9250757853190104\n",
      "Epoch 945 : test_loss 1.9591017456054687\n",
      "Epoch 946 : train_loss 1.9313766004774306\n",
      "Epoch 946 : test_loss 1.91688818359375\n",
      "Epoch 947 : train_loss 1.924572747124566\n",
      "Epoch 947 : test_loss 1.9266956787109375\n",
      "Epoch 948 : train_loss 1.919271708170573\n",
      "Epoch 948 : test_loss 1.9183677978515625\n",
      "Epoch 949 : train_loss 1.9266660698784723\n",
      "Epoch 949 : test_loss 1.9186328125\n",
      "Epoch 950 : train_loss 1.9314252183702256\n",
      "Epoch 950 : test_loss 1.906773681640625\n",
      "Epoch 951 : train_loss 1.9278196445041234\n",
      "Epoch 951 : test_loss 1.9259735107421876\n",
      "Epoch 952 : train_loss 1.926947781032986\n",
      "Epoch 952 : test_loss 1.94990576171875\n",
      "Epoch 953 : train_loss 1.924411627875434\n",
      "Epoch 953 : test_loss 1.925716796875\n",
      "Epoch 954 : train_loss 1.927319593641493\n",
      "Epoch 954 : test_loss 1.9197556762695311\n",
      "Epoch 955 : train_loss 1.9202616305881077\n",
      "Epoch 955 : test_loss 1.9297913818359376\n",
      "Epoch 956 : train_loss 1.931788306342231\n",
      "Epoch 956 : test_loss 1.920827392578125\n",
      "Epoch 957 : train_loss 1.929267822265625\n",
      "Epoch 957 : test_loss 1.9285970458984374\n",
      "Epoch 958 : train_loss 1.9279051174587674\n",
      "Epoch 958 : test_loss 1.932722412109375\n",
      "Epoch 959 : train_loss 1.9242845526801216\n",
      "Epoch 959 : test_loss 1.928060546875\n",
      "Epoch 960 : train_loss 1.9230227322048612\n",
      "Epoch 960 : test_loss 1.9323856201171874\n",
      "Epoch 961 : train_loss 1.9270250244140625\n",
      "Epoch 961 : test_loss 1.9266239013671875\n",
      "Epoch 962 : train_loss 1.9268301120334201\n",
      "Epoch 962 : test_loss 1.93184228515625\n",
      "Epoch 963 : train_loss 1.918526387532552\n",
      "Epoch 963 : test_loss 1.9521231079101562\n",
      "Epoch 964 : train_loss 1.937419209798177\n",
      "Epoch 964 : test_loss 1.9449085083007813\n",
      "Epoch 965 : train_loss 1.9282930908203124\n",
      "Epoch 965 : test_loss 1.9272861938476562\n",
      "Epoch 966 : train_loss 1.927832282172309\n",
      "Epoch 966 : test_loss 1.945607421875\n",
      "Epoch 967 : train_loss 1.9292877943250868\n",
      "Epoch 967 : test_loss 1.8970390625\n",
      "Epoch 968 : train_loss 1.9283084106445312\n",
      "Epoch 968 : test_loss 1.942287109375\n",
      "Epoch 969 : train_loss 1.9277774047851564\n",
      "Epoch 969 : test_loss 1.8986290283203124\n",
      "Epoch 970 : train_loss 1.9289919230143229\n",
      "Epoch 970 : test_loss 1.9250425415039063\n",
      "Epoch 971 : train_loss 1.9376546563042534\n",
      "Epoch 971 : test_loss 1.9227318115234375\n",
      "Epoch 972 : train_loss 1.9335104031032986\n",
      "Epoch 972 : test_loss 1.8971806640625\n",
      "Epoch 973 : train_loss 1.9166104193793403\n",
      "Epoch 973 : test_loss 1.9343450927734376\n",
      "Epoch 974 : train_loss 1.9279989691840278\n",
      "Epoch 974 : test_loss 1.94516796875\n",
      "Epoch 975 : train_loss 1.9334077962239584\n",
      "Epoch 975 : test_loss 1.93840234375\n",
      "Epoch 976 : train_loss 1.931808861626519\n",
      "Epoch 976 : test_loss 1.9192990112304686\n",
      "Epoch 977 : train_loss 1.927767347547743\n",
      "Epoch 977 : test_loss 1.9558299560546875\n",
      "Epoch 978 : train_loss 1.9268542107476128\n",
      "Epoch 978 : test_loss 1.9195609741210937\n",
      "Epoch 979 : train_loss 1.9218863661024306\n",
      "Epoch 979 : test_loss 1.9434636840820312\n",
      "Epoch 980 : train_loss 1.9263481343587239\n",
      "Epoch 980 : test_loss 1.944847412109375\n",
      "Epoch 981 : train_loss 1.9300614691840279\n",
      "Epoch 981 : test_loss 1.9087060546875\n",
      "Epoch 982 : train_loss 1.9194483235677082\n",
      "Epoch 982 : test_loss 1.91431689453125\n",
      "Epoch 983 : train_loss 1.9247756754557293\n",
      "Epoch 983 : test_loss 1.9014054565429688\n",
      "Epoch 984 : train_loss 1.9260286865234375\n",
      "Epoch 984 : test_loss 1.9259437866210938\n",
      "Epoch 985 : train_loss 1.9280999959309897\n",
      "Epoch 985 : test_loss 1.9437310791015625\n",
      "Epoch 986 : train_loss 1.9233319769965278\n",
      "Epoch 986 : test_loss 1.9262000732421876\n",
      "Epoch 987 : train_loss 1.925529534233941\n",
      "Epoch 987 : test_loss 1.9349439697265625\n",
      "Epoch 988 : train_loss 1.9248932495117188\n",
      "Epoch 988 : test_loss 1.9314030151367187\n",
      "Epoch 989 : train_loss 1.9311525607638889\n",
      "Epoch 989 : test_loss 1.9087218017578125\n",
      "Epoch 990 : train_loss 1.9234712863498264\n",
      "Epoch 990 : test_loss 1.9376654052734374\n",
      "Epoch 991 : train_loss 1.926748508029514\n",
      "Epoch 991 : test_loss 1.972744384765625\n",
      "Epoch 992 : train_loss 1.9303648546006944\n",
      "Epoch 992 : test_loss 1.950643798828125\n",
      "Epoch 993 : train_loss 1.921553175184462\n",
      "Epoch 993 : test_loss 1.9347969970703125\n",
      "Epoch 994 : train_loss 1.9248248426649306\n",
      "Epoch 994 : test_loss 1.9456553955078124\n",
      "Epoch 995 : train_loss 1.9311315511067708\n",
      "Epoch 995 : test_loss 1.9045931396484375\n",
      "Epoch 996 : train_loss 1.9234457058376737\n",
      "Epoch 996 : test_loss 1.92087109375\n",
      "Epoch 997 : train_loss 1.9265461900499132\n",
      "Epoch 997 : test_loss 1.9267133178710938\n",
      "Epoch 998 : train_loss 1.9288196614583333\n",
      "Epoch 998 : test_loss 1.9390308837890624\n",
      "Epoch 999 : train_loss 1.9266890055338541\n",
      "Epoch 999 : test_loss 1.9287398071289064\n"
     ]
    }
   ],
   "source": [
    "for n_train in [1000,5000,9000]:\n",
    "    print('Training with {}'.format(n_train))\n",
    "    train(n_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 500])\n",
      "tensor(-1.1258) tensor(-1.9877)\n",
      "torch.Size([2, 500])\n",
      "tensor(-1.1258) tensor(-1.9877)\n",
      "torch.Size([2, 500])\n",
      "tensor(-1.1258) tensor(-1.9877)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAJMCAYAAACcglAxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTZUlEQVR4nO3deXxU5aH/8e/MZIdkQiAbGDCgAmncQImJraKiUCmK5VpLtahFW7lgVbxewZ+K4LW4VW2t1fZeC/aibbXXDWujLC5XQVCQagC5QsMWk7DPQEK2mfP7Y8iQySyZSSYzyeHzfr3ysnPOc855ZpiGL89qMQzDEAAAAHo1a7wrAAAAgK4j1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJhAt4a6Dz/8UJMmTdLAgQNlsVj0+uuv+5w3DEP333+/8vPzlZqaqnHjxunrr7/u8L7PPPOMTj75ZKWkpKikpERr167tpncAAADQO3RrqKurq9OZZ56pZ555JuD5Rx99VL/+9a/13HPPac2aNerTp4/Gjx+vhoaGoPf8y1/+otmzZ2vevHlav369zjzzTI0fP1579uzprrcBAADQ41kMwzBi8iCLRa+99pomT54sydNKN3DgQN155536t3/7N0mSw+FQbm6uFi9erB/+8IcB71NSUqJzzz1Xv/nNbyRJbrdbBQUFuvXWWzVnzpxYvBUAAIAeJyFeD66srFRNTY3GjRvnPWa321VSUqLVq1cHDHVNTU1at26d5s6d6z1mtVo1btw4rV69OuizGhsb1djY6H3tdrt14MAB9e/fXxaLJUrvCAAAIDDDMHT48GENHDhQVmv3dJTGLdTV1NRIknJzc32O5+bmes+1t2/fPrlcroDXfPXVV0GftXDhQs2fP7+LNQYAAOiaXbt26aSTTuqWe8ct1MXS3LlzNXv2bO9rh8OhwYMHa9euXcrIyIhjzQAAwInA6XSqoKBA6enp3faMuIW6vLw8SVJtba3y8/O9x2tra3XWWWcFvGbAgAGy2Wyqra31OV5bW+u9XyDJyclKTk72O56RkUGoAwAAMdOdw77itk5dYWGh8vLytGLFCu8xp9OpNWvWqLS0NOA1SUlJGj16tM81brdbK1asCHoNAADAiaBbW+qOHDmirVu3el9XVlZqw4YNysrK0uDBg3X77bfrP/7jP3TqqaeqsLBQ9913nwYOHOidIStJl1xyia666irNmjVLkjR79mxdf/31OuecczRmzBg99dRTqqur04033tidbwUAAKBH69ZQ99lnn+miiy7yvm4d13b99ddr8eLF+vd//3fV1dXppz/9qQ4dOqRvf/vbKi8vV0pKiveabdu2ad++fd7X11xzjfbu3av7779fNTU1Ouuss1ReXu43eQIAAOBEErN16noSp9Mpu90uh8PBmDoAANDtYpE92PsVAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYQNxD3cknnyyLxeL3M3PmzIDlFy9e7Fc2JSUlxrUGAADoWRLiXYFPP/1ULpfL+7qiokKXXnqprr766qDXZGRkaMuWLd7XFoulW+sIAADQ08U91GVnZ/u8fvjhhzVs2DBdeOGFQa+xWCzKy8vr7qoBAAD0GnHvfm2rqalJS5Ys0U9+8pOQrW9HjhzRkCFDVFBQoCuvvFIbN26MYS0BAAB6nh4V6l5//XUdOnRIN9xwQ9Ayw4cP1x/+8Ae98cYbWrJkidxut8rKyrR79+6g1zQ2NsrpdPr8AAAAmInFMAwj3pVoNX78eCUlJWnp0qVhX9Pc3KyRI0dq6tSpevDBBwOWeeCBBzR//ny/4w6HQxkZGZ2uLwAAQDicTqfsdnu3Zo8e01K3Y8cOLV++XDfddFNE1yUmJurss8/W1q1bg5aZO3euHA6H92fXrl1drS4AAECP0mNC3aJFi5STk6OJEydGdJ3L5dKXX36p/Pz8oGWSk5OVkZHh8wMAAGAmPSLUud1uLVq0SNdff70SEnwn5E6bNk1z5871vl6wYIHeffdd/fOf/9T69et13XXXaceOHRG38AEAAJhJ3Jc0kaTly5dr586d+slPfuJ3bufOnbJaj2fPgwcP6uabb1ZNTY369eun0aNHa9WqVSoqKopllQEAAHqUHjVRIlZiMVgRAACg1Qk1UQIAAACdR6gDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmEPdQ98ADD8hisfj8jBgxIuQ1r7zyikaMGKGUlBSdfvrpevvtt2NUWwAAgJ4p7qFOkr71rW+purra+/PRRx8FLbtq1SpNnTpV06dP1+eff67Jkydr8uTJqqioiGGNAQAAepYeEeoSEhKUl5fn/RkwYEDQsr/61a80YcIE3XXXXRo5cqQefPBBjRo1Sr/5zW9iWGMAAICepUeEuq+//loDBw7U0KFDde2112rnzp1By65evVrjxo3zOTZ+/HitXr066DWNjY1yOp0+PwAAAGYS91BXUlKixYsXq7y8XM8++6wqKyv1ne98R4cPHw5YvqamRrm5uT7HcnNzVVNTE/QZCxculN1u9/4UFBRE9T0AAADEW9xD3Xe/+11dffXVOuOMMzR+/Hi9/fbbOnTokF5++eWoPWPu3LlyOBzen127dkXt3gAAAD1BQrwr0F5mZqZOO+00bd26NeD5vLw81dbW+hyrra1VXl5e0HsmJycrOTk5qvUEAADoSeLeUtfekSNHtG3bNuXn5wc8X1paqhUrVvgcW7ZsmUpLS2NRPQAAgB4p7qHu3/7t3/TBBx9o+/btWrVqla666irZbDZNnTpVkjRt2jTNnTvXW/62225TeXm5fvnLX+qrr77SAw88oM8++0yzZs2K11sAAACIu7h3v+7evVtTp07V/v37lZ2drW9/+9v65JNPlJ2dLUnauXOnrNbj2bOsrEwvvfSS7r33Xt1zzz069dRT9frrr6u4uDhebwEAACDuLIZhGPGuRKw5nU7Z7XY5HA5lZGTEuzoAAMDkYpE94t79CgAAgK4j1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAIJ8a4AAHSWy21obeUB7TncoJz0FI0pzJLNaol3tQAgLgh1AHql8opqzV+6SdWOBu+xfHuK5k0q0oTi/DjWDADig+5XAL1OeUW1ZixZ7xPoJKnG0aAZS9arvKI6TjUDgPgh1AHoVVxuQ/OXbpIR4FzrsflLN8nlDlQCAMyLUAegV1lbecCvha4tQ1K1o0FrKw/ErlIA0AMQ6gD0KnsOBw90nSkHAGZBqAPQq+Skp0S1HACYBaEOQK8ypjBL+fYUBVu4xCLPLNgxhVmxrBYAxB2hDkCvYrNaNG9SkST5BbvW1/MmFbFeHYATDqEOQK8zoThfz143Snl23y7WPHuKnr1uFOvUATghsfgwgF5pQnG+Li3KY0cJADiGUAeg17JZLSod1j/e1QCAHoHuVwAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAE2PsVQI/gchtaW3lAew43KCc9RWMKs2SzWuJdLQDoNQh1AOKuvKJa85duUrWjwXss356ieZOKNKE4P441A4Deg+5XAHFVXlGtGUvW+wQ6SapxNGjGkvUqr6iOU80AoHch1AGIG5fb0Pylm2QEONd6bP7STWpqcWv1tv16Y0OVVm/bL5c70BUAcGKj+xVA3KytPODXQteWIana0aDzFq7Qgbom73G6ZgHAHy11AOJmz+Hgga6ttoFOomsWAAIh1AGIm5z0lE5d17Zrlq5YAPAg1AGImzGFWcq3p6gzC5e0ds2urTwgl9ulT2s+1dv/fFuf1nwql9sV7aoCQI/HmDoAcWOzWjRvUpFmLFkvixRwwoQ/t2xp/5QtbZsk6U9ff6n71i9XbX2tt0RuWq7mjJmjcUPGdUe1AaBHintL3cKFC3XuuecqPT1dOTk5mjx5srZs2RLymsWLF8tisfj8pKR0rhsHQHxNKM7Xs9eNUp7d9//DWX0S/compFeoz6n/obQh/6Xk7PeUnP2e3qt90SfQSdKe+j2a/f5sLd+xvFvrDgA9Sdxb6j744APNnDlT5557rlpaWnTPPffosssu06ZNm9SnT5+g12VkZPiEP4uFleeB3mpCcb4uLcrz2VFi9JB+uvCx91TjaJAhT6BLGbQkrPsZMmSRRY+sfUQXFVwkm9XWvW8AAHqAuIe68vJyn9eLFy9WTk6O1q1bpwsuuCDodRaLRXl5ed1dPQAxYrNaVDqsv8+x412zbiXnvilJCvffb4YM1dTXaP2e9To379xoVxcAepy4d7+253A4JElZWVkhyx05ckRDhgxRQUGBrrzySm3cuDFo2cbGRjmdTp8fAD1fa9fsgOwqWROdYQe6tmrr9kS/YgDQA/WoUOd2u3X77bfr/PPPV3FxcdByw4cP1x/+8Ae98cYbWrJkidxut8rKyrR79+6A5RcuXCi73e79KSgo6K63ACDKJhTna97kzv9/dv7ru1jPDsAJwWIYRo9Z5GnGjBn6+9//ro8++kgnnXRS2Nc1Nzdr5MiRmjp1qh588EG/842NjWpsbPS+djqdKigokMPhUEZGRlTqDqD7rKleo5vevSmiawxDMlrsqt96tySrnr1uFDtQAIgbp9Mpu93erdkj7mPqWs2aNUtvvfWWPvzww4gCnSQlJibq7LPP1tatWwOeT05OVnJycjSqCSDGlu9YrofXPhzRNa3/VG2snSRDVlnkWaj40qI82axMqgJgTnHvfjUMQ7NmzdJrr72mlStXqrCwMOJ7uFwuffnll8rP51/hgJks37Fcs9+f7bdkSUeMFrsaqq5Ty2HPMI62CxUDgFnFvaVu5syZeumll/TGG28oPT1dNTU1kiS73a7U1FRJ0rRp0zRo0CAtXLhQkrRgwQKdd955OuWUU3To0CE99thj2rFjh266KbLuGQA9l8vt0sNrH5YR5pLE0rEuV1cf1W29S4F+vYW71ywA9EZxD3XPPvusJGns2LE+xxctWqQbbrhBkrRz505ZrccbFQ8ePKibb75ZNTU16tevn0aPHq1Vq1apqKgoVtUG0A1cbsO7Vt1B1+aIW+gsFsmSUCdb2g656of5ne/sXrMA0Bv0qIkSsRKLwYoAIlNeUa35Szep2uFpTUvI2KDUQX/u1L2OVv1ALc5RPsfy7Sn66O6LGVMHIC5OqIkSAE5c5RXVmrFkvU9HqzVpX6fvZ7Ed8Tt2xZn5BDoAphb3iRIATgwut6HV2/brjQ1VWr1tv1xuw3t8/tJN7UbOuZWYuVad7UdIGvCeEtIrfI69+Y9q7zMBwIxoqQPQ7dp3rUqe7tB5k4pkT03yOS5JtrRKWRM7v/OLxXZUKYOW+MyAbZ392n4rMgAwC1rqAHSr1q7V9sGtxtGgGUvWa/mmGr9rLAmHu/TM1u3EkvNeleT2Hmf2KwAzI9QB6DaBu1Y9Wo+9+nmV/7mW9C4/22KRrAn1Suq/0nuM2a8AzIxQB6DbrK084NdC15Yh6WB9s99xV32h3M32To+paysxa5Uscqt/nyTVOI76jOcDADNhTB2AbtP57k6rGmsnKWXQEhnG8e7UTt0poV7WtErtrxumO17+h6Tj4/nYCxaAmdBSB6DbdKW7s+VwsRqqrpPh7vq+ze3H6LWO5yuvqO7yvQGgpyDUAeg2YwqzlG9PUddWh3N3XKQD7cfotXa+zl+6ia5YAKZBqAPQbWxWi+ZN8mzfF2mwS0ivUMqgJbJY/cfcRcLd0keu+kK/44aOL3MCAGZAqAPQrSYU5+vZ60Ypzx5JV6xbyblLJXVtPJ0kNTvOUqhfdSxzAsAsmCgBoNtNKM7XpUV5Wlt5QHsON2hLjVO/ff+fQct7Fh92ROXZriNFIc+zzAkAs6ClDkBM2KwWlQ7rryvPGqTvnJoTsmxC301dfp5hSO5me8CuV8nTHZxvT9GYwqwuPwsAegJCHYCYa51AEZhbCfYNUXlOY+0kBfo119qjO29SkWzWLvbvAkAPQagDEHNtJ1D4nUurlDWhrsvPcB0t8O772l5mWqKevW4U69QBMBVCHYC4mFCcrzvGnep3vKv7vrayJtco2HIogXaxAIDejlAHIG5OzfHf4zUa+75KktXWLFtaZdDzrFEHwGwIdQDiwuU2dM/rX/ofry+UuyU1Ks8I1erHGnUAzIZQByAufrNyqw4F7Aa1qvlgWVSe0VGrX43jaFSeAwA9AevUAYg5l9vQoo+Dd4266k/u0v0NQzJagi9n0upAXVOXngMAPQktdQBibm3lAR06GnyygqULs1+NY8Pkmg+d22HZrL7JnX4OAPQ0hDoAMbd8U03I89bEfZ2+t8Xi+UnOXq4+pzyihPSKoGXzMthNAoB5EOoAxFR5RbWe/3h70PMJ6RVKyl7ubXHrCkuCQymDlgQMduwmAcBsCHUAYsblNjR/aagtwNxKzl0qydPa1lWt9/Dc07NmneXYD7tJADAbQh2AmFlbeUDVjoag521plbImOqIS6FpZLJI10eFds85ikX56QSG7SQAwHUIdgJjZczh4oJOit5tEqHu7Den3H1aqvKK6254FAPFAqAMQMznpoScmRGs3iY7ubUia++qX7CgBwFQIdQBiZkxhlvLtKQrWu+rZTaJPVJ9pGJK72X/NuoP1zfrNyq+j+iwAiCdCHYCYsVktmjepKEQJq5odZ0Xtea0zaBtrJynQr7tFH2+ntQ6AaRDqAMTUhOJ8PfOjUQo28dR1JFToi4zhSlND1XVqOVwc8Pyho83s/wrANNgmDEDM9euTpGANZJ4u2FRZEzq/L6thSIY7WXVf36OOfs11NHkDAHoLWuoAxFyoIJWQvkmyuLv8jMbqqxXOv1s7mrwBAL0FLXUAupXLbWht5QHtOdygnHTPLg7BglRCeoVSBi3p8jMNt1Uthzvuxs1MTWRXCQCmQagD0G3KK6o1f+kmnwWH8+0puufykbLIs7TIcdHbTcJqc8uWtk2u+lNDlrvx/JPZVQKAadD9CqBblFdUa8aS9X47SNQ4GnTrnz5X+yF10d5Nwpb2z5Dn+6UlatbFoUMfAPQmhDoAUde6x2uguRDBFhCJ9m4SlsSDIc83trj1m5VbWdIEgGkQ6gBEXUd7vAYS7d0kLJbmkOfrm1x6cvn/afR/LGPLMACmQKgDEHWdWSbEVV8od7Pdu2BwV9n6bpXU8SzaQ/XNumXJeoIdgF6PUAcg6jq3TIj12M4P0WG1NSqp/8qwy89fuomuWAC9GqEOQNR1tMerpIDnWg4Xq8UZePeHzkjMWqVwWuskqdrRwO4SAHo1Qh2AqGu7x2uwYBesTcwworfSkjWhXra0yrDLs7sEgN6MUAegW0woztez141Snj2yrlijOTOq9UjouynssuwuAaA3I9QB6DYTivP10d0X68WbSpSZmhjWNa76YVGtQ4J9gzrqgrXIsygyu0sA6M0IdQC6lc1qkdVi0aGjoZcYaeWqL4zaDFhJsibUheyCbe0enjepiN0lAPRqhDoA3S6SsWq2tB1R21WiVWLWh0HP5dlT9Ox1ozShOD+6DwWAGGPvVwDdLpKxatHeWUKSEvpukdSi9r/y/mXUID3yL2fSQgfAFGipA9DtwlnipFW0d5aQJItFSuy32u/48s17ov4sAIgXQh2AbhfOEietXPWFcrekRr0O1iT/NegOHW1mbToApkGoAxAT4S9xYlXzgfOj/nx3U+CZraxNB8AsCHUAYmZCcb4+uOsiZfUJvbyJuyk7ajNgDUMyDIuaD5YGPM/adADMglAHIKbW7TioA3XBlzdJSP9CKYP+FJUZsK3BsGn/dxRoXhhr0wEwE0IdgJgK1d2ZkF6hlEEvRW1JE4tFanaeoaa9lwc8z9p0AMyEUAcgpgb0SQ5yxq3kvFej/jzXkaKAx392QSFr0wEwFUIdgNgK0jBmS9sma0J91BceDrZEyu8/rFR5RXV0HwYAcUSoAxBTew43BjxuS/tn1J9lGJLFFnwx4/lLN8nljuKeZAAQR4Q6ADF14EjgUNcdLBYpOW+pJLffOUNStaOBdeoAmEaPCHXPPPOMTj75ZKWkpKikpERr164NWf6VV17RiBEjlJKSotNPP11vv/12jGoKoKuy+iQFPO6qH9Ytz7Mm1MmWVhn0POvUATCLuIe6v/zlL5o9e7bmzZun9evX68wzz9T48eO1Z0/g7XtWrVqlqVOnavr06fr88881efJkTZ48WRUVFTGuOYDOyLMH3i3CVT9U7pa0qK1P11ao/WRZpw6AWVgMozt+hYavpKRE5557rn7zm99IktxutwoKCnTrrbdqzpw5fuWvueYa1dXV6a233vIeO++883TWWWfpueeeC+uZTqdTdrtdDodDGRkZ0XkjAMLichv69iMrVe3wbyHzLGmyJOqTJep33BywJTA9JUFr7xmn1CRbdB8IAO3EInvEtaWuqalJ69at07hx47zHrFarxo0bp9Wr/TfflqTVq1f7lJek8ePHBy0PoGdp3Qc2UG5rOVysFucZUXuWYUjuZrtc9YUBzx9uaNG35pVr4dubovZMAIiXuIa6ffv2yeVyKTc31+d4bm6uampqAl5TU1MTUXlJamxslNPp9PkBED+t+8Dm++0D65YtbXtUu2Abaycq1K86tyH97sNKgh2AXi/uY+piYeHChbLb7d6fgoKCeFcJOOFNKM7XR3dfrBenlyjh2K4OtrRKWROdUd1RwnD1Cavs7/+3Uk0t/rNkAaC3iGuoGzBggGw2m2pra32O19bWKi8vL+A1eXl5EZWXpLlz58rhcHh/du3a1fXKA+gym9WiF9dsV8uxteJCTWjorHDvaRjSgqUVrFsHoNeKa6hLSkrS6NGjtWLFCu8xt9utFStWqLS0NOA1paWlPuUladmyZUHLS1JycrIyMjJ8fgDE39tffKO3K47/Iy3Y7g9dEck9l6zZpW8/spKdJgD0SnHvfp09e7b+8z//Uy+88II2b96sGTNmqK6uTjfeeKMkadq0aZo7d663/G233aby8nL98pe/1FdffaUHHnhAn332mWbNmhWvtwCgE1xuQ/e+4bsUkau+UIYr2N6wkTEMyTAsctUPiei6GkeDZixZT7AD0OskxLsC11xzjfbu3av7779fNTU1Ouuss1ReXu6dDLFz505ZrcezZ1lZmV566SXde++9uueee3Tqqafq9ddfV3FxcbzeAoBOWFt5QAfqmtsdtaqlIV+JfbZ3+f6ecXmGbGk7IlrY2JBne9r5Szfp0qI82axRXl8FALpJ3NepiwfWqQPi740NVbrtzxv8jicNeEfJ2e9F7TlHq36oFudZnbr2Tzefp9Jh/aNWFwAnLtOvUwfgxBVsJwdX/dCoPqcr4/TYQgxAb0KoAxAXYwqzlBZgJwdX/TAZruj8ajIMRTymri22EAPQmxDqAMTFo+WbVd/k8juekF4hWaOzXpzFIiX1f79T1+bbUzSmMCsq9QCAWCDUAYi5pha3fv9hZYAzbiXnvRHVvV8Ts1ZJijwkzptUxCQJAL0KoQ5AzN3z6hcKNEPLllYpa0JdVJ9lTaiXLS1QgAwsMy1Rz103ShOK86NaDwDobnFf0gTAiaW8olp/XV8V8Fx37CgR7n0zUxN14/mFmnXxKbTQAeiVCHUAYsblNjR/6aag57tjR4lw7nvHuFM16+JTCXMAejW6XwHEzNrKA6p2BF8mxFVfKHdLalSfabgT5aovDHo+My1Rw/PSCXQAej1CHYCY6XjdN6uaD5ZF9ZlN+76jUL/qHPXNbAsGwBQIdQBiJpx131z1J0fteYbbqqb940KXOfbf+Us3yeU+4TbYAWAihDoAMTOmMEv59hSF6ui0RHH2a8vhYoXza86QVO1o0NrKA1F7NgDEGqEOQMzYrBbNm1QUcDmTVtGaLGEYUkLGF57FjMPEtmAAejNCHYCYurQoL+D2YK1c9YVyu5K7/JzWBYyTc5cq3MWH2RYMQG9GqAMQU2srDwTcHuw4qxqrr5IRheFtFotkTXR0uPiwRWwLBqD3I9QBiKlwujhbDp+llsMjohLspNCLD7eO72NbMAC9HaEOQEyF28XZfPD8qO0BG2qcXp49Rc+yLRgAE2BHCQAxdbCuSVaL1NHqIYmZn3T5WYYhGS12v8WHx5zcT9eeN0Q56Z4uV1roAJgBoQ5AzJRXVGvmS+tDzn6VpIT0CiVkbOzSs1q7bhtrJ6p9p0TpsAG68qxBXbo/APQ0dL8CiInWfV87HibnPjZjtWssFs+P4erjd+7lz3ax0DAA0yHUAYiJjvZ9bWVLq5Q10RG18XSBJkmw0DAAM6L7FUBMhLuwryXBEdXnBpskYZqFht0uaccq6Uit1DdXGlImWYOvAwjAvAh1AGIi3FmvFlv0tglzt/TxmyQRaX16tE1vSuV3S85vjh/LGChNeEQquiJ+9QIQF3S/AoiJg3WNYZUzXH2j8jzDkBprrlSgX3P21ITev9Dwpjell6f5BjpJclZ7jm96Mz71AhA3hDoAUedyG1q9bb/e2FCl1dv2q6nFrQf/tjmsa42WjC4/3zAkw52klsPFAc+PzM/o3cuYuF2eFrqA006OHSuf4ykH4IRB9yuAqCqvqNb8pZt8JkVk9UnUgbrmsK531RfK3WyXJaHzkyUsFslia5ItrVKu+mF+5z/55wGVV1RHtuBwTxq7tmOVfwudD0NyVnnKFX4nZtUCEF+EOgBRU15RrRlL/NehCzfQeVjVWDtJKYOWdLk+oSZdzF+6SZcW5YXXYtfTxq4dqY1uOQCmQPcrgKgIfx26jrUcLpJhdL0VzGILvudr2MuaBB279o308o+l9x+JfTdn39yulXO7pMr/lb78q+e/dNMCpkBLHYCoCHcdunDY0ipltXY9aFhTqkOe73BZk5Bj1455/xfSmuekiU9KxZMjrmOnDCnztBQ6q4PUzeI5P6TM/1RPa3UEEDW01AGIimiu+xZoweBO3cfaFPJ8TnpK6FarDseuHXP0gPTX66V37wteJtRzIm05s9o8IUySFKj72JAmPOw/5o8Zs4Cp0VIHICrCXfctq0+SDtT5h618e4oK+qVq7faDQRcMjpSr/uSg5/LtKRrT8JH01JzgrVaRjklb9Wtp0GjpW5N9j4dqHZM613JWdIX0gz9KS38uHT3oey41wHItLU3SW7cr+IxZi2fG7IiJLF4M9FKEOgBRMaYwS/n2FNU4GoJ1CCrPnqIP7rpI63YcVI3jqA7UNSmrb7J27q/Tn9bu1NrtnnDiqi+U25Ukqy10S1sohiE1HwzQ/XisLr8dtVu2V26TX8hpHSt33r9K9pMif/Bbs6XUflLdXs+Ytrr90l9vCPCcas9zAmltOfvBH4MHO7dL2rPZP9BJnmNtr9/0pvTWHVL9/hAVZ8Ys0NsR6gBEhc1q0bxJRZqxZL0s8o0wrR2E8yYVKSnBqtJh/eVyG/rkn/u15JMd+ntFTbu7WWU0Z0i2fZ2rjGHoXw41qdq6Xu+4x/icyrenaN73huvsZZcq5Fi5T37bpvYRTP84ul/6Y9sgFuz6UPc8du6tO6TTJkgJSb6nA7X8+V1/rOXNcEuv3NDB89pgxizQaxHqAETNhOJ8PXvdKL916vLsKZo3qci7LtzbX3yju/7nC9U1Bh47lpD+hazJnQx0x5Q1OjUu8SnNaL5dzpPH6wfnDFaePVVjCrNk2/FReGPlJEUU6KJ9ff0+6YkR0veeOt5i1zoursP7Hmt5+9udkdUh3Jm1AHocQh2A6Di2OO8Eo1aXXpOjta7TtaeuWTnpKZ4gdWw9uIVvb9LvPqwMdSMl573R6YWHWz02oJ8u2nlU8xL/W9/+5zk646RMXTXqWHdqb2qNqt9/vCt1xMSOZ+P6XR9uOA4xYxZAr0CoA+Av0t0T2nUH2iSVtg72H3a8K/LtL6p9Ap1Vbo2xfqUcHdIeZWqte4QsaZWyJtR1rf4Wi2oSErQhNVnnNuzXjba39eVHg+VK/B9PuEyObCKG4Zbq9yappcGmhBSX0rKbZAmxdkCk5cOogacrNqlvBC2MnRBoxiyAXsNiGEY01grtVZxOp+x2uxwOhzIyur7PJGAq4axj1jb07d/mWavNz7GmtmOD9V1uQyX/8Y5OafhSOTqkIZYa/ShhpfItxxcA3mek65HUYpXnV0XlrTyyZ58ur6uX1Pmg5dyVotr1drUcPR52ElJdyh3lUEaB/zIukZYPJmB9U/pKTUfCvofSBvi11AW8b98B0veeZJ06oBvFInvQUgfguGDjtdrOxpQ6GKTfqs3G8iMmausHf9abrns1MOl4iGv/T8oBlsP6gdarXNEZ15Xt8ozZCydoBQo7h6tSVPVxP7/7thy1qurjfjLKDso++HhQc+4KXV7nHwwr2IWub/jvX2f+UNr4qneR4oD3TTOU+9ATyii6PIIbA+iJaKmjpQ7wcLukp4pDhDWLZ6mOowcV8eD/k86VsftTGYbU0VarLkkXDR6kg7bg3YAWt6GRuwz1OyId7Ct9NUgaUSXv680nSXkut/5n/T7VVaXo4P/1Of4evDzvYdD5B2UYUs1ndrmbjj/TkuiSxSK5m6wKvMCv5x79iw7Lmmiouc4mx/ZUGS3ByhuyJruVd7ZTthRP2HQ1+rca+gZD//oOKD6sAUVHwuvOzRgkjf+F9MoNx+6bGeC+kiwWDfrVU8q47DLPk1wu1X+2Ti179yohO1tp54yWJcSfB4COxSJ7EOoIdYBH5f9KL3wvZJHojxUL7NGsTP23/fj/Ny1uQ0U7DRVvd2vELqmwVkprPl7eZZFsbX6THU6Rkgy3khs7qpwha4Jb7qBBLDZsyS6lZjcpKb1Zjn/2kasxdH2sSW5lnXbEJ9wF/bO5/i0ZR/Zr6w1z1VJnBL6vxaKE3FydsmK5Dq9YoZqHfiFX7fHJJLbcXOX9v3u8oQ9A5Oh+BRA7HcwIjdZYsVBag8ml37i0fpBbMgyN3ipd/IWhtBDrEFvb/dO0b4MU3i6IFrlbbOr6siVd42q06cjuVEmpYZV3N1m1ryJD+zb2UZ/cZtlSXDpSnSJ34/E/G2uSS/aTjyo9508ykvqqpU4KGhQNQy01Ndr33HPa9/Rv/OtXW6uqn98m/fpXBDugB6OljpY6wCNES104XYJJ6S4lpLiU2r9JR/dHZ0ICIhGkFU6SJcF9rFs4NEtqqoyjR4Oet2Zm6rSPP6IrFugEWuoAxM6QMik9Xzpc7XPY3eIZb+bRPjR4dkvYV9H2F5RvuLAmudTv1DolpbvUXG9V06FEuV1S6oBmZZ1aJ2tC8AkGiETw7tpwAp2kkIFOktyHDqlu7afqW3peRDUL+jzG7gFRRagDTkDB/jI1zr5e9a884W1la2m0+k0g8Bco6B3nbrJp/0b/f5UeqZL2/iNDffIbVL83KeC16Hnq16yJSqhzvvuuan+xUC01x7eIS8jLU+49c+niBTqJUAecYJzvvqvah36hljYD4RNyc5XxvYlyvv66WvYPaFO6u0dnWFRXHd44MpiH8913VXXb7X5r2rTU1nqOt5mJCyB8jKljTB1OIM533/UMeA9b8HFaODEVPP9fsiQkqqW2Vi0HDsiWlaXE3FylnTNakgK3ALdpGbb176/qOXN8/lHho81MXLpiYSYsadJNCHU4ERnNTfq/sjK5D3dxCy6c0Gw52XLt2et33JqZKckz7q5VQl6eMiZeLuff3vbpZg1H/1tuUZ/SUsbZwTQIdd2EUIcTzqY3deS/7taut+NdEfR+sW29ZZwdzCIW2aMblg0F0KMc2/qrfrsz3jWBKcS2O751nJ3z3Xdj+lygNyLUAWbmdnn2aWVsHHqrY51Jtb9YKOPYXr4AAiPUAWa2Y5V3L9e0nMZO3uSEG6GBnubYjhf1n62Ld02AHo1QB5hZm62/+uQ0yZrkUmQhzb+sLdmlfqcdUUr/zoZE9D49I9i37PWfoAHgONapA8ysb673f1qsUv65jmM7N4TXHWtNdit/tEO2ZLfPtl+Hq1J08P/6dF+9EWf+3w/PVmMWv+OdZrHIlpOjgQ8vVP2atdr/3HMdXpKQnR2dZwMmRagDzGxImZQxUHJWSzKUUdAgnX9Q36zNlNHc8V/OeWc7lTG4weeY4ZZq19uDXIHez9Mql3FynayJhpL6uNTvlDodqU6J6B8EIVk81+f9v3vUt7RUfcaMkeP11z1r1wVakOHY2nWta+EBCIzuV8DMrDZpwiPHXnj+Is0oaNBJ5x8I6/KEVP+B6fV7k9Ry1CYmXphTQppLg84/qEHnOZQ/2qn+Izz782YUNGjQ+QeVkOoOeJ21b5p3rTrvvfLylDX9J0rIy/M9npurQb96ShnjLpEq/1eWTa8p96bJnpOWdt+rY69z75nLenVAB2ipA3obt8szAeJIrad7dUiZJ7wFU3SF9IM/embBHps00SenSQl9pJag6xAbSkjzdLW219LQ3X+xMlPXX7Q/E9/7WZNdsg85qvRBDUrLbpIlyD/3MwoalD6owRvsWxqssvWzK/Ginyptys8lBd5RImf2bP/jW/4mPVXs/U5mSNK4gapdb1fLfof3mQm5uaxTB4SJxYdZfBi9yaY3fcKZJE/36oRHPOEtlHZh0Pl/daq6fXaA7i7P60HnH/R017ZTV5ukne8N8Dveni3ZJVejVaHDiH+4yB3lUO06u9xNHV0bbT01TBqyJrnlbupqmPbcZ1DZQaUNaNLR/Uk+4ySDBbmA0gZIExZK6fkd/6MikGNrJ/pPwLDIcEv1xfPUknaaTzAEertYZA9a6oDeIthfhM5qz/Ef/DF0sLPapMLveF9mFEr6lVW1Cx5Qy76D3uMJaS7lnu0MGOgkKS27SQmpLrUcDRa6PK18uWc5VbUq0BgsT/0HFB9W/xFHAoYLq1Xa/XE/GfIdI9L6zl8plazHuuU2DzJ0+1Kpb4MhS6dDmSFrolvuZpvPsfBDXlcCYds/T//PaVDZQaWf5Gkd2/dVH9VXp3TiWZ575Z/rUN88T+trn1z/Vtiw1e/zBLo236ew+ayd6F9Pi9WiPrv/U7r9y8jDInCCi9uYuu3bt2v69OkqLCxUamqqhg0bpnnz5qmpKfQvmrFjx8pisfj83HLLLTGqNRAnHfxFKEkqn+MpF4GMyy7TKe+9r8HfkwaWHtTgi/bplO/tCRroJM8s2txRrd1jgVv5co9NsAg0Bqt1zFZ28RFZEzzhwj7kqPrkHm8t6lPQoOevtOhAuu/d96dLv/y+VX+90KZl35bOHnlQd/TZr9NGHzgWc0J1PAT77AwNLDuo066q1eCL9nk/h9OmVCv7TIf6DKxXWm6D+gyq95YP977hLAWSkOZS1ogjQT+njMENslg9n9OQCw9qUNlB2ZLb/zmHrpP3XiH+XCPWZrmcgNwuqfJ/pS//6vlv63ezzdqJgRmSs8pTDkBE4tZS99VXX8ntdut3v/udTjnlFFVUVOjmm29WXV2dHn/88ZDX3nzzzVqwYIH3dVpaWndXF4ivSP4ijLD1xJKYpD4/eeRYK2B4MgoapYtbVPuP/mrZf3z7sYT+mcod5VBG/4Zj5dqMwYqgq299SrLeLUzQshGGRu4y1O+IdLCvtLnAIsPqiW8HbTblulw6t6FRKpCs5x/0jMc6Gn7rjjXJrfxzHd6w0771asDIOmnk8dfOXQ1+z0hIcyn7TKdcR21qqrP5zBZtX9aa5Fa/U48oLbtJrkbfzyPnjMNhfU4Zgxu8LXctDTY1Hbbp4NY+crUZ62hLcanfKfVKSnd1rns1HG2Wy/ETapiAK8wWwo5CIwA/cQt1EyZM0IQJE7yvhw4dqi1btujZZ5/tMNSlpaUpr91sKsDUwv0LrrN/EQaYTBHcsVm0s55Q+vCJ/gPgLfKEy8PVUt1eWdL6q887/8/TZRemvcfGUBlWizYNCd7V+ElKikY1NMom/wBpTXKp8VCimus9QStzWJ2O7k9S/Z4kSRal5TSqT05kYSeSkBppoG1tjQtH+7IDio5EHJw7z+IJaEPKAp/uaJjA2LnhPSZUaAQQUI8aU+dwOJSVldVhuRdffFFLlixRXl6eJk2apPvuuy9ka11jY6MaG4+vfu90srE5eplw/4Lryl+ERVdIIyb6zqyt2y+9OzdAi8vDUtEVskjqUzLG/17tWwsT06SXfxx2VQaEucfn7/vZ9UZ6H83Zf1Dj6o/6hZ30fN+Q1DevyTumrLO6Er66S6ye4x3LN+HhwOPdOhwmYJHWLfZZOzHgM0KFRgBB9ZhQt3XrVj399NMdttL96Ec/0pAhQzRw4EB98cUXuvvuu7Vlyxa9+uqrQa9ZuHCh5s+fH+0qA7HTbhFhf1H6i7DdZApJUtGkyJZQiYLRjc3Ktaao1t3xGLBam02zcwboiT37NK7+aLfWyzRKZkiDz5NeuUERbQHWJtAHFM4wgcPfSGPvkd5fKE9IDDBRJFhoBBBS1Jc0mTNnjh555JGQZTZv3qwRI0Z4X1dVVenCCy/U2LFj9V//9V8RPW/lypW65JJLtHXrVg0bNixgmUAtdQUFBSxpgt7F260lBfyLsKPZr/HidvmsRxYei5alpWp2zoDWl6EZhjLdbr2/s0pEgTCM/4W0c420+Y3wyvbNDS/Qf/lX6X+md3zPKc9LtqQA4+4GhQ6NQC8WiyVNoh7q9u7dq/3794csM3ToUCUlJUmSvvnmG40dO1bnnXeeFi9eLKs1soEgdXV16tu3r8rLyzV+/PiwrmGdOvRaAQeg9/C/CCv/V3rhexFfZsiiV1KzNb9/jqyJ4Q2ZmHngkG5xMLwiJIvVs9dbuKY8L53+L+GVDffP+vq3PC3CkS6kDfRivXKduuzsbGWHuelyVVWVLrroIo0ePVqLFi2KONBJ0oYNGyRJ+fn5EV8L9DqBxr319L8IOzl5wyJDPzi6R//zz5v0eVaVkrPf6/CaJfZ03exw0loXSiSBTopsnGakwwQCdfcD6LS4rVNXVVWlsWPHavDgwXr88ce1d+9e1dTUqKamxqfMiBEjtHbtWknStm3b9OCDD2rdunXavn273nzzTU2bNk0XXHCBzjjjjHi9FSC2Wv8iPP1fPP/tyYFO6vIsxlw55ao/JayyDptN61OSQ5SweFo2r31VKrpSSkjpUt16rNJbPeGprc5Mh03tF9k4zQB7DbepgOc/jJcDuk3cJkosW7ZMW7du1datW3XSSSf5nGvtEW5ubtaWLVtUX18vSUpKStLy5cv11FNPqa6uTgUFBZoyZYruvffemNcfQJg6bL0JbY8y5aovlLslVdaEjidC7A26pVSbUHHqJdKwsZ0Y6xdAWn+p/oCCvjdbiuQKY9Hfoquk3Wt86xNpV2nbrvhL5x9v0T1SK71zT/j3aVUyI/IAFmx5nI4mWXQWXbiAF3u/MqYO6H5BJ3mEYtHR1FwVH3xcblmV2H+5knOWd3jVH6prPQsSt5c2QPrek8dDRSfH+rWtnzIGSpf9QvrrDceOBZjA8i+Lpb/dLh09qKBS+kn/vs3zv9svKfPX60NX48xrPQE11D6s4U5gaCs1S7pra+cDUizCVlf2QgZirFeOqQMAP8Fab1KzpKMHFGxpi9RJj+kZ9zmav3ST9jblqPWfoJaAW84aynO5NCpQoJM8G9C3/Ys+orF+IZbeKLpCsnbQMmW1hl6n74pfHw887ceYWf9bWvpz/1CYmiVN+lV44SXiLnCL595dCWHdPV6uq3shAyZESx0tdUDsBGq9+epvHc7obWpp0bhXLtPBpr2B73vs19gv9+zTZcHWqmudcdkqkpY6b/gMXL+g761tKOrKzOXWfVR3fOTJMIXfkU7+dvihy7usTBhd4D19NrUUxjI5x1pRb/+Srlj0GLTUATCXgIsbdzyj9x/7Pg8e6CRv010/d6DxZ0EWZvaO9QtjW7SEFGnam1Ld3uBdiR21THVl5rLV5uliHTa247LBrp/Qur9voFZHQzrvX6Xhl/eOMWnduBcy0JsR6gDEXweBaG99iEDXtpwtQVLb7tcQMy69Qaej7cuO7YJgsYa/Xlsw8VzCI9YTGLpTd++FDPRShDoAPV52WnhrX2YnZ0p1dccPdBRYiq7wtFB98tuOb26GgNAb1zkMJBZ7IQO9EKEOQI+3d2++3M12WRIcgSdJSMpLy9Ooa/8m7VoTWWAZfnl4oc4sAcEMC/7Gai9koJeJ2+LDABAOl9vQg29tUWPtJEneORFehiHJkO46999lS0iKfGHm1oAQdIPZYwsWExB6DhY5BgIi1AHo0dZWHlC1o0Eth4vVUHWdjBa7z3mjxa6jVdcp3TWqcw8gIPROrWMEM9ptEZkxkOVMcMKi+xVAj7bn8PHdGFoOF6vlcJFsaZWyJByW0ZIuV32hJKtqnIF3bXC5Da2tPKA9hxuUk56iMYVZslnbhTczTSI4kZhljCAQJYQ6AD1aTnr7/VmtctUP8yv34FsblZpo1YTi4y035RXVmr90k6odxwNfvj1F8yYV+ZSTREDorcwwRhCIErpfAfRoYwqzlG9PCTrirdWBumbNWLJe5RXVkjyBbsaS9T6BTpJqHA0+5Xy0BoRIxuQBQA9BqAPQo9msFs2bVCQp+FSGtuYv3aSmFrfmL90UcF6k0aacy33CbagTXa07XXz5V89/3a541wg4odH9CqDHm1Ccr2evG6V7XqvQgbqmoOUMSdWOBv336u1+LXSByq2tPKDSYf2jX+ETQcBtzwZ6Jp0wBhGIC1rqAPQKE4rzdd/EkWGV3XGgPqxybSdhIAKb3vRsOdZ+qy5ntef4pjfjUy/gBEeoA9Br5NlTwyo3JCstrHL+kzDQIbfL00IXqnO7fA5dsUAcEOoA9BodTZqwyDO79celJ4dVbkxhVvdU1Mx2rPJvofNhSM4qTzkAMUWoA9BrhJo00fp63qQiJSVYwyrnt14dOhbuHrhm2CsX6GUIdQB6ldZJE3l2367TPHuKnr1ulHf9uXDLIULh7oFrlr1ygV7EYhjtd1I0P6fTKbvdLofDoYyMjHhXB0AnhLVTRATlECa3S3qq2DMpIuC4OotnFuztX7LOH9BGLLIHS5oA6JVsVktYy5GEWw5hat0r9+Vp8nRmtw127JULxBPdrwCAyLTulZvRrgs7Y6DnOOvUAXFBSx0AIHLslQv0OIQ6AEDntO6VC6BHoPsVAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABOIa6k4++WRZLBafn4cffjjkNQ0NDZo5c6b69++vvn37asqUKaqtrY1RjQEAAHqmuLfULViwQNXV1d6fW2+9NWT5O+64Q0uXLtUrr7yiDz74QN98842+//3vx6i2AAAAPVNCvCuQnp6uvLy8sMo6HA49//zzeumll3TxxRdLkhYtWqSRI0fqk08+0XnnndedVQUAAOix4t5S9/DDD6t///46++yz9dhjj6mlpSVo2XXr1qm5uVnjxo3zHhsxYoQGDx6s1atXx6K6AAAAPVJcW+p+/vOfa9SoUcrKytKqVas0d+5cVVdX64knnghYvqamRklJScrMzPQ5npubq5qamqDPaWxsVGNjo/e10+mMSv0BAAB6iqi31M2ZM8dv8kP7n6+++kqSNHv2bI0dO1ZnnHGGbrnlFv3yl7/U008/7RPAomHhwoWy2+3en4KCgqjeHwAAIN6i3lJ355136oYbbghZZujQoQGPl5SUqKWlRdu3b9fw4cP9zufl5ampqUmHDh3yaa2rra0NOS5v7ty5mj17tve10+kk2AEAAFOJeqjLzs5WdnZ2p67dsGGDrFarcnJyAp4fPXq0EhMTtWLFCk2ZMkWStGXLFu3cuVOlpaVB75ucnKzk5ORO1QkAAKA3iNuYutWrV2vNmjW66KKLlJ6ertWrV+uOO+7Qddddp379+kmSqqqqdMkll+iPf/yjxowZI7vdrunTp2v27NnKyspSRkaGbr31VpWWljLzFQAAnNDiFuqSk5P15z//WQ888IAaGxtVWFioO+64w6ebtLm5WVu2bFF9fb332JNPPimr1aopU6aosbFR48eP129/+9t4vAUAAIAew2IYhhHvSsSa0+mU3W6Xw+FQRkZGvKsDAABMLhbZI+7r1AEAAKDrCHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAE4hbq3n//fVksloA/n376adDrxo4d61f+lltuiWHNAQAAep6EeD24rKxM1dXVPsfuu+8+rVixQuecc07Ia2+++WYtWLDA+zotLa1b6ggAANBbxC3UJSUlKS8vz/u6ublZb7zxhm699VZZLJaQ16alpflcCwAAcKLrMWPq3nzzTe3fv1833nhjh2VffPFFDRgwQMXFxZo7d67q6+tDlm9sbJTT6fT5AQAAMJO4tdS19/zzz2v8+PE66aSTQpb70Y9+pCFDhmjgwIH64osvdPfdd2vLli169dVXg16zcOFCzZ8/P9pVBgAA6DEshmEY0bzhnDlz9Mgjj4Qss3nzZo0YMcL7evfu3RoyZIhefvllTZkyJaLnrVy5Updccom2bt2qYcOGBSzT2NioxsZG72un06mCggI5HA5lZGRE9DwAAIBIOZ1O2e32bs0eUW+pu/POO3XDDTeELDN06FCf14sWLVL//v11xRVXRPy8kpISSQoZ6pKTk5WcnBzxvQEAAHqLqIe67OxsZWdnh13eMAwtWrRI06ZNU2JiYsTP27BhgyQpPz8/4msBAADMIu4TJVauXKnKykrddNNNfueqqqo0YsQIrV27VpK0bds2Pfjgg1q3bp22b9+uN998U9OmTdMFF1ygM844I9ZVBwAA6DHiPlHi+eefV1lZmc8Yu1bNzc3asmWLd3ZrUlKSli9frqeeekp1dXUqKCjQlClTdO+998a62gAAAD1K1CdK9AaxGKwIAADQKhbZI+7drwAAAOg6Qh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACbQbaHuoYceUllZmdLS0pSZmRmwzM6dOzVx4kSlpaUpJydHd911l1paWkLe98CBA7r22muVkZGhzMxMTZ8+XUeOHOmGdwAAANB7dFuoa2pq0tVXX60ZM2YEPO9yuTRx4kQ1NTVp1apVeuGFF7R48WLdf//9Ie977bXXauPGjVq2bJneeustffjhh/rpT3/aHW8BAACg17AYhmF05wMWL16s22+/XYcOHfI5/ve//13f+9739M033yg3N1eS9Nxzz+nuu+/W3r17lZSU5HevzZs3q6ioSJ9++qnOOeccSVJ5ebkuv/xy7d69WwMHDgyrTk6nU3a7XQ6HQxkZGV17gwAAAB2IRfZI6Ja7hmH16tU6/fTTvYFOksaPH68ZM2Zo48aNOvvsswNek5mZ6Q10kjRu3DhZrVatWbNGV111VcBnNTY2qrGx0fva4XBI8nzAAAAA3a01c3RnW1rcQl1NTY1PoJPkfV1TUxP0mpycHJ9jCQkJysrKCnqNJC1cuFDz58/3O15QUBBptQEAADpt//79stvt3XLviELdnDlz9Mgjj4Qss3nzZo0YMaJLlYq2uXPnavbs2d7Xhw4d0pAhQ7Rz585u+2B7E6fTqYKCAu3atYvuaPF5tMfncRyfhS8+D198Hr74PHw5HA4NHjxYWVlZ3faMiELdnXfeqRtuuCFkmaFDh4Z1r7y8PK1du9bnWG1trfdcsGv27Nnjc6ylpUUHDhwIeo0kJScnKzk52e+43W7ni9ZGRkYGn0cbfB6++DyO47Pwxefhi8/DF5+HL6u1+1aTiyjUZWdnKzs7OyoPLi0t1UMPPaQ9e/Z4u1SXLVumjIwMFRUVBb3m0KFDWrdunUaPHi1JWrlypdxut0pKSqJSLwAAgN6o2+Lizp07tWHDBu3cuVMul0sbNmzQhg0bvGvKXXbZZSoqKtKPf/xj/eMf/9A777yje++9VzNnzvS2qq1du1YjRoxQVVWVJGnkyJGaMGGCbr75Zq1du1Yff/yxZs2apR/+8Idhz3wFAAAwo26bKHH//ffrhRde8L5unc363nvvaezYsbLZbHrrrbc0Y8YMlZaWqk+fPrr++uu1YMEC7zX19fXasmWLmpubvcdefPFFzZo1S5dccomsVqumTJmiX//61xHVLTk5WfPmzQvYJXsi4vPwxefhi8/jOD4LX3wevvg8fPF5+IrF59Ht69QBAACg+7H3KwAAgAkQ6gAAAEyAUAcAAGAChDoAAAATMGWoe+ihh1RWVqa0tDRlZmYGLLNz505NnDhRaWlpysnJ0V133aWWlpaQ9z1w4ICuvfZaZWRkKDMzU9OnT/cu0dKbvP/++7JYLAF/Pv3006DXjR071q/8LbfcEsOad4+TTz7Z7309/PDDIa9paGjQzJkz1b9/f/Xt21dTpkzxLp7dm23fvl3Tp09XYWGhUlNTNWzYMM2bN09NTU0hrzPTd+OZZ57RySefrJSUFJWUlPgtkt7eK6+8ohEjRiglJUWnn3663n777RjVtHstXLhQ5557rtLT05WTk6PJkydry5YtIa9ZvHix3/cgJSUlRjXuXg888IDfe+to9ySzfjekwL83LRaLZs6cGbC82b4bH374oSZNmqSBAwfKYrHo9ddf9zlvGIbuv/9+5efnKzU1VePGjdPXX3/d4X0j/f3TnilDXVNTk66++mrNmDEj4HmXy6WJEyeqqalJq1at0gsvvKDFixfr/vvvD3nfa6+9Vhs3btSyZcv01ltv6cMPP9RPf/rT7ngL3aqsrEzV1dU+PzfddJMKCwt1zjnnhLz25ptv9rnu0UcfjVGtu9eCBQt83tett94asvwdd9yhpUuX6pVXXtEHH3ygb775Rt///vdjVNvu89VXX8ntdut3v/udNm7cqCeffFLPPfec7rnnng6vNcN34y9/+Ytmz56tefPmaf369TrzzDM1fvx4v51sWq1atUpTp07V9OnT9fnnn2vy5MmaPHmyKioqYlzz6Pvggw80c+ZMffLJJ1q2bJmam5t12WWXqa6uLuR1GRkZPt+DHTt2xKjG3e9b3/qWz3v76KOPgpY183dDkj799FOfz2LZsmWSpKuvvjroNWb6btTV1enMM8/UM888E/D8o48+ql//+td67rnntGbNGvXp00fjx49XQ0ND0HtG+vsnIMPEFi1aZNjtdr/jb7/9tmG1Wo2amhrvsWeffdbIyMgwGhsbA95r06ZNhiTj008/9R77+9//blgsFqOqqirqdY+lpqYmIzs721iwYEHIchdeeKFx2223xaZSMTRkyBDjySefDLv8oUOHjMTEROOVV17xHtu8ebMhyVi9enU31DC+Hn30UaOwsDBkGbN8N8aMGWPMnDnT+9rlchkDBw40Fi5cGLD8D37wA2PixIk+x0pKSoyf/exn3VrPeNizZ48hyfjggw+Clgn2O9cM5s2bZ5x55plhlz+RvhuGYRi33XabMWzYMMPtdgc8b+bvhiTjtdde8752u91GXl6e8dhjj3mPHTp0yEhOTjb+9Kc/Bb1PpL9/AjFlS11HVq9erdNPP125ubneY+PHj5fT6dTGjRuDXpOZmenTkjVu3DhZrVatWbOm2+vcnd58803t379fN954Y4dlX3zxRQ0YMEDFxcWaO3eu6uvrY1DD7vfwww+rf//+Ovvss/XYY4+F7Ipft26dmpubNW7cOO+xESNGaPDgwVq9enUsqhtTDocjrA2oe/t3o6mpSevWrfP5c7VarRo3blzQP9fVq1f7lJc8v0vM+j2Q1OF34ciRIxoyZIgKCgp05ZVXBv2d2ht9/fXXGjhwoIYOHaprr71WO3fuDFr2RPpuNDU1acmSJfrJT34ii8UStJyZvxttVVZWqqamxufP3263q6SkJOiff2d+/wTSbTtK9GQ1NTU+gU6S93VNTU3Qa1r3qG2VkJCgrKysoNf0Fs8//7zGjx+vk046KWS5H/3oRxoyZIgGDhyoL774Qnfffbe2bNmiV199NUY17R4///nPNWrUKGVlZWnVqlWaO3euqqur9cQTTwQsX1NTo6SkJL/xmrm5ub3+u9De1q1b9fTTT+vxxx8PWc4M3419+/bJ5XIF/N3w1VdfBbwm2O8Ss30P3G63br/9dp1//vkqLi4OWm748OH6wx/+oDPOOEMOh0OPP/64ysrKtHHjxg5/v/R0JSUlWrx4sYYPH67q6mrNnz9f3/nOd1RRUaH09HS/8ifKd0OSXn/9dR06dEg33HBD0DJm/m601/pnHMmff2d+/wTSa0LdnDlz9Mgjj4Qss3nz5g4HrppZZz6j3bt365133tHLL7/c4f3bjh88/fTTlZ+fr0suuUTbtm3TsGHDOl/xbhDJZzF79mzvsTPOOENJSUn62c9+poULF5pme5vOfDeqqqo0YcIEXX311br55ptDXtubvhuI3MyZM1VRURFyDJkklZaWqrS01Pu6rKxMI0eO1O9+9zs9+OCD3V3NbvXd737X+7/POOMMlZSUaMiQIXr55Zc1ffr0ONYs/p5//nl997vfDbkHu5m/Gz1Jrwl1d955Z8h/BUjS0KFDw7pXXl6e34yS1pmLeXl5Qa9pP1ixpaVFBw4cCHpNrHXmM1q0aJH69++vK664IuLnlZSUSPK05vS0v7i78n0pKSlRS0uLtm/fruHDh/udz8vLU1NTkw4dOuTTWldbW9tjvgvtRfp5fPPNN7roootUVlam3//+9xE/ryd/N4IZMGCAbDab3yzmUH+ueXl5EZXvjWbNmuWdGBZpi0piYqLOPvtsbd26tZtqFz+ZmZk67bTTgr63E+G7IUk7duzQ8uXLI26VN/N3o/XPuLa2Vvn5+d7jtbW1OuusswJe05nfP4H0mlCXnZ2t7OzsqNyrtLRUDz30kPbs2ePtUl22bJkyMjJUVFQU9JpDhw5p3bp1Gj16tCRp5cqVcrvd3r/A4i3Sz8gwDC1atEjTpk1TYmJixM/bsGGDJPl8aXuKrnxfNmzYIKvV6tfd3mr06NFKTEzUihUrNGXKFEnSli1btHPnTp9/ifYkkXweVVVVuuiiizR69GgtWrRIVmvkQ2978ncjmKSkJI0ePVorVqzQ5MmTJXm6HVesWKFZs2YFvKa0tFQrVqzQ7bff7j22bNmyHvs9iIRhGLr11lv12muv6f3331dhYWHE93C5XPryyy91+eWXd0MN4+vIkSPatm2bfvzjHwc8b+bvRluLFi1STk6OJk6cGNF1Zv5uFBYWKi8vTytWrPCGOKfTqTVr1gRdlaMzv38CimSGR2+xY8cO4/PPPzfmz59v9O3b1/j888+Nzz//3Dh8+LBhGIbR0tJiFBcXG5dddpmxYcMGo7y83MjOzjbmzp3rvceaNWuM4cOHG7t37/YemzBhgnH22Wcba9asMT766CPj1FNPNaZOnRrz9xcty5cvNyQZmzdv9ju3e/duY/jw4caaNWsMwzCMrVu3GgsWLDA+++wzo7Ky0njjjTeMoUOHGhdccEGsqx1Vq1atMp588kljw4YNxrZt24wlS5YY2dnZxrRp07xl2n8WhmEYt9xyizF48GBj5cqVxmeffWaUlpYapaWl8XgLUbV7927jlFNOMS655BJj9+7dRnV1tfenbRmzfjf+/Oc/G8nJycbixYuNTZs2GT/96U+NzMxM70z5H//4x8acOXO85T/++GMjISHBePzxx43Nmzcb8+bNMxITE40vv/wyXm8hambMmGHY7Xbj/fff9/ke1NfXe8u0/zzmz59vvPPOO8a2bduMdevWGT/84Q+NlJQUY+PGjfF4C1F15513Gu+//75RWVlpfPzxx8a4ceOMAQMGGHv27DEM48T6brRyuVzG4MGDjbvvvtvvnNm/G4cPH/ZmC0nGE088YXz++efGjh07DMMwjIcfftjIzMw03njjDeOLL74wrrzySqOwsNA4evSo9x4XX3yx8fTTT3tfd/T7JxymDHXXX3+9Icnv57333vOW2b59u/Hd737XSE1NNQYMGGDceeedRnNzs/f8e++9Z0gyKisrvcf2799vTJ061ejbt6+RkZFh3Hjjjd6g2BtNnTrVKCsrC3iusrLS5zPbuXOnccEFFxhZWVlGcnKyccoppxh33XWX4XA4Yljj6Fu3bp1RUlJi2O12IyUlxRg5cqTxi1/8wmhoaPCWaf9ZGIZhHD161PjXf/1Xo1+/fkZaWppx1VVX+QSf3mrRokUB/7/T9t9/Zv9uPP3008bgwYONpKQkY8yYMcYnn3ziPXfhhRca119/vU/5l19+2TjttNOMpKQk41vf+pbxt7/9LcY17h7BvgeLFi3ylmn/edx+++3ezy43N9e4/PLLjfXr18e+8t3gmmuuMfLz842kpCRj0KBBxjXXXGNs3brVe/5E+m60eueddwxJxpYtW/zOmf270ZoR2v+0vme3223cd999Rm5urpGcnGxccsklfp/TkCFDjHnz5vkcC/X7JxwWwzCM8Nv1AAAA0BOdkOvUAQAAmA2hDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADCB/w/O73qLDwcvTwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n_train in [1000,5000,9000]:\n",
    "    vis(n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4e60172799112451e675ed8f39ac039c9c5784e1c33f4b2379e9e5e54444ff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
